{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#standard libraries\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "#custom libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file_format(filename_queue):\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    _, value = reader.read(filename_queue)\n",
    "\n",
    "    record_defaults = [tf.constant([], dtype=tf.float32), tf.constant([], dtype=tf.int32)]\n",
    "    _, col2 = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "    \n",
    "    example = tf.stack([col2])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_pipeline(filenames, batch_size = 3, seq_length=3,\n",
    "                   num_epochs = None, evaluation = False):   \n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        filenames, num_epochs=num_epochs, shuffle=False)\n",
    "\n",
    "    example = read_file_format(filename_queue)\n",
    "        \n",
    "    min_after_dequeue = 10\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch = tf.train.batch(\n",
    "        [example], batch_size=batch_size*seq_length, capacity=capacity\n",
    "    )    \n",
    "\n",
    "    label_batch = tf.concat(\n",
    "        [example_batch[-1], example_batch[1:,0]],\n",
    "        axis=0)\n",
    "\n",
    "    example_batch = tf.reshape(example_batch, (batch_size, seq_length))\n",
    "    label_batch = tf.reshape(label_batch, (batch_size, seq_length))\n",
    "\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _activation_summary(x):\n",
    "    tensor_name = x.name\n",
    "    tensor_name = tensor_name.replace(':', '_')\n",
    "    tensor_name = tensor_name.replace('(', '_')\n",
    "    tensor_name = tensor_name.replace(')', '_')\n",
    "    tensor_name = tensor_name.replace(' ', '_')\n",
    "\n",
    "    tf.summary.histogram(tensor_name + '/activations', x)\n",
    "    tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _add_loss_summaries(total_loss, averager, include_averaged_loss=False):\n",
    "    # Compute the moving average of all individual losses and the total loss.\n",
    "    losses = tf.get_collection('losses')\n",
    "    if include_averaged_loss:\n",
    "        loss_averages_op = averager.apply(losses + [total_loss])\n",
    "\n",
    "    # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "    # same for the averaged version of the losses.\n",
    "    for l in losses + [total_loss]:\n",
    "    # Name each loss as '(raw)' and name the moving average version of the loss\n",
    "    # as the original loss name.\n",
    "        l_name = l.name.replace(\":\", \"_\")\n",
    "\n",
    "        tf.summary.scalar(l_name + '_raw_', l)        \n",
    "        if include_averaged_loss:\n",
    "            tf.summary.scalar(l_name + '_raw_', l)\n",
    "            tf.summary.scalar(l_name, averager.average(l))\n",
    "        \n",
    "    if include_averaged_loss:\n",
    "        return loss_averages_op\n",
    "    else:\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kl_divergence(P, Q, sample_size=1000): \n",
    "    '''\n",
    "    Calculate KL Divergence between a reference distribution P and an approximating\n",
    "    distribution Q using values drawn from the two distributions.\n",
    "    \n",
    "    P and Q are used here per canonical IT notation, giving KL(P||Q).\n",
    "\n",
    "    Note that the typical approach in variational inference minimizes KL(Q||P),\n",
    "    where the variational distribution Q is the reference distribution and\n",
    "    P is the approximating distribution.\n",
    "    \n",
    "    Args:\n",
    "    P: A tf.contrib.distributions.Distribution for reference distribution\n",
    "    Q: A tf.contrib.distributions.Distribution for approximating distribution\n",
    "\n",
    "    '''\n",
    "    P_vals = P.sample([sample_size])\n",
    "    \n",
    "    P_probs = P.prob(P_vals)\n",
    "    Q_probs = Q.prob(P_vals)\n",
    "    \n",
    "    PQ = tf.div(P_probs+1e-10, Q_probs+1e-10)\n",
    "    safe_PQ = tf.where(tf.equal(PQ, 0.), tf.ones(PQ.shape), PQ) \n",
    "    log_PQ = tf.where(tf.equal(safe_PQ, 0.), tf.zeros(PQ.shape), tf.log(safe_PQ)) \n",
    "    return tf.reduce_sum(tf.multiply(P_probs, log_PQ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_, y_hat, vocab_size):\n",
    "    y_ = tf.one_hot(tf.reshape(tf.cast(y_, tf.int32), [-1]), depth=vocab_size)\n",
    "    return tf.losses.softmax_cross_entropy(y_, y_hat)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variational_free_energy(y_, y_hat, weight_dist, weight_prior, vocab_size, monte_carlo_size):\n",
    "    '''\n",
    "    Calculation variational free energy or evidence lower bound loss.\n",
    "    \n",
    "    It can be shown that the variational free energy is equivalent to the\n",
    "    sum of the expected log loss of the network with w ~ Q(w) and the KL \n",
    "    Divergence of the variational distribution Q(w) from the weight prior P(w).\n",
    "    '''\n",
    "\n",
    "    nll_f = nll(y_, y_hat, vocab_size)\n",
    "\n",
    "    sess = tf.get_default_session() \n",
    "    graph = tf.get_default_graph()\n",
    "    weights = graph.get_tensor_by_name(\"weights/W:0\")\n",
    "    try:\n",
    "        nll_eval = lambda x: sess.run(nll_f, feed_dict={weights: x.eval()})\n",
    "        exp_nll = tf.contrib.bayesflow.monte_carlo.expectation(\n",
    "            f=nll_eval,\n",
    "            p=weight_dist,\n",
    "            n=monte_carlo_size,\n",
    "            name='expectation'\n",
    "        )\n",
    "\n",
    "    except:\n",
    "        exp_nll = np.inf\n",
    "\n",
    "    kld = kl_divergence(weight_dist, weight_prior)\n",
    "\n",
    "    return tf.reduce_sum([exp_nll, kld])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM_Cell:\n",
    "    \n",
    "    def __init__(self, args, scope_name, current_weights=None):\n",
    "        self.rnn_size = args.rnn_size\n",
    "        self.num_proj = args.vocab_size\n",
    "        self.input_size = args.batch_size * args.seq_length\n",
    "        self.state_size = self.rnn_size * 2        \n",
    "        \n",
    "        if args.weight_noise_type == None:\n",
    "            with tf.variable_scope(scope_name):\n",
    "                self.W = tf.get_variable('W', [self.input_size + self.num_proj, 4 * self.rnn_size],\n",
    "                                         tf.float32, tf.random_normal_initializer())\n",
    "                self.b = tf.get_variable('b', [self.rnn_size * 4], tf.float32, tf.constant_initializer(0.0))\n",
    "\n",
    "        if args.weight_noise_type == \"adaptive\":\n",
    "            with tf.variable_scope(scope_name):\n",
    "                \n",
    "                self.W = tf.reshape(current_weights, [self.input_size + self.num_proj, 4 * self.rnn_size])\n",
    "\n",
    "                self.b = tf.get_variable('b', [1, self.rnn_size * 4], tf.float32, tf.constant_initializer(0.0))\n",
    "            \n",
    "            \n",
    "    def __call__(self, i, state):\n",
    "        self.c_prev = tf.slice(state, [0, 0], [-1, self.rnn_size])\n",
    "        self.h_prev = tf.slice(state, [0, self.rnn_size], [-1, self.num_proj])\n",
    "\n",
    "        data = tf.concat([i, self.h_prev], 1)\n",
    "\n",
    "        weighted = tf.matmul(data, self.W)\n",
    "\n",
    "        self.i, self.j, self.f, self.o = tf.split(weighted, num_or_size_splits=4, axis=1)\n",
    "        self.i_b, self.j_b, _, self.o_b = tf.split(self.b, num_or_size_splits=4, axis=1)\n",
    "        \n",
    "        self.c = (tf.sigmoid(self.f + args.forget_bias) * self.c_prev +\n",
    "                  tf.sigmoid(self.i + self.i_b) * tf.tanh(self.j + self.j_b))\n",
    "        self.h = tf.sigmoid(self.o + self.o_b) * tf.tanh(self.c)\n",
    "        \n",
    "        self.state = tf.concat([self.c, self.h], axis=1)\n",
    "        return self.h, self.state\n",
    "    \n",
    "    def zero_state(self, batch_size, dtype):\n",
    "        return tf.zeros([batch_size, self.state_size], dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weight_means(y_, y_hat, x, weight_dist, prior_dist, args):\n",
    " \n",
    "    diff = tf.subtract(weight_dist.mean(), prior_dist.mean())\n",
    "    centered = tf.div(diff, prior_dist.covariance())\n",
    "\n",
    "    weight_sample = weight_dist.sample([args.gradient_sample_size])\n",
    "\n",
    "    nll_f = nll(y_, y_hat, args.vocab_size)\n",
    "\n",
    "    sess = tf.get_default_session()\n",
    "    graph = tf.get_default_graph()\n",
    "\n",
    "    weights = graph.get_tensor_by_name(\"weights/W:0\")\n",
    "    nll_grad = tf.gradients(nll_f, weights)\n",
    "    try:\n",
    "        nll_grads = []\n",
    "        for i in range(args.gradient_sample_size):\n",
    "            nll_grads.append(sess.run(nll_grad, feed_dict={weights: weight_sample[i,:].eval(),\n",
    "                                                      x: x.eval()\n",
    "                                                     }))\n",
    "        update = centered + nll_grads\n",
    "    except:\n",
    "        print(\"Error encountered in mean gradient computation, setting update values to 0.\")\n",
    "        update = tf.zeros(shape=weight_dist.mean().shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weight_variances(fn, variables):\n",
    "##########NEED TO FINISH THIS AND MAKE SURE MEAN UPDATE WORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \n",
    "    def __init__(self, args):\n",
    "\n",
    "        self.batch_size = args.batch_size\n",
    "        self.seq_length = args.seq_length\n",
    "\n",
    "        self.x = tf.placeholder(tf.float32, shape=[args.batch_size, args.seq_length])\n",
    "        self.y_ = tf.placeholder(tf.float32, shape=[args.batch_size, args.seq_length])\n",
    "        \n",
    "        with tf.variable_scope(\"weights\"):\n",
    "            if args.weight_noise_type == \"adaptive\":\n",
    "                lstm_weight_size = (args.batch_size * args.seq_length + args.vocab_size) * (4 * args.rnn_size) * args.num_layers\n",
    "                softmax_weight_size = (args.rnn_size * args.vocab_size)\n",
    "                embedding_weight_size = (args.rnn_size * args.vocab_size)\n",
    "                weight_size = lstm_weight_size + softmax_weight_size + embedding_weight_size\n",
    "\n",
    "                prior_loc = [0.] * weight_size\n",
    "                prior_scale_diag = [args.weight_prior_variance] * weight_size\n",
    "                self.weight_prior = tf.contrib.distributions.MultivariateNormalDiag(\n",
    "                        prior_loc,\n",
    "                        prior_scale_diag\n",
    "                )\n",
    "                S_hat = tf.get_variable(\"S_hat\", initializer=prior_scale_diag)\n",
    "                S = tf.exp(S_hat) # make sure sigma matrix is positive\n",
    "\n",
    "                mu = tf.get_variable(\"mu\", initializer=prior_loc)\n",
    "\n",
    "                self.weight_dist = tf.contrib.distributions.MultivariateNormalDiag(mu, S)\n",
    "\n",
    "                weights_st = tf.contrib.bayesflow.stochastic_tensor.StochasticTensor(self.weight_dist)\n",
    "                \n",
    "                current_weights = tf.squeeze(weights_st, name=\"W\")\n",
    "\n",
    "                lstm_W, softmax_W, embedding_mat = tf.split(current_weights,\n",
    "                                                            num_or_size_splits=[lstm_weight_size,\n",
    "                                                                                softmax_weight_size,\n",
    "                                                                                embedding_weight_size])\n",
    "                lstm_W = tf.split(lstm_W, num_or_size_splits=args.num_layers)\n",
    "            \n",
    "            else:\n",
    "                self.lstm_W = None\n",
    "\n",
    "            self.lstm_cells = []\n",
    "            for i in range(args.num_layers):\n",
    "                self.lstm_cells.append(LSTM_Cell(args, \"lstm_{0}\".format(i), lstm_W[i]))\n",
    "            self.lstm = tf.contrib.rnn.MultiRNNCell(self.lstm_cells)\n",
    "\n",
    "            self.initial_state = self.lstm.zero_state(args.batch_size, tf.float32)\n",
    "\n",
    "            if args.weight_noise_type is None:\n",
    "                softmax_W = tf.get_variable('softmax_W', [args.rnn_size, args.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "                embedding_mat = tf.get_variable('embedding', [args.vocab_size, args.rnn_size],\n",
    "                                            tf.float32, tf.random_normal_initializer())\n",
    "\n",
    "            elif args.weight_noise_type == \"static\":\n",
    "                softmax_W = tf.get_variable('softmax_W', [args.rnn_size, args.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "                softmax_noise = tf.truncated_normal([args.rnn_size, args.vocab_size], stddev=args.weight_prior_variance)\n",
    "                softmax_W = softmax_W + softmax_noise\n",
    "\n",
    "                embedding_mat = tf.get_variable('embedding', [args.vocab_size, args.rnn_size],\n",
    "                                            tf.float32, tf.random_normal_initializer())\n",
    "                embedding_noise = tf.truncated_normal([args.vocab_size, args.rnn_size], stddev=args.weight_prior_variance)\n",
    "                embedding_mat = embedding_mat + embedding_noise\n",
    "\n",
    "\n",
    "            elif args.weight_noise_type == \"adaptive\":\n",
    "                softmax_W = tf.reshape(softmax_W, [args.rnn_size, args.vocab_size])\n",
    "                embedding_mat = tf.reshape(embedding_mat, [args.vocab_size, args.rnn_size])\n",
    "\n",
    "            else:\n",
    "                raise Exception(\"Unrecognized value for weight_noise_type; \" +\n",
    "                                \"recognized values are: None, 'static', and 'adaptive'.\")\n",
    "\n",
    "            b = tf.get_variable('b', [args.vocab_size], tf.float32, tf.constant_initializer(0.0))\n",
    "\n",
    "\n",
    "            embedding_output = tf.nn.embedding_lookup(embedding_mat, tf.cast(self.x, tf.int32))\n",
    "\n",
    "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.seq_length, value=embedding_output)\n",
    "            rnn_inputs = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "\n",
    "        outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(rnn_inputs,\n",
    "                                                                    self.initial_state,\n",
    "                                                                    self.lstm,\n",
    "                                                                    scope='lstm')\n",
    "        output = tf.reshape(tf.concat(outputs,1), [-1, args.rnn_size])\n",
    "\n",
    "        self.logits = tf.matmul(output, softmax_W) + b\n",
    "        self.softmax_p = tf.nn.softmax(self.logits)\n",
    "        \n",
    "    def train(self, args):\n",
    "        \n",
    "        t = time.time()\n",
    "        if args.weight_noise_type is None:\n",
    "            self.loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "                [self.softmax_p],\n",
    "                [self.y_],\n",
    "                [tf.ones([args.batch_size * args.seq_length], dtype=tf.float32)]\n",
    "            )\n",
    "        else:\n",
    "            self.loss = variational_free_energy(\n",
    "                self.y_,\n",
    "                self.softmax_p,\n",
    "                self.weight_dist,\n",
    "                self.weight_prior,\n",
    "                args.vocab_size,\n",
    "                args.loss_sample_size\n",
    "            )\n",
    "        print(\"{0}s elapsed for loss calculation\".format(str(time.time()-t)))\n",
    "\n",
    "        tf.add_to_collection('losses', self.loss)\n",
    "        tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(args.learning_rate)\n",
    "\n",
    "        if args.weight_noise_type in [None, \"static\"]:\n",
    "            grads = opt.compute_gradients(self.loss)\n",
    "            trunc_grads = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in grads]\n",
    "            apply_gradient_op = opt.apply_gradients(trunc_grads, global_step=global_step)\n",
    "\n",
    "        else: \n",
    "            with tf.variable_scope(\"weights\", reuse=True):\n",
    "                means = tf.get_variable(\"mu\")\n",
    "                variances = tf.get_variable(\"S_hat\")\n",
    "\n",
    "            t = time.time()\n",
    "\n",
    "#             variance_update = update_weight_variance(self.y_, self.softmax_p, self.weight_dist, self.weight_prior, args)\n",
    "#             var_grads = opt.compute_gradients(self.loss, var_list=variances, grad_loss=variance_update)\n",
    "\n",
    "#             trunc_var_grads = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in var_grads]\n",
    "#             apply_var_grad_op = opt.apply_gradients(trunc_grads, global_step=global_step)\n",
    "\n",
    "            means_update = update_weight_means(self.y_, self.softmax_p, self.x, self.weight_dist, self.weight_prior, args)\n",
    "            mean_grads = opt.compute_gradients(self.loss, var_list=means, grad_loss=means_update)\n",
    "\n",
    "            trunc_mean_grads = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in mean_grads]\n",
    "            apply_mean_grad_op = opt.apply_gradients(trunc_mean_grads, global_step=global_step)\n",
    "\n",
    "            apply_gradient_op = [apply_mean_grad_op] # + [apply_var_grad_op]\n",
    "            print(\"{0}s elapsed for gradient/hessian calculation\".format(str(time.time()-t)))\n",
    "\n",
    "\n",
    "        for var in tf.global_variables():\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "        for grad, var in trunc_mean_grads:\n",
    "            if grad is not None:\n",
    "                tf.summary.histogram(var.op.name + '/mean_gradients', grad)\n",
    "#         for grad, var in trunc_var_grads:\n",
    "#             if grad is not None:\n",
    "#                 tf.summary.histogram(var.op.name + '/variance_gradients', grad)\n",
    "\n",
    "\n",
    "        if args.compute_variable_averages:\n",
    "            variable_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "            variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "            with tf.control_dependencies(apply_gradient_op + [variables_averages_op]):\n",
    "                self.train_op = tf.no_op(name='train')\n",
    "        else:\n",
    "            with tf.control_dependencies(apply_gradient_op):\n",
    "                self.train_op = tf.no_op(name='train')\n",
    "        \n",
    "        return self.train_op\n",
    "\n",
    "        \n",
    "    def sample(self):\n",
    "        flat_y_ = tf.expand_dims(tf.reshape(self.y_, [-1]), 1)\n",
    "        flat_y_ = tf.cast(flat_y_, tf.int64)\n",
    "        samples = tf.multinomial(self.softmax_p, 1)\n",
    "        \n",
    "        self.sampled_results = tf.concat([flat_y_, samples], axis=1)\n",
    "        return self.sampled_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "\n",
    "vocab_file = \"vocab1.csv\"\n",
    "\n",
    "train_file = \"train1.csv\"\n",
    "\n",
    "model_path = 'VanillaLSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download/store Shakespeare data\n",
    "full_model_dir = os.path.join(data_path, model_path)\n",
    "\n",
    "# Make Model Directory\n",
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = pd.read_csv(\"{0}/{1}\".format(data_path, vocab_file),\n",
    "                    header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ArgStruct:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_dict = {\n",
    "    'data_path': data_path,\n",
    "    'model_path': model_path,\n",
    "    'rnn_size': 256,\n",
    "    'num_layers': 1,\n",
    "    'batch_size': 16,\n",
    "    'seq_length': 16,\n",
    "    'forget_bias': 1.,\n",
    "    'num_epochs': 1,\n",
    "    'learning_rate': 0.0001,\n",
    "    'momentum': 0.9,\n",
    "    'logdir': 'TF_Logs',\n",
    "    'vocab_size': len(vocab)+1,\n",
    "    'save_every': 100,\n",
    "    'print_every': 10,\n",
    "    'compute_variable_averages': True,\n",
    "    'weight_noise_type': \"adaptive\",\n",
    "    'weight_prior_variance': 0.05,\n",
    "    'loss_sample_size': 10000,\n",
    "    'gradient_sample_size': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = ArgStruct(**arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24619221687316895s elapsed for loss calculation\n",
      "Error encountered in mean gradient computation, setting update values to 0.\n",
      "3.9057679176330566s elapsed for gradient/hessian calculation\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value weights/S_hat/avg\n\t [[Node: weights/S_hat/avg/read = Identity[T=DT_FLOAT, _class=[\"loc:@weights/S_hat\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](weights/S_hat/avg)]]\n\nCaused by op 'weights/S_hat/avg/read', defined at:\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-19d907d6fa7b>\", line 27, in <module>\n    train_op = model.train(args=args)\n  File \"<ipython-input-12-29a5ff380e60>\", line 157, in train\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\moving_averages.py\", line 367, in apply\n    colocate_with_primary=True)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 113, in create_slot\n    return _create_slot_var(primary, val, \"\", validate_shape, None, None)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 66, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1065, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 962, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 367, in get_variable\n    validate_shape=validate_shape, use_resource=use_resource)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 352, in _true_getter\n    use_resource=use_resource)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 725, in _get_single_variable\n    validate_shape=validate_shape)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 200, in __init__\n    expected_shape=expected_shape)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 319, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1303, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value weights/S_hat/avg\n\t [[Node: weights/S_hat/avg/read = Identity[T=DT_FLOAT, _class=[\"loc:@weights/S_hat\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](weights/S_hat/avg)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value weights/S_hat/avg\n\t [[Node: weights/S_hat/avg/read = Identity[T=DT_FLOAT, _class=[\"loc:@weights/S_hat\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](weights/S_hat/avg)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-19d907d6fa7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m                     \u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerged\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                     feed_dict={model.x: example_batch,\n\u001b[1;32m---> 61\u001b[1;33m                                model.y_: label_batch})\n\u001b[0m\u001b[0;32m     62\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value weights/S_hat/avg\n\t [[Node: weights/S_hat/avg/read = Identity[T=DT_FLOAT, _class=[\"loc:@weights/S_hat\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](weights/S_hat/avg)]]\n\nCaused by op 'weights/S_hat/avg/read', defined at:\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-19d907d6fa7b>\", line 27, in <module>\n    train_op = model.train(args=args)\n  File \"<ipython-input-12-29a5ff380e60>\", line 157, in train\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\moving_averages.py\", line 367, in apply\n    colocate_with_primary=True)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 113, in create_slot\n    return _create_slot_var(primary, val, \"\", validate_shape, None, None)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 66, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1065, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 962, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 367, in get_variable\n    validate_shape=validate_shape, use_resource=use_resource)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 352, in _true_getter\n    use_resource=use_resource)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 725, in _get_single_variable\n    validate_shape=validate_shape)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 200, in __init__\n    expected_shape=expected_shape)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 319, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1303, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value weights/S_hat/avg\n\t [[Node: weights/S_hat/avg/read = Identity[T=DT_FLOAT, _class=[\"loc:@weights/S_hat\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](weights/S_hat/avg)]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "    example_feed, label_feed = input_pipeline(\n",
    "        [\"{0}/{1}\".format(args.data_path, train_file)],\n",
    "        batch_size=args.batch_size,\n",
    "        seq_length=args.seq_length,\n",
    "        num_epochs=args.num_epochs)\n",
    "    \n",
    "    with tf.Session().as_default() as sess:\n",
    "        model = Model(args=args)\n",
    "        \n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(args.logdir, sess.graph)\n",
    "        \n",
    "        #initialize all variables\n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        # Start populating the filename queue.\n",
    "        coord = tf.train.Coordinator()  \n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "        \n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        \n",
    "        train_op = model.train(args=args)\n",
    "        sample_op = model.sample()\n",
    "        \n",
    "        while not coord.should_stop():\n",
    "            try:\n",
    "                start_time = time.time()                \n",
    "\n",
    "                example_batch, label_batch = sess.run([example_feed,\n",
    "                                                      label_feed])\n",
    "                \n",
    "                if global_step.eval() % args.print_every == 0:\n",
    "                    latest_loss = sess.run([model.loss],\n",
    "                                           feed_dict={model.x: example_batch,\n",
    "                                                      model.y_: label_batch})\n",
    "                    try:\n",
    "                        summary_nums = (global_step.eval(), duration,\n",
    "                                        np.mean(latest_loss))\n",
    "                        print('Iteration: {0}, Last Step Duration: {1}, Loss: {2}'.format(*summary_nums))\n",
    "                    except:\n",
    "                        pass\n",
    "                    results = sess.run([sample_op],\n",
    "                                       feed_dict={model.x: example_batch,\n",
    "                                                  model.y_: label_batch})\n",
    "                    results = pd.DataFrame(results[0]).T\n",
    "                    recode_results = results.replace(vocab.set_index(1).to_dict().get(0))\n",
    "                    if not os.path.exists(\"translation.txt\"):\n",
    "                        recode_results.to_csv(\"translation.txt\", header=False, index=False, sep=\"\\t\", mode=\"w\")\n",
    "                    else:\n",
    "                        recode_results.to_csv(\"translation.txt\", header=False, index=False, sep=\"\\t\", mode=\"a\")\n",
    "\n",
    "\n",
    "                result, summary =  sess.run(\n",
    "                    [train_op, merged],\n",
    "                    feed_dict={model.x: example_batch,\n",
    "                               model.y_: label_batch})\n",
    "                writer.add_summary(summary, global_step.eval())\n",
    "                \n",
    "                duration = time.time() - start_time\n",
    "        \n",
    "                # Save the model and the vocab\n",
    "                if global_step.eval() % args.save_every == 0:\n",
    "                    # Save model\n",
    "                    model_file_name = os.path.join(full_model_dir, 'model')\n",
    "                    saver.save(sess, model_file_name, global_step=global_step)\n",
    "                    print('Model Saved To: {}'.format(model_file_name))\n",
    "\n",
    "            except (tf.errors.OutOfRangeError, tf.errors.InvalidArgumentError) as e:\n",
    "           \n",
    "                print('Done training for %d epochs, %d steps.' % (args.num_epochs, global_step.eval()))\n",
    "                # When done, ask the threads to stop.\n",
    "                coord.request_stop()\n",
    "\n",
    "        \n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
