{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard libraries\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "\n",
    "#custom libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file_format(filename_queue):\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    _, value = reader.read(filename_queue)\n",
    "\n",
    "    record_defaults = [tf.constant([], dtype=tf.int32), tf.constant([], dtype=tf.int32)]\n",
    "    _, col2 = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "    \n",
    "    example = tf.stack([col2])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_pipeline(filenames, batch_size = 3, num_epochs = None, evaluation = False):   \n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        filenames, num_epochs=num_epochs, shuffle=False)\n",
    "\n",
    "    example = read_file_format(filename_queue)\n",
    "        \n",
    "    min_after_dequeue = 10\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch = tf.train.batch(\n",
    "        [example], batch_size=batch_size, capacity=capacity\n",
    "    )    \n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _activation_summary(x):\n",
    "    tensor_name = x.name\n",
    "    tensor_name = tensor_name.replace(':', '_')\n",
    "    tensor_name = tensor_name.replace('(', '_')\n",
    "    tensor_name = tensor_name.replace(')', '_')\n",
    "    tensor_name = tensor_name.replace(' ', '_')\n",
    "\n",
    "    tf.summary.histogram(tensor_name + '/activations', x)\n",
    "    tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(x):\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell]*num_layers)\n",
    "        \n",
    "    initial_state = lstm.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    with tf.variable_scope('lstm'):\n",
    "        W = tf.get_variable('W', [rnn_size, vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "        b = tf.get_variable('b', [vocab_size], tf.float32, tf.constant_initializer(0.0))\n",
    "\n",
    "        # Define Embedding\n",
    "        embedding_mat = tf.get_variable('embedding', [vocab_size, rnn_size],\n",
    "                                        tf.float32, tf.random_normal_initializer())\n",
    "        embedding_output = tf.nn.embedding_lookup(embedding_mat, x)\n",
    "        \n",
    "        rnn_inputs = tf.split(axis=1, num_or_size_splits=seq_length, value=embedding_output)\n",
    "        rnn_inputs = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "        \n",
    "    outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(rnn_inputs,\n",
    "                                                                initial_state,\n",
    "                                                                lstm)\n",
    "    output = tf.reshape(tf.concat(outputs,1), [-1, rnn_size])\n",
    "    \n",
    "    logits = tf.matmul(output, W) + b\n",
    "    softmax_y = tf.nn.softmax(logits)\n",
    "    \n",
    "    return softmax_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we are inferring (generating text), we add a 'loop' function\n",
    "# Define how to get the i+1 th input from the i th output\n",
    "def inference_loop(prev, count):\n",
    "    W = tf.get_variable('W')\n",
    "    b = tf.get_variable('b')\n",
    "    # Apply hidden layer\n",
    "    prev_lin = tf.matmul(prev, W) + b\n",
    "    # Get the index of the output (also don't run the gradient)\n",
    "    prev_symbol = tf.stop_gradient(tf.argmax(prev_lin, 1))\n",
    "    # Get embedded vector\n",
    "    output = tf.nn.embedding_lookup(embedding_mat, prev_symbol)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(logits, labels):\n",
    "    loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "        [logits],\n",
    "        [labels],\n",
    "        [tf.ones([batch_size * seq_length], dtype=tf.float32)])\n",
    "    \n",
    "    tf.add_to_collection('losses', loss)\n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _add_loss_summaries(total_loss):\n",
    "    # Compute the moving average of all individual losses and the total loss.\n",
    "    loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "    losses = tf.get_collection('losses')\n",
    "    loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "\n",
    "    # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "    # same for the averaged version of the losses.\n",
    "    for l in losses + [total_loss]:\n",
    "    # Name each loss as '(raw)' and name the moving average version of the loss\n",
    "    # as the original loss name.\n",
    "        l_name = l.name.replace(\":\", \"_\")\n",
    "        tf.summary.scalar(l_name + '_raw_', l)\n",
    "        tf.summary.scalar(l_name, loss_averages.average(l))\n",
    "\n",
    "    return loss_averages_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(total_loss, global_step):\n",
    " \n",
    "    # Generate moving averages of all losses and associated summaries.\n",
    "    loss_averages_op = _add_loss_summaries(total_loss)\n",
    "\n",
    "    # Compute gradients.\n",
    "    with tf.control_dependencies([loss_averages_op]):\n",
    "        opt = tf.train.AdamOptimizer(learning_rate)\n",
    "        grads = opt.compute_gradients(total_loss)\n",
    "        trunc_grads = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in grads]\n",
    "\n",
    "    # Apply gradients.\n",
    "    apply_gradient_op = opt.apply_gradients(trunc_grads, global_step=global_step)\n",
    "\n",
    "    # Add histograms for trainable variables.\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "    # Add histograms for gradients.\n",
    "    for grad, var in trunc_grads:\n",
    "        if grad is not None:\n",
    "            tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "\n",
    "    # Track the moving averages of all trainable variables.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        0.9, global_step\n",
    "    )\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global data_path\n",
    "data_path = \"data\"\n",
    "\n",
    "global vocab_file\n",
    "vocab_file = \"vocab1.csv\"\n",
    "\n",
    "global train_file\n",
    "train_file = \"train1.csv\"\n",
    "\n",
    "global model_path\n",
    "model_path = 'VanillaLSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download/store Shakespeare data\n",
    "full_model_dir = os.path.join(data_path, model_path)\n",
    "\n",
    "# Make Model Directory\n",
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = pd.read_csv(\"{0}/{1}\".format(data_path, vocab_file),\n",
    "                    header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global vocab_size\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "global rnn_size\n",
    "rnn_size = 256\n",
    "\n",
    "global num_layers\n",
    "num_layers = 1\n",
    "\n",
    "global batch_size\n",
    "batch_size = 16\n",
    "\n",
    "global seq_length\n",
    "seq_length = 16\n",
    "\n",
    "global num_epochs\n",
    "num_epochs = 1\n",
    "\n",
    "global learning_rate\n",
    "learning_rate = 0.0001\n",
    "\n",
    "global momentum\n",
    "momentum = 0.9\n",
    "\n",
    "global save_every\n",
    "save_every = 100\n",
    "\n",
    "global print_every\n",
    "print_every = 10\n",
    "\n",
    "global logdir\n",
    "logdir = 'TF_Logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    x = tf.placeholder(tf.int32, shape=[batch_size, seq_length])\n",
    "    y_ = tf.placeholder(tf.int32, shape=[batch_size, seq_length])\n",
    "\n",
    "    y_hat = make_prediction(x)\n",
    "\n",
    "    loss = calculate_loss(y_hat, y_)\n",
    "\n",
    "    train_op = train(loss, global_step=global_step)\n",
    "\n",
    "#     accuracy = evaluate_accuracy(y_hat, y_)\n",
    "\n",
    "    example_feed = input_pipeline([\"{0}/{1}\".format(data_path, train_file)],\n",
    "                                  batch_size = batch_size, num_epochs = num_epochs)\n",
    "        \n",
    "    with tf.Session() as sess: \n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "        \n",
    "        #initialize all variables\n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        # Start populating the filename queue.\n",
    "        coord = tf.train.Coordinator()  \n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "        \n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        \n",
    "        while not coord.should_stop():\n",
    "            try:\n",
    "                start_time = time.time()                \n",
    "\n",
    "                example_batch = sess.run(example_feed)\n",
    "                last_example = example_batch[-1]\n",
    "                if global_step == 0:\n",
    "                    example_batch = [[0]] + example_batch\n",
    "                else:\n",
    "                    example_batch = last_example + example_batch\n",
    "                label_batch = example_batch[1:]\n",
    "                \n",
    "                result, summary =  sess.run([train_op, merged],\n",
    "                                            feed_dict={x: example_batch})\n",
    "                                \n",
    "                writer.add_summary(summary, global_step.eval())\n",
    "                \n",
    "                duration = time.time() - start_time\n",
    "                if global_step % print_every == 0:\n",
    "                    summary_nums = (global_step, duration, loss)\n",
    "                    print('Iteration: {}, Last Step Duration: {}, Loss: {:.2f}'.format(*summary_nums))\n",
    "        \n",
    "                # Save the model and the vocab\n",
    "                if global_step % save_every == 0:\n",
    "                    # Save model\n",
    "                    model_file_name = os.path.join(full_model_dir, 'model')\n",
    "                    saver.save(sess, model_file_name, global_step=global_step)\n",
    "                    print('Model Saved To: {}'.format(model_file_name))\n",
    "\n",
    "            except (tf.errors.OutOfRangeError, tf.errors.InvalidArgumentError) as e:\n",
    "           \n",
    "                print('Done training for %d epochs, %d steps.' % (num_epochs, step))\n",
    "                # When done, ask the threads to stop.\n",
    "                coord.request_stop()\n",
    "\n",
    "        \n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
