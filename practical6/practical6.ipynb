{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard libraries\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "#custom libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file_format(filename_queue):\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    _, value = reader.read(filename_queue)\n",
    "\n",
    "    record_defaults = [tf.constant([], dtype=tf.int32), tf.constant([], dtype=tf.int32)]\n",
    "    _, col2 = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "    \n",
    "    example = tf.stack([col2])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_pipeline(filenames, batch_size = 3, seq_length=3,\n",
    "                   num_epochs = None, evaluation = False):   \n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        filenames, num_epochs=num_epochs, shuffle=False)\n",
    "\n",
    "    example = read_file_format(filename_queue)\n",
    "        \n",
    "    min_after_dequeue = 10\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch = tf.train.batch(\n",
    "        [example], batch_size=batch_size*seq_length, capacity=capacity\n",
    "    )    \n",
    "\n",
    "    label_batch = tf.concat(\n",
    "        [example_batch[-1], example_batch[1:,0]],\n",
    "        axis=0)\n",
    "\n",
    "    example_batch = tf.reshape(example_batch, (batch_size, seq_length))\n",
    "    label_batch = tf.reshape(label_batch, (batch_size, seq_length))\n",
    "\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _activation_summary(x):\n",
    "    tensor_name = x.name\n",
    "    tensor_name = tensor_name.replace(':', '_')\n",
    "    tensor_name = tensor_name.replace('(', '_')\n",
    "    tensor_name = tensor_name.replace(')', '_')\n",
    "    tensor_name = tensor_name.replace(' ', '_')\n",
    "\n",
    "    tf.summary.histogram(tensor_name + '/activations', x)\n",
    "    tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_loss_summaries(total_loss, averager, include_averaged_loss=False):\n",
    "    # Compute the moving average of all individual losses and the total loss.\n",
    "    losses = tf.get_collection('losses')\n",
    "    if include_averaged_loss:\n",
    "        loss_averages_op = averager.apply(losses + [total_loss])\n",
    "\n",
    "    # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "    # same for the averaged version of the losses.\n",
    "    for l in losses + [total_loss]:\n",
    "    # Name each loss as '(raw)' and name the moving average version of the loss\n",
    "    # as the original loss name.\n",
    "        l_name = l.name.replace(\":\", \"_\")\n",
    "\n",
    "        tf.summary.scalar(l_name + '_raw_', l)        \n",
    "        if include_averaged_loss:\n",
    "            tf.summary.scalar(l_name + '_raw_', l)\n",
    "            tf.summary.scalar(l_name, averager.average(l))\n",
    "        \n",
    "    if include_averaged_loss:\n",
    "        return loss_averages_op\n",
    "    else:\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \n",
    "    def __init__(self, args, infer=False):\n",
    "        if infer:\n",
    "            self.batch_size = 1\n",
    "            self.seq_length = 1\n",
    "        else:\n",
    "            self.batch_size = args.batch_size\n",
    "            self.seq_length = args.seq_length\n",
    "\n",
    "        self.x = tf.placeholder(tf.int32, shape=[args.batch_size, args.seq_length])\n",
    "        self.y_ = tf.placeholder(tf.int32, shape=[args.batch_size, args.seq_length])\n",
    "\n",
    "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(args.rnn_size)\n",
    "        self.lstm = tf.contrib.rnn.MultiRNNCell([self.lstm_cell]*args.num_layers)\n",
    "\n",
    "        self.initial_state = self.lstm.zero_state(args.batch_size, tf.float32)\n",
    "\n",
    "        with tf.variable_scope('lstm'):\n",
    "            W = tf.get_variable('W', [args.rnn_size, args.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "            b = tf.get_variable('b', [args.vocab_size], tf.float32, tf.constant_initializer(0.0))\n",
    "\n",
    "            # Define Embedding\n",
    "            embedding_mat = tf.get_variable('embedding', [args.vocab_size, args.rnn_size],\n",
    "                                            tf.float32, tf.random_normal_initializer())\n",
    "            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x)\n",
    "\n",
    "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.seq_length, value=embedding_output)\n",
    "            rnn_inputs = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "\n",
    "            def loop(prev, _):\n",
    "                prev = tf.matmul(prev, W) + b\n",
    "                prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "                return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "\n",
    "        outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(rnn_inputs,\n",
    "                                                                    self.initial_state,\n",
    "                                                                    self.lstm,\n",
    "                                                                    loop_function=loop if infer else None,\n",
    "                                                                    scope='lstm')\n",
    "        output = tf.reshape(tf.concat(outputs,1), [-1, args.rnn_size])\n",
    "\n",
    "        self.logits = tf.matmul(output, W) + b\n",
    "        self.softmax_p = tf.nn.softmax(self.logits)\n",
    "\n",
    "        self.loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "            [self.softmax_p],\n",
    "            [self.y_],\n",
    "            [tf.ones([args.batch_size * args.seq_length], dtype=tf.float32)]\n",
    "        )\n",
    "        tf.add_to_collection('losses', self.loss)\n",
    "        tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "    \n",
    "           # Generate moving averages of all losses and associated summaries.\n",
    "        loss_avgs = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "        losses = _add_loss_summaries(self.loss, loss_avgs, True)\n",
    "\n",
    "        # Compute gradients.\n",
    "        with tf.control_dependencies([losses]):\n",
    "            opt = tf.train.AdamOptimizer(args.learning_rate)\n",
    "            grads = opt.compute_gradients(self.loss)\n",
    "            trunc_grads = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in grads]\n",
    "\n",
    "\n",
    "        # Apply gradients.\n",
    "        apply_gradient_op = opt.apply_gradients(trunc_grads, global_step=global_step)\n",
    "\n",
    "        # Add histograms for trainable variables.\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "        # Add histograms for gradients.\n",
    "        for grad, var in trunc_grads:\n",
    "            if grad is not None:\n",
    "                tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "\n",
    "\n",
    "        if args.compute_variable_averages:\n",
    "            variable_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "            variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "            \n",
    "            with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "                self.train_op = tf.no_op(name='train')\n",
    "        else:\n",
    "            with tf.control_dependencies([apply_gradient_op]):\n",
    "                self.train_op = tf.no_op(name='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "\n",
    "vocab_file = \"vocab1.csv\"\n",
    "\n",
    "train_file = \"train1.csv\"\n",
    "\n",
    "model_path = 'VanillaLSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download/store Shakespeare data\n",
    "full_model_dir = os.path.join(data_path, model_path)\n",
    "\n",
    "# Make Model Directory\n",
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = pd.read_csv(\"{0}/{1}\".format(data_path, vocab_file),\n",
    "                    header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ArgStruct:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_dict = {\n",
    "    'data_path': data_path,\n",
    "    'model_path': model_path,\n",
    "    'rnn_size': 256,\n",
    "    'num_layers': 1,\n",
    "    'batch_size': 16,\n",
    "    'seq_length': 16,\n",
    "    'num_epochs': 1,\n",
    "    'learning_rate': 0.0001,\n",
    "    'momentum': 0.9,\n",
    "    'logdir': 'TF_Logs',\n",
    "    'vocab_size': len(vocab)+1,\n",
    "    'save_every': 100,\n",
    "    'print_every': 10,\n",
    "    'compute_variable_averages': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ArgStruct(**arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Moving average already computed for: sequence_loss_by_example/truediv:0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-70f30c4aa089>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mglobal_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'global_step'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     example_feed, label_feed = input_pipeline(\n",
      "\u001b[1;32m<ipython-input-6-7592d0acef97>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, infer)\u001b[0m\n\u001b[0;32m     55\u001b[0m            \u001b[1;31m# Generate moving averages of all losses and associated summaries.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mloss_avgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExponentialMovingAverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'avg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_add_loss_summaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_avgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m# Compute gradients.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-e1fb96453486>\u001b[0m in \u001b[0;36m_add_loss_summaries\u001b[1;34m(total_loss, averager, include_averaged_loss)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'losses'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minclude_averaged_loss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mloss_averages_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maverager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Attach a scalar summary to all individual losses and the total loss; do the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\moving_averages.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, var_list)\u001b[0m\n\u001b[0;32m    355\u001b[0m                         var.name)\n\u001b[0;32m    356\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_averages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Moving average already computed for: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m       \u001b[1;31m# For variables: to lower communication bandwidth across devices we keep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Moving average already computed for: sequence_loss_by_example/truediv:0"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    model = Model(args=args, infer=False)\n",
    "\n",
    "    example_feed, label_feed = input_pipeline(\n",
    "        [\"{0}/{1}\".format(args.data_path, train_file)],\n",
    "        batch_size=args.batch_size,\n",
    "        seq_length=args.seq_length,\n",
    "        num_epochs=args.num_epochs)\n",
    "    \n",
    "    with tf.Session() as sess: \n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(args.logdir, sess.graph)\n",
    "        \n",
    "        #initialize all variables\n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        # Start populating the filename queue.\n",
    "        coord = tf.train.Coordinator()  \n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "        \n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        \n",
    "        while not coord.should_stop():\n",
    "#             try:\n",
    "                start_time = time.time()                \n",
    "\n",
    "                example_batch, label_batch = sess.run([example_feed,\n",
    "                                                      label_feed])\n",
    "\n",
    "                result, summary =  sess.run(\n",
    "                    [model.train_op, merged],\n",
    "                    feed_dict={model.x: example_batch,\n",
    "                               model.y_: label_batch})\n",
    "                writer.add_summary(summary, global_step.eval())\n",
    "                \n",
    "                duration = time.time() - start_time\n",
    "                if global_step.eval() % args.print_every == 0:\n",
    "                    summary_nums = (global_step.eval(), duration, model.loss)\n",
    "                    print('Iteration: {}, Last Step Duration: {}, Loss: {:.2f}'.format(*summary_nums))\n",
    "        \n",
    "                # Save the model and the vocab\n",
    "                if global_step.eval() % args.save_every == 0:\n",
    "                    # Save model\n",
    "                    model_file_name = os.path.join(full_model_dir, 'model')\n",
    "                    saver.save(sess, model_file_name, global_step=global_step)\n",
    "                    print('Model Saved To: {}'.format(model_file_name))\n",
    "\n",
    "#             except (tf.errors.OutOfRangeError, tf.errors.InvalidArgumentError) as e:\n",
    "           \n",
    "#                 print('Done training for %d epochs, %d steps.' % (args.num_epochs, global_step.eval()))\n",
    "#                 # When done, ask the threads to stop.\n",
    "#                 coord.request_stop()\n",
    "\n",
    "        \n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
