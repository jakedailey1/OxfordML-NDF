{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#standard libraries\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "#custom libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file_format(filename_queue):\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    _, value = reader.read(filename_queue)\n",
    "\n",
    "    record_defaults = [tf.constant([], dtype=tf.float32), tf.constant([], dtype=tf.int32)]\n",
    "    _, col2 = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "    \n",
    "    example = tf.stack([col2])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_pipeline(filenames, batch_size = 3, seq_length=3,\n",
    "                   num_epochs = None, evaluation = False):   \n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        filenames, num_epochs=num_epochs, shuffle=False)\n",
    "\n",
    "    example = read_file_format(filename_queue)\n",
    "        \n",
    "    min_after_dequeue = 10\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch = tf.train.batch(\n",
    "        [example], batch_size=batch_size*seq_length, capacity=capacity\n",
    "    )    \n",
    "\n",
    "    label_batch = tf.concat(\n",
    "        [example_batch[-1], example_batch[1:,0]],\n",
    "        axis=0)\n",
    "\n",
    "    example_batch = tf.reshape(example_batch, (batch_size, seq_length))\n",
    "    label_batch = tf.reshape(label_batch, (batch_size, seq_length))\n",
    "\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _add_loss_summaries(total_loss, averager=None, include_averaged_loss=False):\n",
    "    losses = tf.get_collection('losses')\n",
    "    if include_averaged_loss:\n",
    "        loss_averages_op = averager.apply(losses + [total_loss])\n",
    "\n",
    "    for l in losses + [total_loss]:\n",
    "\n",
    "        l_name = l.name.replace(\":\", \"_\")\n",
    "\n",
    "        tf.summary.scalar(l_name + '_raw_', tf.reduce_sum(l))        \n",
    "        if include_averaged_loss:\n",
    "            tf.summary.scalar(l_name + '_raw_', l)\n",
    "            tf.summary.scalar(l_name, averager.average(l))\n",
    "        \n",
    "    if include_averaged_loss:\n",
    "        return loss_averages_op\n",
    "    else:\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nll(y_, logits, vocab_size):\n",
    "    y_ = tf.one_hot(tf.reshape(tf.cast(y_, tf.int32), [-1]), depth=vocab_size)\n",
    "    return tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logits), axis=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM_Cell:\n",
    "    \n",
    "    def __init__(self, args, scope_name, current_weights=None):\n",
    "        self.rnn_size = args.rnn_size\n",
    "        self.num_proj = args.vocab_size\n",
    "        self.input_size = args.batch_size * args.seq_length\n",
    "        self.state_size = self.rnn_size * 2        \n",
    "        \n",
    "        with tf.variable_scope(scope_name):\n",
    "            if args.weight_noise_type == None:\n",
    "                self.W = tf.get_variable('W', [self.input_size + self.num_proj, 4 * self.rnn_size],\n",
    "                                         tf.float32, tf.random_normal_initializer(), trainable=True)\n",
    "\n",
    "            elif args.weight_noise_type == \"static\":\n",
    "                self.W = tf.get_variable('W', [self.input_size + self.num_proj, 4 * self.rnn_size],\n",
    "                                         tf.float32, tf.random_normal_initializer(), trainable=True)\n",
    "                weight_noise = tf.truncated_normal([self.input_size + self.num_proj, 4 * self.rnn_size],\n",
    "                                                   stddev=args.weight_prior_variance)\n",
    "                self.W = self.W + weight_noise        \n",
    "\n",
    "            elif args.weight_noise_type == \"adaptive\":\n",
    "                self.W = tf.reshape(current_weights, [self.input_size + self.num_proj, 4 * self.rnn_size])\n",
    "\n",
    "\n",
    "            self.b = tf.get_variable('b', [1, self.rnn_size * 4], tf.float32,\n",
    "                                     tf.constant_initializer(0.0), trainable=True)\n",
    "        \n",
    "            \n",
    "    def __call__(self, i, state):\n",
    "        self.c_prev = tf.slice(state, [0, 0], [-1, self.rnn_size])\n",
    "        self.h_prev = tf.slice(state, [0, self.rnn_size], [-1, self.num_proj])\n",
    "\n",
    "        data = tf.concat([i, self.h_prev], 1)\n",
    "\n",
    "        weighted = tf.matmul(data, self.W)\n",
    "\n",
    "        self.i, self.j, self.f, self.o = tf.split(weighted, num_or_size_splits=4, axis=1)\n",
    "        self.i_b, self.j_b, _, self.o_b = tf.split(self.b, num_or_size_splits=4, axis=1)\n",
    "        \n",
    "        self.c = (tf.sigmoid(self.f + args.forget_bias) * self.c_prev +\n",
    "                  tf.sigmoid(self.i + self.i_b) * tf.tanh(self.j + self.j_b))\n",
    "        self.h = tf.sigmoid(self.o + self.o_b) * tf.tanh(self.c)\n",
    "        \n",
    "        self.state = tf.concat([self.c, self.h], axis=1)\n",
    "        return self.h, self.state\n",
    "    \n",
    "    def zero_state(self, batch_size, dtype):\n",
    "        return tf.zeros([batch_size, self.state_size], dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \n",
    "    def __init__(self, args):\n",
    "\n",
    "        self.batch_size = args.batch_size\n",
    "        self.seq_length = args.seq_length\n",
    "\n",
    "        self.x = tf.placeholder(tf.float32, shape=[args.batch_size, args.seq_length])\n",
    "        self.y_ = tf.placeholder(tf.float32, shape=[args.batch_size, args.seq_length])\n",
    "        \n",
    "        with tf.variable_scope(\"weights\"):\n",
    "            if args.weight_noise_type == \"adaptive\":\n",
    "                lstm_weight_size = (args.batch_size * args.seq_length + args.vocab_size) * (4 * args.rnn_size) * args.num_layers\n",
    "                softmax_weight_size = (args.rnn_size * args.vocab_size)\n",
    "                embedding_weight_size = (args.rnn_size * args.vocab_size)\n",
    "                weight_size = lstm_weight_size + softmax_weight_size + embedding_weight_size\n",
    "\n",
    "                prior_loc = [0.] * weight_size\n",
    "                prior_scale_diag = [args.weight_prior_variance] * weight_size\n",
    "                self.weight_prior = tf.contrib.distributions.Normal(\n",
    "                        prior_loc,\n",
    "                        prior_scale_diag\n",
    "                )\n",
    "                \n",
    "                S_hat = tf.get_variable(\"S_hat\", initializer=prior_scale_diag, trainable=True)\n",
    "                S = tf.exp(S_hat)\n",
    "\n",
    "                mu = tf.get_variable(\"mu\", initializer=prior_loc, trainable=True)\n",
    "\n",
    "                self.weight_dist = tf.contrib.distributions.Normal(mu, S)\n",
    "\n",
    "                self.weight_st = tf.contrib.bayesflow.stochastic_tensor.StochasticTensor(self.weight_dist)\n",
    "                \n",
    "                current_weights = tf.squeeze(self.weight_st, name=\"W\")\n",
    "\n",
    "                lstm_W, softmax_W, embedding_mat = tf.split(current_weights,\n",
    "                                                            num_or_size_splits=[lstm_weight_size,\n",
    "                                                                                softmax_weight_size,\n",
    "                                                                                embedding_weight_size])\n",
    "                lstm_W = tf.split(lstm_W, num_or_size_splits=args.num_layers)\n",
    "            \n",
    "            else:\n",
    "                lstm_W = [None for i in range(args.num_layers)]\n",
    "\n",
    "            self.lstm_cells = []\n",
    "            for i in range(args.num_layers):\n",
    "                self.lstm_cells.append(LSTM_Cell(args, \"lstm_{0}\".format(i), lstm_W[i]))\n",
    "            self.lstm = tf.contrib.rnn.MultiRNNCell(self.lstm_cells)\n",
    "\n",
    "            self.initial_state = self.lstm.zero_state(args.batch_size, tf.float32)\n",
    "\n",
    "            if args.weight_noise_type is None:\n",
    "                self.softmax_W = tf.get_variable('softmax_W', [args.rnn_size, args.vocab_size],\n",
    "                                            tf.float32, tf.random_normal_initializer(), trainable=True)\n",
    "                self.embedding_mat = tf.get_variable('embedding', [args.vocab_size, args.rnn_size],\n",
    "                                            tf.float32, tf.random_normal_initializer(), trainable=True)\n",
    "\n",
    "            elif args.weight_noise_type == \"static\":\n",
    "                softmax_W = tf.get_variable('softmax_W', [args.rnn_size, args.vocab_size],\n",
    "                                            tf.float32, tf.random_normal_initializer(), trainable=True)\n",
    "                softmax_noise = tf.truncated_normal([args.rnn_size, args.vocab_size],\n",
    "                                                    stddev=args.weight_prior_variance)\n",
    "                self.softmax_W = softmax_W + softmax_noise\n",
    "\n",
    "                embedding_mat = tf.get_variable('embedding', [args.vocab_size, args.rnn_size],\n",
    "                                            tf.float32, tf.random_normal_initializer(), trainable=True)\n",
    "                embedding_noise = tf.truncated_normal([args.vocab_size, args.rnn_size],\n",
    "                                                      stddev=args.weight_prior_variance)\n",
    "                self.embedding_mat = embedding_mat + embedding_noise\n",
    "\n",
    "            elif args.weight_noise_type == \"adaptive\":\n",
    "                softmax_W = tf.reshape(softmax_W, [args.rnn_size, args.vocab_size])\n",
    "                self.embedding_mat = tf.reshape(embedding_mat, [args.vocab_size, args.rnn_size])\n",
    "\n",
    "            else:\n",
    "                raise Exception(\"Unrecognized value for weight_noise_type; \" +\n",
    "                                \"recognized values are: None, 'static', and 'adaptive'.\")\n",
    "\n",
    "            self.b = tf.get_variable('b', [args.vocab_size], tf.float32,\n",
    "                                tf.constant_initializer(0.0), trainable=True)\n",
    "\n",
    "        embedding_output = tf.nn.embedding_lookup(self.embedding_mat, tf.cast(self.x, tf.int32))\n",
    "\n",
    "        rnn_inputs = tf.split(axis=1, num_or_size_splits=self.seq_length, value=embedding_output)\n",
    "        rnn_inputs = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "\n",
    "        outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(rnn_inputs,\n",
    "                                                                    self.initial_state,\n",
    "                                                                    self.lstm,\n",
    "                                                                    scope='lstm')\n",
    "        output = tf.reshape(tf.concat(outputs,1), [-1, args.rnn_size])\n",
    "\n",
    "        self.logits = tf.matmul(output, softmax_W) + self.b\n",
    "        \n",
    "        \n",
    "    def train(self, args):\n",
    "\n",
    "        if args.weight_noise_type is None:\n",
    "            self.y_ = tf.cast(self.y_, tf.int32)\n",
    "            self.loss = tf.contrib.legacy_seq2seq.sequence_loss(\n",
    "                logits=[self.logits],\n",
    "                targets=[self.y_],\n",
    "                weights=[tf.ones([args.batch_size * args.seq_length], dtype=tf.float32)]\n",
    "            )\n",
    "        else:\n",
    "            self.loss = tf.contrib.bayesflow.variational_inference.elbo(\n",
    "                nll(self.y_, self.logits, args.vocab_size),\n",
    "                {self.weight_st: self.weight_prior},\n",
    "                keep_batch_dim=True\n",
    "            )\n",
    "            \n",
    "        tf.add_to_collection('losses', self.loss)\n",
    "        tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "                \n",
    "        opt = tf.train.AdamOptimizer(args.learning_rate)\n",
    "\n",
    "        if args.weight_noise_type in [None, \"static\"]:\n",
    "            grads = opt.compute_gradients(self.loss)\n",
    "            trunc_grads = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in grads]\n",
    "            apply_gradient_op = [opt.apply_gradients(trunc_grads, global_step=global_step)]\n",
    "            \n",
    "            for grad, var in trunc_grads:\n",
    "                if grad is not None:\n",
    "                    tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "\n",
    "        else: \n",
    "            with tf.variable_scope(\"weights\", reuse=True):\n",
    "                means = tf.get_variable(\"mu\")\n",
    "                variances = tf.get_variable(\"S_hat\")\n",
    "                biases = [tf.get_variable(\"b\")]\n",
    "                for i in range(args.num_layers):\n",
    "                    with tf.variable_scope(\"lstm_{0}\".format(i)):\n",
    "                        biases.append(tf.get_variable(\"b\"))               \n",
    "\n",
    "            var_grads = opt.compute_gradients(self.loss, var_list=variances)\n",
    "            self.trunc_var_grads = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in var_grads]\n",
    "            apply_var_grad_op = opt.apply_gradients(self.trunc_var_grads, global_step=global_step)\n",
    "\n",
    "            mean_grads = opt.compute_gradients(self.loss, var_list=means)\n",
    "            self.trunc_mean_grads = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in mean_grads]\n",
    "            apply_mean_grad_op = opt.apply_gradients(self.trunc_mean_grads, global_step=global_step)\n",
    "            \n",
    "            self.bias_grads = opt.compute_gradients(self.loss, var_list=biases)\n",
    "            apply_bias_grad_op = opt.apply_gradients(self.bias_grads, global_step=global_step)\n",
    "            \n",
    "            apply_gradient_op = [apply_mean_grad_op] + [apply_var_grad_op] + [apply_bias_grad_op]\n",
    "            \n",
    "            for grad, var in self.trunc_mean_grads:\n",
    "                if grad is not None:\n",
    "                    tf.summary.histogram(var.op.name + '/mean_gradients', grad)\n",
    "            for grad, var in self.trunc_var_grads:\n",
    "                if grad is not None:\n",
    "                    tf.summary.histogram(var.op.name + '/variance_gradients', grad)\n",
    "\n",
    "        for var in tf.global_variables():\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "        if args.compute_variable_averages:\n",
    "            moving_averager = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "            variables_averages_op = moving_averager.apply(tf.trainable_variables())\n",
    "            _add_loss_summaries(model.loss, moving_averager, args.compute_variable_averages)\n",
    "\n",
    "            with tf.control_dependencies(apply_gradient_op + [variables_averages_op]):\n",
    "                self.train_op = tf.no_op(name='train')\n",
    "        else:\n",
    "            _add_loss_summaries(model.loss)\n",
    "            with tf.control_dependencies(apply_gradient_op):\n",
    "                self.train_op = tf.no_op(name='train')\n",
    "        \n",
    "        return self.train_op\n",
    "\n",
    "    \n",
    "    def sample(self):\n",
    "        flat_y_ = tf.expand_dims(tf.reshape(self.y_, [-1]), 1)\n",
    "        flat_y_ = tf.cast(flat_y_, tf.int64)\n",
    "        softmax = tf.nn.softmax(self.logits)\n",
    "        samples = tf.multinomial(softmax, 1)\n",
    "        \n",
    "        self.sampled_results = tf.concat([flat_y_, samples], axis=1)\n",
    "        return self.sampled_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "\n",
    "vocab_file = \"vocab1.csv\"\n",
    "\n",
    "train_file = \"train1.csv\"\n",
    "\n",
    "model_path = 'VanillaLSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download/store Shakespeare data\n",
    "full_model_dir = os.path.join(data_path, model_path)\n",
    "\n",
    "# Make Model Directory\n",
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = pd.read_csv(\"{0}/{1}\".format(data_path, vocab_file),\n",
    "                    header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ArgStruct:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arg_dict = {\n",
    "    'data_path': data_path,\n",
    "    'model_path': model_path,\n",
    "    'rnn_size': 256,\n",
    "    'num_layers': 2,\n",
    "    'batch_size': 16,\n",
    "    'seq_length': 16,\n",
    "    'forget_bias': 1.,\n",
    "    'num_epochs': 1,\n",
    "    'learning_rate': 0.0001,\n",
    "    'momentum': 0.9,\n",
    "    'logdir': 'TF_Logs',\n",
    "    'vocab_size': len(vocab)+1,\n",
    "    'save_every': 1000,\n",
    "    'print_every': 250,\n",
    "    'compute_variable_averages': True,\n",
    "    'weight_noise_type': \"adaptive\",\n",
    "    'weight_prior_variance': 0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = ArgStruct(**arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    \n",
    "    global global_step\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "    example_feed, label_feed = input_pipeline(\n",
    "        [\"{0}/{1}\".format(args.data_path, train_file)],\n",
    "        batch_size=args.batch_size,\n",
    "        seq_length=args.seq_length,\n",
    "        num_epochs=args.num_epochs)\n",
    "    \n",
    "    with tf.Session().as_default() as sess:\n",
    "        print(\"Adding model to graph.\")\n",
    "        model = Model(args=args)\n",
    "        \n",
    "        print(\"Adding training and sampling ops to graph.\")\n",
    "        train_op = model.train(args=args)\n",
    "        sample_op = model.sample()\n",
    "        \n",
    "        writer = tf.summary.FileWriter(args.logdir, sess.graph)\n",
    "        \n",
    "        config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "        \n",
    "        embedding = config.embeddings.add()\n",
    "        embedding.tensor_name = model.embedding_mat.name\n",
    "        embedding.metadata_path = os.path.join(args.logdir, 'metadata.tsv')\n",
    "\n",
    "\n",
    "        tf.contrib.tensorboard.plugins.projector.visualize_embeddings(writer, config)\n",
    "        \n",
    "        merged = tf.summary.merge_all()\n",
    "        \n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        saver.save(sess, os.path.join(full_model_dir, \"model.ckpt\"), global_step.eval())\n",
    "        \n",
    "        coord = tf.train.Coordinator()  \n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "                \n",
    "        global queue_stopped\n",
    "        queue_stopped = coord.should_stop()\n",
    "        \n",
    "        while not coord.should_stop():\n",
    "            try:\n",
    "                start_time = time.time()                \n",
    "\n",
    "                example_batch, label_batch = sess.run([example_feed,\n",
    "                                                      label_feed])\n",
    "                \n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                \n",
    "                result, summary = sess.run(\n",
    "                    [train_op, merged],\n",
    "                    feed_dict={model.x: example_batch,\n",
    "                               model.y_: label_batch},\n",
    "                    options=run_options,\n",
    "                    run_metadata=run_metadata\n",
    "                )\n",
    "\n",
    "                writer.add_summary(summary, global_step.eval())\n",
    "                writer.add_run_metadata(run_metadata,\n",
    "                                        tag=\"step{0}\".format(global_step.eval()),\n",
    "                                        global_step=global_step.eval())\n",
    "            \n",
    "                if global_step.eval() % args.print_every == 0:\n",
    "                    latest_loss = sess.run([model.loss],\n",
    "                                           feed_dict={model.x: example_batch,\n",
    "                                                      model.y_: label_batch})\n",
    "                    try:\n",
    "                        summary_nums = (global_step.eval(), duration,\n",
    "                                        np.mean(latest_loss))\n",
    "                        print('Iteration: {0}, Last Step Duration: {1}, Loss: {2}'.format(*summary_nums))\n",
    "                    except:\n",
    "                        pass\n",
    "                    results = sess.run([sample_op],\n",
    "                                       feed_dict={model.x: example_batch,\n",
    "                                                  model.y_: label_batch})\n",
    "                    results = pd.DataFrame(results[0]).T\n",
    "                    recode_results = results.replace(vocab.set_index(1).to_dict().get(0))\n",
    "                    if not os.path.exists(\"translation.txt\"):\n",
    "                        recode_results.to_csv(\"translation.txt\", header=False, index=False, sep=\"\\t\", mode=\"w\")\n",
    "                    else:\n",
    "                        recode_results.to_csv(\"translation.txt\", header=False, index=False, sep=\"\\t\", mode=\"a\")\n",
    "\n",
    "\n",
    "                # Save the model and the vocab\n",
    "                if global_step.eval() % args.save_every == 0:\n",
    "                    # Save model\n",
    "                    model_file_name = os.path.join(full_model_dir, 'model.ckpt')\n",
    "                    saver.save(sess, model_file_name, global_step=global_step)\n",
    "                    print('Model Saved To: {}'.format(model_file_name))\n",
    "                    \n",
    "                queue_stopped = coord.should_stop()\n",
    "                    \n",
    "                duration = time.time() - start_time\n",
    "                \n",
    "                writer.flush()\n",
    "\n",
    "            except (tf.errors.OutOfRangeError, tf.errors.InvalidArgumentError) as e:\n",
    "           \n",
    "                print('Done training for %d epochs, %d steps.' % (args.num_epochs, global_step.eval()))\n",
    "                # When done, ask the threads to stop.\n",
    "                coord.request_stop()\n",
    "                queue_stopped = coord.should_stop()\n",
    "\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
