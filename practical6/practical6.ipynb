{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard libraries\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "#custom libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file_format(filename_queue):\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    _, value = reader.read(filename_queue)\n",
    "\n",
    "    record_defaults = [tf.constant([], dtype=tf.int32), tf.constant([], dtype=tf.int32)]\n",
    "    _, col2 = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "    \n",
    "    example = tf.stack([col2])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_pipeline(filenames, batch_size = 3, seq_length=3,\n",
    "                   num_epochs = None, evaluation = False):   \n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        filenames, num_epochs=num_epochs, shuffle=False)\n",
    "\n",
    "    example = read_file_format(filename_queue)\n",
    "        \n",
    "    min_after_dequeue = 10\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch = tf.train.batch(\n",
    "        [example], batch_size=batch_size*seq_length, capacity=capacity\n",
    "    )    \n",
    "\n",
    "    label_batch = tf.concat(\n",
    "        [example_batch[-1], example_batch[1:,0]],\n",
    "        axis=0)\n",
    "\n",
    "    example_batch = tf.reshape(example_batch, (batch_size, seq_length))\n",
    "    label_batch = tf.reshape(label_batch, (batch_size, seq_length))\n",
    "\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _activation_summary(x):\n",
    "    tensor_name = x.name\n",
    "    tensor_name = tensor_name.replace(':', '_')\n",
    "    tensor_name = tensor_name.replace('(', '_')\n",
    "    tensor_name = tensor_name.replace(')', '_')\n",
    "    tensor_name = tensor_name.replace(' ', '_')\n",
    "\n",
    "    tf.summary.histogram(tensor_name + '/activations', x)\n",
    "    tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_loss_summaries(total_loss, averager, include_averaged_loss=False):\n",
    "    # Compute the moving average of all individual losses and the total loss.\n",
    "    losses = tf.get_collection('losses')\n",
    "    if include_averaged_loss:\n",
    "        loss_averages_op = averager.apply(losses + [total_loss])\n",
    "\n",
    "    # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "    # same for the averaged version of the losses.\n",
    "    for l in losses + [total_loss]:\n",
    "    # Name each loss as '(raw)' and name the moving average version of the loss\n",
    "    # as the original loss name.\n",
    "        l_name = l.name.replace(\":\", \"_\")\n",
    "\n",
    "        tf.summary.scalar(l_name + '_raw_', l)        \n",
    "        if include_averaged_loss:\n",
    "            tf.summary.scalar(l_name + '_raw_', l)\n",
    "            tf.summary.scalar(l_name, averager.average(l))\n",
    "        \n",
    "    if include_averaged_loss:\n",
    "        return loss_averages_op\n",
    "    else:\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kl_divergence(P, Q, sample_size=1000): \n",
    "    '''\n",
    "    Calculate KL Divergence between a reference distribution P and an approximating\n",
    "    distribution Q using values drawn from the two distributions.\n",
    "    \n",
    "    P and Q are used here per canonical IT notation, giving KL(P||Q).\n",
    "\n",
    "    Note that the typical approach in variational inference minimizes KL(Q||P),\n",
    "    where the variational distribution Q is the reference distribution and\n",
    "    P is the approximating distribution.\n",
    "    \n",
    "    Args:\n",
    "    P: A tf.contrib.distributions.Distribution for reference distribution\n",
    "    Q: A tf.contrib.distributions.Distribution for approximating distribution\n",
    "\n",
    "    '''\n",
    "    P_vals = P.sample([sample_size])\n",
    "    Q_vals = Q.sample([sample_size])\n",
    "\n",
    "    return tf.reduce_sum(tf.multiply(P_vals, tf.log(tf.div(P_vals, Q_vals))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variational_free_energy(y_, y_hat, weight_dist, weight_prior):\n",
    "    '''\n",
    "    Calculation variational free energy or evidence lower bound loss.\n",
    "    \n",
    "    It can be shown that the variational free energy is equivalent to the\n",
    "    sum of the expected log loss of the network with w ~ Q(w) and the KL \n",
    "    Divergence of the variational distribution Q(w) from the weight prior P(w).\n",
    "    '''\n",
    "    \n",
    "    return tf.reduce_sum([tf.losses.log_loss(y_, y_hat), kl_divergence(weight_dist, weight_prior)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \n",
    "    def __init__(self, args, infer=False):\n",
    "        if infer:\n",
    "            self.batch_size = 1\n",
    "            self.seq_length = 1\n",
    "        else:\n",
    "            self.batch_size = args.batch_size\n",
    "            self.seq_length = args.seq_length\n",
    "\n",
    "        self.x = tf.placeholder(tf.int32, shape=[args.batch_size, args.seq_length])\n",
    "        self.y_ = tf.placeholder(tf.int32, shape=[args.batch_size, args.seq_length])\n",
    "\n",
    "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(args.rnn_size)\n",
    "        self.lstm = tf.contrib.rnn.MultiRNNCell([self.lstm_cell]*args.num_layers)\n",
    "\n",
    "        self.initial_state = self.lstm.zero_state(args.batch_size, tf.float32)\n",
    "\n",
    "        with tf.variable_scope('lstm'):\n",
    "            if args.weight_noise_type is None:\n",
    "                W = tf.get_variable('W', [args.rnn_size, args.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "\n",
    "            elif args.weight_noise_type == \"static\":\n",
    "                W = tf.get_variable('W', [args.rnn_size, args.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "                noise = tf.truncated_normal([args.rnn_size, args.vocab_size], stddev=0.05)\n",
    "                W = W + noise\n",
    "                \n",
    "            elif args.weight_noise_type == \"adaptive\":\n",
    "                prior_loc = [0.]*(args.rnn_size*args.vocab_size)\n",
    "                prior_scale_diag = [args.weight_prior_variance]*(args.rnn_size*args.vocab_size)\n",
    "                self.weight_prior =tf.contrib.distributions.MultivariateNormalDiag(\n",
    "                    prior_loc,\n",
    "                    prior_scale_diag\n",
    "                )\n",
    "                \n",
    "                S_hat = tf.get_variable(\"S_hat\", initializer=prior_scale_diag)\n",
    "                S = tf.exp(S_hat) # make sure sigma matrix is positive\n",
    "\n",
    "                mu = tf.get_variable(\"mu\", initializer=prior_loc)\n",
    "                \n",
    "                self.weight_dist = tf.contrib.distributions.MultivariateNormalDiag(mu, S) # draw each weight from a Gaussian distribution\n",
    "                W = self.weight_dist.sample(sample_shape=(1))\n",
    "                W = tf.reshape(W, [args.rnn_size, args.vocab_size])\n",
    "            else:\n",
    "                raise Exception(\"Unrecognized value for weight_noise_type; \" +\n",
    "                                \"recognized values are: None, 'static', and 'adaptive'.\")\n",
    "            \n",
    "            b = tf.get_variable('b', [args.vocab_size], tf.float32, tf.constant_initializer(0.0))\n",
    "\n",
    "            # Define Embedding\n",
    "            embedding_mat = tf.get_variable('embedding', [args.vocab_size, args.rnn_size],\n",
    "                                            tf.float32, tf.random_normal_initializer())\n",
    "            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x)\n",
    "\n",
    "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.seq_length, value=embedding_output)\n",
    "            rnn_inputs = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "\n",
    "            def loop(prev, _):\n",
    "                prev = tf.matmul(prev, W) + b\n",
    "                prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "                return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "\n",
    "        outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(rnn_inputs,\n",
    "                                                                    self.initial_state,\n",
    "                                                                    self.lstm,\n",
    "                                                                    loop_function=loop if infer else None,\n",
    "                                                                    scope='lstm')\n",
    "        output = tf.reshape(tf.concat(outputs,1), [-1, args.rnn_size])\n",
    "\n",
    "        self.logits = tf.matmul(output, W) + b\n",
    "        self.softmax_p = tf.nn.softmax(self.logits)\n",
    "\n",
    "        if args.weight_noise_type is None:\n",
    "            self.loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "                [self.softmax_p],\n",
    "                [self.y_],\n",
    "                [tf.ones([args.batch_size * args.seq_length], dtype=tf.float32)]\n",
    "            )\n",
    "        else:\n",
    "            self.loss = variational_free_energy(\n",
    "                self.y_,\n",
    "                self.softmax_p,\n",
    "                self.weight_dist,\n",
    "                self.weight_prior\n",
    "            )\n",
    "        tf.add_to_collection('losses', self.loss)\n",
    "        tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "    \n",
    "#            # Generate moving averages of all losses and associated summaries.\n",
    "#         loss_avgs = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "#         losses = _add_loss_summaries(self.loss, loss_avgs, True)\n",
    "\n",
    "        # Compute gradients.\n",
    "#         with tf.control_dependencies([losses]):\n",
    "        opt = tf.train.AdamOptimizer(args.learning_rate)\n",
    "        grads = opt.compute_gradients(self.loss)\n",
    "        trunc_grads = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in grads]\n",
    "\n",
    "\n",
    "        # Apply gradients.\n",
    "        apply_gradient_op = opt.apply_gradients(trunc_grads, global_step=global_step)\n",
    "\n",
    "        # Add histograms for trainable variables.\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "        # Add histograms for gradients.\n",
    "        for grad, var in trunc_grads:\n",
    "            if grad is not None:\n",
    "                tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "\n",
    "\n",
    "        if args.compute_variable_averages:\n",
    "            variable_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "            variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "            \n",
    "            with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "                self.train_op = tf.no_op(name='train')\n",
    "        else:\n",
    "            with tf.control_dependencies([apply_gradient_op]):\n",
    "                self.train_op = tf.no_op(name='train')\n",
    "        \n",
    "        \n",
    "    def sample(self):\n",
    "        flat_y_ = tf.expand_dims(tf.reshape(self.y_, [-1]), 1)\n",
    "        flat_y_ = tf.cast(flat_y_, tf.int64)\n",
    "        samples = tf.multinomial(self.softmax_p, 1)\n",
    "        \n",
    "        self.sampled_results = tf.concat([flat_y_, samples], axis=1)\n",
    "        return self.sampled_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "\n",
    "vocab_file = \"vocab1.csv\"\n",
    "\n",
    "train_file = \"train1.csv\"\n",
    "\n",
    "model_path = 'VanillaLSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download/store Shakespeare data\n",
    "full_model_dir = os.path.join(data_path, model_path)\n",
    "\n",
    "# Make Model Directory\n",
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = pd.read_csv(\"{0}/{1}\".format(data_path, vocab_file),\n",
    "                    header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ArgStruct:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_dict = {\n",
    "    'data_path': data_path,\n",
    "    'model_path': model_path,\n",
    "    'rnn_size': 256,\n",
    "    'num_layers': 5,\n",
    "    'batch_size': 16,\n",
    "    'seq_length': 16,\n",
    "    'num_epochs': 1,\n",
    "    'learning_rate': 0.0001,\n",
    "    'momentum': 0.9,\n",
    "    'logdir': 'TF_Logs',\n",
    "    'vocab_size': len(vocab)+1,\n",
    "    'save_every': 100,\n",
    "    'print_every': 10,\n",
    "    'compute_variable_averages': True,\n",
    "    'weight_noise_type': \"adaptive\",\n",
    "    'weight_prior_variance': 0.05\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ArgStruct(**arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes (256, 36) and (16, 16) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-ce4550f6bdd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mglobal_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'global_step'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     example_feed, label_feed = input_pipeline(\n",
      "\u001b[1;32m<ipython-input-52-589f3e3aad3e>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, infer)\u001b[0m\n\u001b[0;32m     81\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_p\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_dist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_prior\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             )\n\u001b[0;32m     85\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'losses'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-24bfdfe50848>\u001b[0m in \u001b[0;36mvariational_free_energy\u001b[1;34m(y_, y_hat, weight_dist, weight_prior)\u001b[0m\n\u001b[0;32m      8\u001b[0m     '''\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkl_divergence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_prior\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py\u001b[0m in \u001b[0;36mlog_loss\u001b[1;34m(labels, predictions, weights, epsilon, scope, loss_collection)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_float\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_float\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m     \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m     losses = -math_ops.multiply(\n\u001b[0;32m    310\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    733\u001b[0m     \"\"\"\n\u001b[0;32m    734\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 735\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mis_fully_defined\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes (256, 36) and (16, 16) are incompatible"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    model = Model(args=args, infer=False)\n",
    "\n",
    "    example_feed, label_feed = input_pipeline(\n",
    "        [\"{0}/{1}\".format(args.data_path, train_file)],\n",
    "        batch_size=args.batch_size,\n",
    "        seq_length=args.seq_length,\n",
    "        num_epochs=args.num_epochs)\n",
    "    \n",
    "    with tf.Session() as sess: \n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(args.logdir, sess.graph)\n",
    "        \n",
    "        #initialize all variables\n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        # Start populating the filename queue.\n",
    "        coord = tf.train.Coordinator()  \n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "        \n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        \n",
    "        while not coord.should_stop():\n",
    "            try:\n",
    "                start_time = time.time()                \n",
    "\n",
    "                example_batch, label_batch = sess.run([example_feed,\n",
    "                                                      label_feed])\n",
    "                \n",
    "                if global_step.eval() % args.print_every == 0:\n",
    "                    latest_loss = sess.run([model.loss],\n",
    "                                           feed_dict={model.x: example_batch,\n",
    "                                                      model.y_: label_batch})\n",
    "                    try:\n",
    "                        summary_nums = (global_step.eval(), duration,\n",
    "                                        np.mean(latest_loss))\n",
    "                        print('Iteration: {0}, Last Step Duration: {1}, Loss: {2}'.format(*summary_nums))\n",
    "                    except:\n",
    "                        pass\n",
    "                    sampled_results = model.sample()\n",
    "                    results = sess.run([model.sampled_results],\n",
    "                                       feed_dict={model.x: example_batch,\n",
    "                                                  model.y_: label_batch})\n",
    "                    results = pd.DataFrame(results[0]).T\n",
    "                    recode_results = results.replace(vocab.set_index(1).to_dict().get(0))\n",
    "                    if not os.path.exists(\"translation.txt\"):\n",
    "                        recode_results.to_csv(\"translation.txt\", header=False, index=False, sep=\"\\t\", mode=\"w\")\n",
    "                    else:\n",
    "                        recode_results.to_csv(\"translation.txt\", header=False, index=False, sep=\"\\t\", mode=\"a\")\n",
    "\n",
    "                result, summary =  sess.run(\n",
    "                    [model.train_op, merged],\n",
    "                    feed_dict={model.x: example_batch,\n",
    "                               model.y_: label_batch})\n",
    "                writer.add_summary(summary, global_step.eval())\n",
    "                \n",
    "                duration = time.time() - start_time\n",
    "        \n",
    "                # Save the model and the vocab\n",
    "                if global_step.eval() % args.save_every == 0:\n",
    "                    # Save model\n",
    "                    model_file_name = os.path.join(full_model_dir, 'model')\n",
    "                    saver.save(sess, model_file_name, global_step=global_step)\n",
    "                    print('Model Saved To: {}'.format(model_file_name))\n",
    "\n",
    "            except (tf.errors.OutOfRangeError, tf.errors.InvalidArgumentError) as e:\n",
    "           \n",
    "                print('Done training for %d epochs, %d steps.' % (args.num_epochs, global_step.eval()))\n",
    "                # When done, ask the threads to stop.\n",
    "                coord.request_stop()\n",
    "\n",
    "        \n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
