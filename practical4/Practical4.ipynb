{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"iris.data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(filename) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "        data_len = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def datasplitter(filename):\n",
    "    train_rows = 0\n",
    "    valid_rows = 0\n",
    "    test_rows = 0\n",
    "    \n",
    "    if os.path.exists('iristrain'):\n",
    "        shutil.rmtree('iristrain')\n",
    "    os.makedirs('iristrain')\n",
    "    \n",
    "    \n",
    "    if os.path.exists('irisvalid'):\n",
    "        shutil.rmtree('irisvalid')\n",
    "    os.makedirs('irisvalid')\n",
    "    \n",
    "    if os.path.exists('iristest'):\n",
    "        shutil.rmtree('iristest')\n",
    "    os.makedirs('iristest')\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            if len(row)>0:\n",
    "                filtered = (line.replace('\\n', '') for line in row)\n",
    "                filtered = (line.replace(\"\\r\", \"\") for line in filtered)\n",
    "                filtered = (line.replace(\"'\", \"\") for line in filtered)\n",
    "                row = filtered\n",
    "\n",
    "                group = np.random.multinomial(1,[.6,.1,.3])\n",
    "                if np.argmax(group)==0:\n",
    "                    with open('iristrain/iristrain{0}.csv'.format(train_rows), 'w+', newline = None) as trainfile:\n",
    "                        trainwriter = csv.writer(trainfile, delimiter=',')\n",
    "                        trainwriter.writerow(row)\n",
    "                    train_rows += 1\n",
    "\n",
    "\n",
    "                if np.argmax(group)==1:\n",
    "                    with open('irisvalid/irisvalid{0}.csv'.format(valid_rows), 'w+', newline = None) as validfile:\n",
    "                        validwriter = csv.writer(validfile, delimiter=',')\n",
    "                        validwriter.writerow(row)\n",
    "                    valid_rows += 1\n",
    "\n",
    "                if np.argmax(group)==2:\n",
    "                    with open('iristest/iristest{0}.csv'.format(test_rows), 'w+', newline = None) as testfile:\n",
    "                        testwriter = csv.writer(testfile, delimiter=',')\n",
    "                        testwriter.writerow(row)\n",
    "                    test_rows += 1\n",
    "                    \n",
    "    return train_rows, valid_rows, test_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_len, valid_len, test_len = datasplitter(filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_file_format(filename_queue, possible_labels):\n",
    "    reader = tf.TextLineReader()\n",
    "    _, value = reader.read(filename_queue)\n",
    "\n",
    "    record_defaults = [tf.constant([],dtype=tf.float32), tf.constant([],dtype=tf.float32),\n",
    "                       tf.constant([],dtype=tf.float32), tf.constant([],dtype=tf.float32),\n",
    "                       tf.constant([],dtype=tf.float32), tf.constant([],dtype=tf.string)]\n",
    "    col1, col2, col3, col4, col5, col6 = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "    \n",
    "    example = tf.stack([col1, col2, col3, col4, col5])\n",
    "    label = tf.one_hot(tf.where(tf.equal(possible_labels, col6))[0], depth = possible_labels.shape[0], on_value = 1, off_value = 0)\n",
    "    label = label[0]\n",
    "    return example, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def input_pipeline(filenames, possible_labels, batch_size = 3, num_epochs = None, evaluation = False):   \n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        train_set, num_epochs=num_epochs, shuffle=True)\n",
    "    \n",
    "    example, label = read_file_format(filename_queue, possible_labels)\n",
    "\n",
    "#     # min_after_dequeue defines how big a buffer we will randomly sample\n",
    "#     #   from -- bigger means better shuffling but slower start up and more\n",
    "#     #   memory used.\n",
    "#     # capacity must be larger than min_after_dequeue and the amount larger\n",
    "#     #   determines the maximum we will prefetch.  Recommendation:\n",
    "#     #   min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
    "    min_after_dequeue = 10\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch, label_batch = tf.train.shuffle_batch(\n",
    "      [example, label], batch_size=batch_size,capacity = capacity, \n",
    "        min_after_dequeue = min_after_dequeue\n",
    "    )\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = ['iristrain/iristrain{0}.csv'.format(i) for i in range(train_len)]\n",
    "valid_set = ['irisvalid/irisvalid{0}.csv'.format(i) for i in range(valid_len)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(train_len):\n",
    "    if i == 0:\n",
    "        debug = pd.read_csv('iristrain/iristrain{0}.csv'.format(i), header = None)\n",
    "    else:\n",
    "        temp = pd.read_csv('iristrain/iristrain{0}.csv'.format(i), header = None)\n",
    "        debug = debug.append(temp)\n",
    "debug_ex = debug.ix[:,:4]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_ex = debug_ex.iloc[0:0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training for 1 epochs, 33 steps.\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    \n",
    "    unique_labels = tf.constant([\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"], dtype=tf.string)\n",
    "    \n",
    "    example_feed, labels_feed = input_pipeline(train_set, possible_labels = unique_labels,\n",
    "                                               batch_size = batch_size, num_epochs = num_epochs)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Start populating the filename queue.\n",
    "        \n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()  \n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "        \n",
    "        step = 0\n",
    "        while not coord.should_stop():\n",
    "            try:\n",
    "                start_time = time.time()                \n",
    "               \n",
    "                example_batch, label_batch = sess.run([example_feed, labels_feed])\n",
    "                \n",
    "                pipe_ex = pipe_ex.append(pd.DataFrame(example_batch))\n",
    "                \n",
    "                duration = time.time() - start_time\n",
    "                step += 1\n",
    "\n",
    "            except (tf.errors.OutOfRangeError, tf.errors.InvalidArgumentError) as e:\n",
    "           \n",
    "                print('Done training for %d epochs, %d steps.' % (num_epochs, step))\n",
    "                # When done, ask the threads to stop.\n",
    "                coord.request_stop()\n",
    "\n",
    "        \n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_ex.ix[:,0].astype(float).reset_index(drop=True).equals((pipe_ex.sort_values(0).reset_index(drop=True).ix[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost_tracker = pd.DataFrame([None,None,None,None],\n",
    "#                                   index = ['Iteration','Training Cost','Training Precision', 'Validation Cost']).T\n",
    "# iter_count = 0\n",
    "# batch_losses = []\n",
    "# batch_accs = []\n",
    "# batch_debug = []\n",
    "# batch_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training for 1 epochs, 33 steps.\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "    y_ = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "    \n",
    "    W1 = tf.Variable(tf.random_normal([4,3]), name='weights', trainable=True)\n",
    "    b1 = tf.Variable(tf.random_normal([3]), name = 'bias', trainable = True)\n",
    "    lin_y1 = tf.matmul(x,W1) + b1\n",
    "    sig_y1 = tf.sigmoid(lin_y1)\n",
    "    \n",
    "    W2 = tf.Variable(tf.random_normal([3,3]), name='weights', trainable=True)\n",
    "    b2 = tf.Variable(tf.random_normal([3]), name = 'bias', trainable = True)\n",
    "    lin_y2 = tf.matmul(sig_y1,W2) + b2\n",
    "    \n",
    "    smax_num = tf.transpose(tf.exp(lin_y2-tf.reduce_max(lin_y2)))\n",
    "    smax_den = tf.reduce_sum(tf.exp(lin_y2-tf.reduce_max(lin_y2)),-1)\n",
    "    softmax_y2 = tf.transpose(tf.divide(smax_num, smax_den))\n",
    "    \n",
    "    NLLCriterion = -tf.reduce_mean(tf.reduce_sum(tf.multiply(y_,tf.log(softmax_y2+1e-10)),axis=1))\n",
    "    learning_rate = .1\n",
    "    momentum = .2\n",
    "    train_step = tf.train.MomentumOptimizer(learning_rate,momentum).minimize(NLLCriterion)\n",
    "    \n",
    "    unique_labels = tf.constant([\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"], dtype=tf.string)\n",
    "    \n",
    "    example_feed, labels_feed = input_pipeline(train_set, possible_labels = unique_labels,\n",
    "                                               batch_size = batch_size, num_epochs = num_epochs)\n",
    "    v_example_feed, v_labels_feed = input_pipeline(valid_set, possible_labels = unique_labels,\n",
    "                                               batch_size = valid_len, num_epochs = 1)\n",
    "    \n",
    "    correct = tf.equal(tf.argmax(softmax_y2,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Start populating the filename queue.\n",
    "        \n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()  \n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "        \n",
    "        step = 0\n",
    "        while not coord.should_stop():\n",
    "            try:\n",
    "                start_time = time.time()                \n",
    "               \n",
    "                example_batch, label_batch = sess.run([example_feed, labels_feed])\n",
    "                example_batch = example_batch[:,1:]\n",
    "                \n",
    "                sess.run(train_step, feed_dict = {x: example_batch, y_: label_batch})\n",
    "\n",
    "                duration = time.time() - start_time\n",
    "                step += 1\n",
    "\n",
    "            except (tf.errors.OutOfRangeError, tf.errors.InvalidArgumentError) as e:\n",
    "                \n",
    "                valid_examples, valid_labels = sess.run([v_example_feed, v_labels_feed])\n",
    "                valid_examples = valid_examples[:,1:]\n",
    "                valid_cost = sess.run(NLLCriterion, feed_dict = {x: valid_examples, y_: valid_labels})\n",
    "                acc = sess.run(accuracy, feed_dict = {x: valid_examples, y_: valid_labels})\n",
    "\n",
    "                \n",
    "                print('Done training for %d epochs, %d steps.' % (num_epochs, step))\n",
    "                # When done, ask the threads to stop.\n",
    "                coord.request_stop()\n",
    "\n",
    "        \n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84150225"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61538464"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
