{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"iris.data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming in and splitting example/label pairs in to training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def datasplitter(filename):\n",
    "    train_rows = 0\n",
    "    valid_rows = 0\n",
    "    test_rows = 0\n",
    "    \n",
    "    if os.path.exists('iristrain'):\n",
    "        shutil.rmtree('iristrain')\n",
    "    os.makedirs('iristrain')\n",
    "    \n",
    "    \n",
    "    if os.path.exists('irisvalid'):\n",
    "        shutil.rmtree('irisvalid')\n",
    "    os.makedirs('irisvalid')\n",
    "    \n",
    "    if os.path.exists('iristest'):\n",
    "        shutil.rmtree('iristest')\n",
    "    os.makedirs('iristest')\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            if len(row)>0:\n",
    "                filtered = (line.replace('\\n', '') for line in row)\n",
    "                filtered = (line.replace(\"\\r\", \"\") for line in filtered)\n",
    "                filtered = (line.replace(\"'\", \"\") for line in filtered)\n",
    "                row = filtered\n",
    "\n",
    "                group = np.random.multinomial(1,[.6,.2,.2])\n",
    "                if np.argmax(group)==0:\n",
    "                    with open('iristrain/iristrain{0}.csv'.format(train_rows), 'w+', newline = None) as trainfile:\n",
    "                        trainwriter = csv.writer(trainfile, delimiter=',')\n",
    "                        trainwriter.writerow(row)\n",
    "                    train_rows += 1\n",
    "\n",
    "\n",
    "                if np.argmax(group)==1:\n",
    "                    with open('irisvalid/irisvalid{0}.csv'.format(valid_rows), 'w+', newline = None) as validfile:\n",
    "                        validwriter = csv.writer(validfile, delimiter=',')\n",
    "                        validwriter.writerow(row)\n",
    "                    valid_rows += 1\n",
    "\n",
    "                if np.argmax(group)==2:\n",
    "                    with open('iristest/iristest{0}.csv'.format(test_rows), 'w+', newline = None) as testfile:\n",
    "                        testwriter = csv.writer(testfile, delimiter=',')\n",
    "                        testwriter.writerow(row)\n",
    "                    test_rows += 1\n",
    "                    \n",
    "    return train_rows, valid_rows, test_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to format inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous valued predicators and one-hot encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file_format(filename_queue, possible_labels):\n",
    "    reader = tf.TextLineReader()\n",
    "    _, value = reader.read(filename_queue)\n",
    "\n",
    "    record_defaults = [tf.constant([],dtype=tf.float32), tf.constant([],dtype=tf.float32),\n",
    "                       tf.constant([],dtype=tf.float32), tf.constant([],dtype=tf.float32),\n",
    "                       tf.constant([],dtype=tf.float32), tf.constant([],dtype=tf.string)]\n",
    "    col1, col2, col3, col4, col5, col6 = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "    \n",
    "    example = tf.stack([col1, col2, col3, col4, col5])\n",
    "    label = tf.one_hot(tf.where(tf.equal(possible_labels, col6))[0], depth = possible_labels.shape[0], on_value = 1, off_value = 0)\n",
    "    label = label[0]\n",
    "    return example, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to format, queue and read inputs in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_pipeline(filenames, possible_labels, batch_size = 3, num_epochs = None, evaluation = False):   \n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        train_set, num_epochs=num_epochs, shuffle=True)\n",
    "\n",
    "    example, label = read_file_format(filename_queue, possible_labels)\n",
    "\n",
    "#     # min_after_dequeue defines how big a buffer we will randomly sample\n",
    "#     #   from -- bigger means better shuffling but slower start up and more\n",
    "#     #   memory used.\n",
    "#     # capacity must be larger than min_after_dequeue and the amount larger\n",
    "#     #   determines the maximum we will prefetch.  Recommendation:\n",
    "#     #   min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
    "    min_after_dequeue = 10\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch, label_batch = tf.train.shuffle_batch(\n",
    "      [example, label], batch_size=batch_size,capacity = capacity, \n",
    "        min_after_dequeue = min_after_dequeue\n",
    "    )    \n",
    "        \n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for creating layer activation summaries for Tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _activation_summary(x):\n",
    "    tensor_name = x.name\n",
    "    tensor_name = tensor_name.replace(':', '_')\n",
    "    tensor_name = tensor_name.replace('(', '_')\n",
    "    tensor_name = tensor_name.replace(')', '_')\n",
    "    tensor_name = tensor_name.replace(' ', '_')\n",
    "\n",
    "    tf.summary.histogram(tensor_name + '/activations', x)\n",
    "    tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the predictive function we're looking to optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, we'll do: linear => sigmoid => linear => log-softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_prediction1(X):  \n",
    "    with tf.variable_scope('sigmoid1') as scope:\n",
    "        weights = tf.Variable(tf.random_normal([4, 3]), name='weights', trainable=True)\n",
    "        biases = tf.Variable(tf.random_normal([3]), name='bias', trainable=True)\n",
    "        lin_y1 = tf.matmul(X, weights) + biases\n",
    "        _activation_summary(lin_y1)\n",
    "        \n",
    "        sig_y1 = tf.sigmoid(lin_y1)\n",
    "        _activation_summary(sig_y1)\n",
    "    \n",
    "    with tf.variable_scope('softmax2') as scope:\n",
    "        weights = tf.Variable(tf.random_normal([3, 3]), name='weights', trainable=True)\n",
    "        biases = tf.Variable(tf.random_normal([3]), name='bias', trainable = True)\n",
    "        lin_y2 = tf.matmul(sig_y1, weights) + biases\n",
    "        _activation_summary(lin_y2)\n",
    "        \n",
    "        smax_num = tf.transpose(tf.exp(lin_y2 - tf.reduce_max(lin_y2)))\n",
    "        smax_den = tf.reduce_sum(tf.exp(lin_y2 - tf.reduce_max(lin_y2)), -1)\n",
    "        softmax_y2 = tf.transpose(tf.divide(smax_num, smax_den))\n",
    "        _activation_summary(softmax_y2)\n",
    "\n",
    "    return softmax_y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also implementing the made-up \"ReQu\" unit; our model is now: linear => ReQu => linear => log-softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_prediction2(X):  \n",
    "    with tf.variable_scope('requ') as scope:\n",
    "        weights = tf.Variable(tf.random_normal([4, 3]), name='weights', trainable=True)\n",
    "        biases = tf.Variable(tf.random_normal([3]), name='bias', trainable=True)\n",
    "        lin_y1 = tf.matmul(X, weights) + biases\n",
    "        _activation_summary(lin_y1)\n",
    "       \n",
    "        requ_y1 = tf.square(tf.maximum(0., lin_y1))\n",
    "        _activation_summary(requ_y1)\n",
    "    \n",
    "    with tf.variable_scope('softmax2') as scope:\n",
    "        weights = tf.Variable(tf.random_normal([3, 3]), name='weights', trainable=True)\n",
    "        biases = tf.Variable(tf.random_normal([3]), name='bias', trainable = True)\n",
    "        lin_y2 = tf.matmul(requ_y1, weights) + biases\n",
    "        _activation_summary(lin_y2)\n",
    "        \n",
    "        smax_num = tf.transpose(tf.exp(lin_y2 - tf.reduce_max(lin_y2)))\n",
    "        smax_den = tf.reduce_sum(tf.exp(lin_y2 - tf.reduce_max(lin_y2)), -1)\n",
    "        softmax_y2 = tf.transpose(tf.divide(smax_num, smax_den))\n",
    "        _activation_summary(softmax_y2)\n",
    "\n",
    "    return softmax_y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note the use of variable_scope so we don't have to name new tf.Variable()'s for every layer; rather we can just keep calling them weights and biases and their \"scope\" will be limited to their specific use in each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying our loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And giving ourselves Tensorboard summaries to monitor change in loss during SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(logits, labels):\n",
    "    NLLCriterion = -tf.reduce_mean(tf.reduce_sum(tf.multiply(labels, tf.log(logits + 1e-10)), axis=1))\n",
    "\n",
    "    tf.add_to_collection('losses', NLLCriterion)\n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, borrowing a TF function which adds a smoothed loss, to reduce noise in our loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _add_loss_summaries(total_loss):\n",
    "    # Compute the moving average of all individual losses and the total loss.\n",
    "    loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "    losses = tf.get_collection('losses')\n",
    "    loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "\n",
    "    # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "    # same for the averaged version of the losses.\n",
    "    for l in losses + [total_loss]:\n",
    "    # Name each loss as '(raw)' and name the moving average version of the loss\n",
    "    # as the original loss name.\n",
    "        l_name = l.name.replace(\":\", \"_\")\n",
    "        tf.summary.scalar(l_name + '_raw_', l)\n",
    "        tf.summary.scalar(l_name, loss_averages.average(l))\n",
    "\n",
    "    return loss_averages_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying a training operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We're going to decay our loss rate to avoid jumping over potentially lower error rates in our parameter space. Also using tf.train.MomentumOptimizer() to mitigate the same concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the gradient of the loss here, given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial z^4} \\centerdot \\frac{\\partial z^4}{\\partial z^3} \\centerdot \\frac{\\partial z^3}{\\partial z^2} \\centerdot \\frac{\\partial z^2}{\\partial z^1} \\centerdot \\frac{\\partial z^1}{\\partial X} =$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(total_loss, global_step):\n",
    " \n",
    "    # Variables that affect learning rate.\n",
    "    num_batches_per_epoch = num_examples_per_train_epoch / batch_size\n",
    "    decay_steps = int(num_batches_per_epoch * num_epochs_to_decay)\n",
    "\n",
    "    # Decay the learning rate exponentially based on the number of steps.\n",
    "    lr = tf.train.exponential_decay(\n",
    "        initial_learning_rate, global_step,\n",
    "        decay_steps, learning_rate_decay_factor, staircase=True)\n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "\n",
    "    # Generate moving averages of all losses and associated summaries.\n",
    "    loss_averages_op = _add_loss_summaries(total_loss)\n",
    "\n",
    "    # Compute gradients.\n",
    "    with tf.control_dependencies([loss_averages_op]):\n",
    "        opt = tf.train.MomentumOptimizer(lr, momentum)\n",
    "        grads = opt.compute_gradients(total_loss)\n",
    "\n",
    "    # Apply gradients.\n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "    # Add histograms for trainable variables.\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "    # Add histograms for gradients.\n",
    "    for grad, var in grads:\n",
    "        if grad is not None:\n",
    "            tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "\n",
    "    # Track the moving averages of all trainable variables.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        moving_average_decay, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(y_hat, y_):\n",
    "    correct = tf.equal(tf.argmax(y_hat, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    tf.summary.scalar('validation_accuracy', accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_len, valid_len, test_len = datasplitter(filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = ['iristrain/iristrain{0}.csv'.format(i) for i in range(train_len)]\n",
    "valid_set = ['irisvalid/irisvalid{0}.csv'.format(i) for i in range(valid_len)]\n",
    "test_set = ['iristest/iristest{0}.csv'.format(i) for i in range(test_len)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(train_len):\n",
    "    if i == 0:\n",
    "        debug = pd.read_csv('iristrain/iristrain{0}.csv'.format(i), header = None)\n",
    "    else:\n",
    "        temp = pd.read_csv('iristrain/iristrain{0}.csv'.format(i), header = None)\n",
    "        debug = debug.append(temp)\n",
    "debug_ex = debug.ix[:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training for 1 epochs, 32 steps.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "batch_size = 3\n",
    "\n",
    "pipe_ex = debug_ex.iloc[0:0,:]\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    unique_labels = tf.constant([\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"], dtype=tf.string)\n",
    "    \n",
    "    example_feed, labels_feed = input_pipeline(train_set, possible_labels = unique_labels,\n",
    "                                               batch_size = batch_size, num_epochs = num_epochs)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Start populating the filename queue.\n",
    "        \n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()  \n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "        \n",
    "        step = 0\n",
    "        while not coord.should_stop():\n",
    "            try:\n",
    "                start_time = time.time()                \n",
    "               \n",
    "                example_batch, label_batch = sess.run([example_feed, labels_feed])\n",
    "                \n",
    "                pipe_ex = pipe_ex.append(pd.DataFrame(example_batch))\n",
    "                \n",
    "                duration = time.time() - start_time\n",
    "                step += 1\n",
    "\n",
    "            except (tf.errors.OutOfRangeError, tf.errors.InvalidArgumentError) as e:\n",
    "           \n",
    "                print('Done training for %d epochs, %d steps.' % (num_epochs, step))\n",
    "                # When done, ask the threads to stop.\n",
    "                coord.request_stop()\n",
    "\n",
    "        \n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_ex.ix[:,0].astype(float).reset_index(drop=True).equals((pipe_ex.sort_values(0).reset_index(drop=True).ix[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global num_examples_per_train_epoch\n",
    "num_examples_per_train_epoch = train_len\n",
    "\n",
    "global num_epochs\n",
    "num_epochs= 10\n",
    "\n",
    "global batch_size\n",
    "batch_size = 3\n",
    "\n",
    "global moving_average_decay\n",
    "moving_average_decay = 0.9999     # The decay to use for the moving average.\n",
    "\n",
    "global num_epochs_to_decay\n",
    "num_epochs_to_decay = 5    # Epochs after which learning rate decays.\n",
    "\n",
    "global learning_rate_decay_factor\n",
    "learning_rate_decay_factor = 0.001  # Learning rate decay factor.\n",
    "\n",
    "global initial_learning_rate\n",
    "initial_learning_rate = 0.01       # Initial learning rate.\n",
    "\n",
    "global momentum\n",
    "momentum = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logdir = 'TF_Logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.05374288558959961\n",
      "2\n",
      "0.008307695388793945\n",
      "3\n",
      "0.007004976272583008\n",
      "4\n",
      "0.004003286361694336\n",
      "5\n",
      "0.00400233268737793\n",
      "6\n",
      "0.006012678146362305\n",
      "7\n",
      "0.009998798370361328\n",
      "8\n",
      "0.006457090377807617\n",
      "9\n",
      "0.006005525588989258\n",
      "10\n",
      "0.004003047943115234\n",
      "11\n",
      "0.009326696395874023\n",
      "12\n",
      "0.006003856658935547\n",
      "13\n",
      "0.015505313873291016\n",
      "14\n",
      "0.0072362422943115234\n",
      "15\n",
      "0.007004976272583008\n",
      "16\n",
      "0.00600433349609375\n",
      "17\n",
      "0.007005453109741211\n",
      "18\n",
      "0.013160228729248047\n",
      "19\n",
      "0.007004737854003906\n",
      "20\n",
      "0.008387327194213867\n",
      "21\n",
      "0.005004405975341797\n",
      "22\n",
      "0.008006811141967773\n",
      "23\n",
      "0.008218050003051758\n",
      "24\n",
      "0.008006572723388672\n",
      "25\n",
      "0.005284309387207031\n",
      "26\n",
      "0.004001140594482422\n",
      "27\n",
      "0.008001327514648438\n",
      "28\n",
      "0.015150785446166992\n",
      "29\n",
      "0.008335351943969727\n",
      "30\n",
      "0.008006095886230469\n",
      "31\n",
      "0.004002094268798828\n",
      "32\n",
      "0.008632183074951172\n",
      "33\n",
      "0.005003452301025391\n",
      "34\n",
      "0.008162498474121094\n",
      "35\n",
      "0.008007287979125977\n",
      "36\n",
      "0.009226322174072266\n",
      "37\n",
      "0.006003379821777344\n",
      "38\n",
      "0.008109569549560547\n",
      "39\n",
      "0.008006572723388672\n",
      "40\n",
      "0.010011672973632812\n",
      "41\n",
      "0.008000373840332031\n",
      "42\n",
      "0.004002809524536133\n",
      "43\n",
      "0.0060045719146728516\n",
      "44\n",
      "0.009006500244140625\n",
      "45\n",
      "0.0060045719146728516\n",
      "46\n",
      "0.011007547378540039\n",
      "47\n",
      "0.005002737045288086\n",
      "48\n",
      "0.0060045719146728516\n",
      "49\n",
      "0.009007692337036133\n",
      "50\n",
      "0.006003141403198242\n",
      "51\n",
      "0.00800633430480957\n",
      "52\n",
      "0.007004737854003906\n",
      "53\n",
      "0.005002737045288086\n",
      "54\n",
      "0.0040035247802734375\n",
      "55\n",
      "0.00600433349609375\n",
      "56\n",
      "0.008005619049072266\n",
      "57\n",
      "0.004002094268798828\n",
      "58\n",
      "0.007004499435424805\n",
      "59\n",
      "0.006005048751831055\n",
      "60\n",
      "0.006005048751831055\n",
      "61\n",
      "0.009004831314086914\n",
      "62\n",
      "0.005004167556762695\n",
      "63\n",
      "0.006003856658935547\n",
      "64\n",
      "0.005003929138183594\n",
      "65\n",
      "0.006003856658935547\n",
      "66\n",
      "0.010006904602050781\n",
      "67\n",
      "0.006218910217285156\n",
      "68\n",
      "0.004002571105957031\n",
      "69\n",
      "0.003999233245849609\n",
      "70\n",
      "0.004000425338745117\n",
      "71\n",
      "0.008000612258911133\n",
      "72\n",
      "0.003999948501586914\n",
      "73\n",
      "0.004000663757324219\n",
      "74\n",
      "0.008000373840332031\n",
      "75\n",
      "0.004000186920166016\n",
      "76\n",
      "0.008000612258911133\n",
      "77\n",
      "0.003999948501586914\n",
      "78\n",
      "0.004000186920166016\n",
      "79\n",
      "0.008000850677490234\n",
      "80\n",
      "0.004000186920166016\n",
      "81\n",
      "0.004000425338745117\n",
      "82\n",
      "0.00800180435180664\n",
      "83\n",
      "0.0039997100830078125\n",
      "84\n",
      "0.007999658584594727\n",
      "85\n",
      "0.004000663757324219\n",
      "86\n",
      "0.004000663757324219\n",
      "87\n",
      "0.003999471664428711\n",
      "88\n",
      "0.00800323486328125\n",
      "89\n",
      "0.003997802734375\n",
      "90\n",
      "0.004000425338745117\n",
      "91\n",
      "0.003999948501586914\n",
      "92\n",
      "0.008000612258911133\n",
      "93\n",
      "0.0040013790130615234\n",
      "94\n",
      "0.0039997100830078125\n",
      "95\n",
      "0.008000850677490234\n",
      "96\n",
      "0.008790016174316406\n",
      "97\n",
      "0.0050029754638671875\n",
      "98\n",
      "0.005004167556762695\n",
      "99\n",
      "0.007005453109741211\n",
      "100\n",
      "0.009011507034301758\n",
      "101\n",
      "0.005999088287353516\n",
      "102\n",
      "0.008005857467651367\n",
      "103\n",
      "0.006596803665161133\n",
      "104\n",
      "0.00800013542175293\n",
      "105\n",
      "0.0039997100830078125\n",
      "106\n",
      "0.004659175872802734\n",
      "107\n",
      "0.004002094268798828\n",
      "108\n",
      "0.00633549690246582\n",
      "109\n",
      "0.007004261016845703\n",
      "110\n",
      "0.006005048751831055\n",
      "111\n",
      "0.006003379821777344\n",
      "112\n",
      "0.006004810333251953\n",
      "113\n",
      "0.0030546188354492188\n",
      "114\n",
      "0.004001617431640625\n",
      "115\n",
      "0.004000425338745117\n",
      "116\n",
      "0.008001089096069336\n",
      "117\n",
      "0.003999471664428711\n",
      "118\n",
      "0.008001089096069336\n",
      "119\n",
      "0.004000425338745117\n",
      "120\n",
      "0.008000612258911133\n",
      "121\n",
      "0.004000186920166016\n",
      "122\n",
      "0.0039997100830078125\n",
      "123\n",
      "0.008000373840332031\n",
      "124\n",
      "0.00400090217590332\n",
      "125\n",
      "0.0039997100830078125\n",
      "126\n",
      "0.004000425338745117\n",
      "127\n",
      "0.008001327514648438\n",
      "128\n",
      "0.004000186920166016\n",
      "129\n",
      "0.008000850677490234\n",
      "130\n",
      "0.003999471664428711\n",
      "131\n",
      "0.004000425338745117\n",
      "132\n",
      "0.009311199188232422\n",
      "133\n",
      "0.0060045719146728516\n",
      "134\n",
      "0.007004976272583008\n",
      "135\n",
      "0.0070056915283203125\n",
      "136\n",
      "0.006005287170410156\n",
      "137\n",
      "0.006002664566040039\n",
      "138\n",
      "0.005339860916137695\n",
      "139\n",
      "0.006003379821777344\n",
      "140\n",
      "0.004002094268798828\n",
      "141\n",
      "0.005003929138183594\n",
      "142\n",
      "0.004682779312133789\n",
      "143\n",
      "0.008001565933227539\n",
      "144\n",
      "0.00800013542175293\n",
      "145\n",
      "0.0039997100830078125\n",
      "146\n",
      "0.004000425338745117\n",
      "147\n",
      "0.008000612258911133\n",
      "148\n",
      "0.004000186920166016\n",
      "149\n",
      "0.004000425338745117\n",
      "150\n",
      "0.004000186920166016\n",
      "151\n",
      "0.008000850677490234\n",
      "152\n",
      "0.00400090217590332\n",
      "153\n",
      "0.008001327514648438\n",
      "154\n",
      "0.007998466491699219\n",
      "155\n",
      "0.0040013790130615234\n",
      "156\n",
      "0.0039997100830078125\n",
      "157\n",
      "0.008000612258911133\n",
      "158\n",
      "0.004000425338745117\n",
      "159\n",
      "0.003999948501586914\n",
      "160\n",
      "0.004000663757324219\n",
      "161\n",
      "0.003999948501586914\n",
      "162\n",
      "0.008001089096069336\n",
      "163\n",
      "0.008000850677490234\n",
      "164\n",
      "0.004000186920166016\n",
      "165\n",
      "0.007999658584594727\n",
      "166\n",
      "0.004000425338745117\n",
      "167\n",
      "0.00400090217590332\n",
      "168\n",
      "0.008000612258911133\n",
      "169\n",
      "0.008000612258911133\n",
      "170\n",
      "0.007999658584594727\n",
      "171\n",
      "0.013617753982543945\n",
      "172\n",
      "0.01410222053527832\n",
      "173\n",
      "0.008187294006347656\n",
      "174\n",
      "0.009005546569824219\n",
      "175\n",
      "0.0030536651611328125\n",
      "176\n",
      "0.012002229690551758\n",
      "177\n",
      "0.008000850677490234\n",
      "178\n",
      "0.012001514434814453\n",
      "179\n",
      "0.008000373840332031\n",
      "180\n",
      "0.007999897003173828\n",
      "181\n",
      "0.008000850677490234\n",
      "182\n",
      "0.007999420166015625\n",
      "183\n",
      "0.008002519607543945\n",
      "184\n",
      "0.003998517990112305\n",
      "185\n",
      "0.008000612258911133\n",
      "186\n",
      "0.004000425338745117\n",
      "187\n",
      "0.008001089096069336\n",
      "188\n",
      "0.00800013542175293\n",
      "189\n",
      "0.003999948501586914\n",
      "190\n",
      "0.008001327514648438\n",
      "191\n",
      "0.004000663757324219\n",
      "192\n",
      "0.003999471664428711\n",
      "193\n",
      "0.008000373840332031\n",
      "194\n",
      "0.00416111946105957\n",
      "195\n",
      "0.0\n",
      "196\n",
      "0.0\n",
      "197\n",
      "0.015627622604370117\n",
      "198\n",
      "0.0\n",
      "199\n",
      "0.0\n",
      "200\n",
      "0.015625\n",
      "201\n",
      "0.0\n",
      "202\n",
      "0.016089916229248047\n",
      "203\n",
      "0.004276275634765625\n",
      "204\n",
      "0.0\n",
      "205\n",
      "0.0\n",
      "206\n",
      "0.015626907348632812\n",
      "207\n",
      "0.0\n",
      "208\n",
      "0.0\n",
      "209\n",
      "0.015625476837158203\n",
      "210\n",
      "0.0\n",
      "211\n",
      "0.0\n",
      "212\n",
      "0.015625953674316406\n",
      "213\n",
      "0.0\n",
      "214\n",
      "0.0\n",
      "215\n",
      "0.015623807907104492\n",
      "216\n",
      "0.0\n",
      "217\n",
      "0.0\n",
      "218\n",
      "0.0\n",
      "219\n",
      "0.0156252384185791\n",
      "220\n",
      "0.0\n",
      "221\n",
      "0.0\n",
      "222\n",
      "0.015625476837158203\n",
      "223\n",
      "0.0\n",
      "224\n",
      "0.0\n",
      "225\n",
      "0.015786409378051758\n",
      "226\n",
      "0.004004001617431641\n",
      "227\n",
      "0.0050046443939208984\n",
      "228\n",
      "0.004002571105957031\n",
      "229\n",
      "0.005004167556762695\n",
      "230\n",
      "0.006004810333251953\n",
      "231\n",
      "0.00400090217590332\n",
      "232\n",
      "0.0010650157928466797\n",
      "233\n",
      "0.0\n",
      "234\n",
      "0.0\n",
      "235\n",
      "0.01562666893005371\n",
      "236\n",
      "0.0\n",
      "237\n",
      "0.0\n",
      "238\n",
      "0.0\n",
      "239\n",
      "0.015625\n",
      "240\n",
      "0.0\n",
      "241\n",
      "0.0\n",
      "242\n",
      "0.015625715255737305\n",
      "243\n",
      "0.0\n",
      "244\n",
      "0.0\n",
      "245\n",
      "0.015642404556274414\n",
      "246\n",
      "0.0\n",
      "247\n",
      "0.0\n",
      "248\n",
      "0.0\n",
      "249\n",
      "0.01599907875061035\n",
      "250\n",
      "0.004215717315673828\n",
      "251\n",
      "0.0\n",
      "252\n",
      "0.0\n",
      "253\n",
      "0.015626907348632812\n",
      "254\n",
      "0.0\n",
      "255\n",
      "0.0\n",
      "256\n",
      "0.015624761581420898\n",
      "257\n",
      "0.0\n",
      "258\n",
      "0.0\n",
      "259\n",
      "0.0\n",
      "260\n",
      "0.015625476837158203\n",
      "261\n",
      "0.0\n",
      "262\n",
      "0.0\n",
      "263\n",
      "0.015624761581420898\n",
      "264\n",
      "0.0\n",
      "265\n",
      "0.0\n",
      "266\n",
      "0.0\n",
      "267\n",
      "0.0156404972076416\n",
      "268\n",
      "0.0\n",
      "269\n",
      "0.0\n",
      "270\n",
      "0.015612125396728516\n",
      "271\n",
      "0.0\n",
      "272\n",
      "0.0\n",
      "273\n",
      "0.015928268432617188\n",
      "274\n",
      "0.004223823547363281\n",
      "275\n",
      "0.0\n",
      "276\n",
      "0.0\n",
      "277\n",
      "0.0\n",
      "278\n",
      "0.01562643051147461\n",
      "279\n",
      "0.0\n",
      "280\n",
      "0.0\n",
      "281\n",
      "0.015625715255737305\n",
      "282\n",
      "0.0\n",
      "283\n",
      "0.0\n",
      "284\n",
      "0.015642642974853516\n",
      "285\n",
      "0.0\n",
      "286\n",
      "0.0\n",
      "287\n",
      "0.0\n",
      "288\n",
      "0.015607595443725586\n",
      "289\n",
      "0.0\n",
      "290\n",
      "0.0\n",
      "291\n",
      "0.0\n",
      "292\n",
      "0.0\n",
      "293\n",
      "0.0\n",
      "294\n",
      "0.015642404556274414\n",
      "295\n",
      "0.0\n",
      "296\n",
      "0.0\n",
      "297\n",
      "0.015625715255737305\n",
      "298\n",
      "0.0011475086212158203\n",
      "299\n",
      "0.004411935806274414\n",
      "300\n",
      "0.0\n",
      "301\n",
      "0.0\n",
      "302\n",
      "0.018893003463745117\n",
      "303\n",
      "0.0050122737884521484\n",
      "304\n",
      "0.0039942264556884766\n",
      "305\n",
      "0.0050201416015625\n",
      "306\n",
      "0.004046916961669922\n",
      "307\n",
      "0.0\n",
      "308\n",
      "0.0\n",
      "309\n",
      "0.015627384185791016\n",
      "310\n",
      "0.0\n",
      "311\n",
      "0.0\n",
      "312\n",
      "0.0\n",
      "313\n",
      "0.015624523162841797\n",
      "314\n",
      "0.0\n",
      "315\n",
      "0.0\n",
      "316\n",
      "0.015643835067749023\n",
      "317\n",
      "0.0\n",
      "318\n",
      "0.0\n",
      "319\n",
      "0.015869617462158203\n",
      "320\n",
      "0.004416704177856445\n",
      "321\n",
      "0.0\n",
      "322\n",
      "0.0\n",
      "323\n",
      "0.01562666893005371\n",
      "324\n",
      "0.0\n",
      "325\n",
      "0.0\n",
      "326\n",
      "0.0\n",
      "Training complete, entering validation...\n",
      "Evaluating inputs...\n",
      "Calculating accuracy...\n",
      "Evaluating validation summary...\n",
      "Done training for 10 epochs, 326 steps.\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "    unique_labels = tf.constant([\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"], dtype=tf.string)\n",
    "    \n",
    "    example_feed, labels_feed = input_pipeline(train_set, possible_labels=unique_labels,\n",
    "                                               batch_size=batch_size, num_epochs=num_epochs)\n",
    "    \n",
    "    valid_example_feed, valid_labels_feed = input_pipeline(\n",
    "        valid_set, possible_labels=unique_labels,\n",
    "        batch_size=valid_len, num_epochs=1)\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "    y_ = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "    \n",
    "    y_hat = make_prediction1(x)\n",
    "    \n",
    "    loss = calculate_loss(y_hat, y_)\n",
    "    \n",
    "    train_op = train(loss, global_step=global_step)\n",
    "    step = 0\n",
    "    \n",
    "    accuracy = evaluate_accuracy(y_hat, y_)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Start populating the filename queue.\n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()  \n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "\n",
    "        step = 0\n",
    "        while not coord.should_stop():\n",
    "            try:\n",
    "                start_time = time.time()                \n",
    "\n",
    "                example_batch, label_batch = sess.run([example_feed, labels_feed])\n",
    "                example_batch = example_batch[:, 1:]\n",
    "\n",
    "                result, summary =  sess.run([train_op, merged],\n",
    "                                            feed_dict={x: example_batch,\n",
    "                                                       y_: label_batch})\n",
    "                writer.add_summary(summary, global_step.eval())\n",
    "\n",
    "                step += 1\n",
    "                print(step)\n",
    "                duration = time.time() - start_time\n",
    "                print(duration)\n",
    "\n",
    "            except (tf.errors.OutOfRangeError, tf.errors.InvalidArgumentError) as e:\n",
    "                print(\"Training complete, entering validation...\")\n",
    "\n",
    "                print(\"Evaluating inputs...\")\n",
    "                valid_examples, valid_labels = sess.run([valid_example_feed, valid_labels_feed])\n",
    "                valid_examples = valid_examples[:, 1:]\n",
    "\n",
    "                print(\"Calculating accuracy...\")\n",
    "                acc = sess.run(accuracy, feed_dict={x: valid_examples,\n",
    "                                                             y_: valid_labels})\n",
    "                print(\"Evaluating validation summary...\")\n",
    "                summary = sess.run(merged,\n",
    "                                   feed_dict={\n",
    "                                       x: valid_examples, y_: valid_labels\n",
    "                                   })\n",
    "\n",
    "                print('Done training for %d epochs, %d steps.' % (num_epochs, step))\n",
    "                # When done, ask the threads to stop.\n",
    "                coord.request_stop()\n",
    "\n",
    "        \n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4375"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.05187797546386719\n",
      "2\n",
      "0.0\n",
      "3\n",
      "0.0\n",
      "4\n",
      "0.015626192092895508\n",
      "5\n",
      "0.0\n",
      "6\n",
      "0.0\n",
      "7\n",
      "0.015625476837158203\n",
      "8\n",
      "0.0\n",
      "9\n",
      "0.0\n",
      "10\n",
      "0.01563882827758789\n",
      "11\n",
      "0.0\n",
      "12\n",
      "0.0\n",
      "13\n",
      "0.0\n",
      "14\n",
      "0.015629291534423828\n",
      "15\n",
      "0.0\n",
      "16\n",
      "0.0\n",
      "17\n",
      "0.015607595443725586\n",
      "18\n",
      "0.0\n",
      "19\n",
      "0.0\n",
      "20\n",
      "0.01562643051147461\n",
      "21\n",
      "0.0\n",
      "22\n",
      "0.0\n",
      "23\n",
      "0.0\n",
      "24\n",
      "0.016911029815673828\n",
      "25\n",
      "0.004274129867553711\n",
      "26\n",
      "0.0\n",
      "27\n",
      "0.0\n",
      "28\n",
      "0.015630006790161133\n",
      "29\n",
      "0.0\n",
      "30\n",
      "0.0\n",
      "31\n",
      "0.0\n",
      "32\n",
      "0.015621662139892578\n",
      "33\n",
      "0.0\n",
      "34\n",
      "0.0\n",
      "35\n",
      "0.015625715255737305\n",
      "36\n",
      "0.0\n",
      "37\n",
      "0.0\n",
      "38\n",
      "0.0\n",
      "39\n",
      "0.01811981201171875\n",
      "40\n",
      "0.004984617233276367\n",
      "41\n",
      "0.00412750244140625\n",
      "42\n",
      "0.004015207290649414\n",
      "43\n",
      "0.003990650177001953\n",
      "44\n",
      "0.0030701160430908203\n",
      "45\n",
      "0.011417150497436523\n",
      "46\n",
      "0.002998828887939453\n",
      "47\n",
      "0.005005598068237305\n",
      "48\n",
      "0.006002664566040039\n",
      "49\n",
      "0.008008480072021484\n",
      "50\n",
      "0.005002498626708984\n",
      "51\n",
      "0.0040013790130615234\n",
      "52\n",
      "0.0052318572998046875\n",
      "53\n",
      "0.004003286361694336\n",
      "54\n",
      "0.007999420166015625\n",
      "55\n",
      "0.008068561553955078\n",
      "56\n",
      "0.0\n",
      "57\n",
      "0.0\n",
      "58\n",
      "0.01562666893005371\n",
      "59\n",
      "0.0\n",
      "60\n",
      "0.015642642974853516\n",
      "61\n",
      "0.0\n",
      "62\n",
      "0.0\n",
      "63\n",
      "0.0\n",
      "64\n",
      "0.016172170639038086\n",
      "65\n",
      "0.004264116287231445\n",
      "66\n",
      "0.0\n",
      "67\n",
      "0.0\n",
      "68\n",
      "0.01562809944152832\n",
      "69\n",
      "0.0\n",
      "70\n",
      "0.0\n",
      "71\n",
      "0.0\n",
      "72\n",
      "0.015625476837158203\n",
      "73\n",
      "0.0\n",
      "74\n",
      "0.0\n",
      "75\n",
      "0.0\n",
      "76\n",
      "0.01562356948852539\n",
      "77\n",
      "0.0\n",
      "78\n",
      "0.0\n",
      "79\n",
      "0.015626192092895508\n",
      "80\n",
      "0.0\n",
      "81\n",
      "0.0\n",
      "82\n",
      "0.0\n",
      "83\n",
      "0.015624523162841797\n",
      "84\n",
      "0.0\n",
      "85\n",
      "0.0\n",
      "86\n",
      "0.0\n",
      "87\n",
      "0.015625715255737305\n",
      "88\n",
      "0.00879049301147461\n",
      "89\n",
      "0.006003856658935547\n",
      "90\n",
      "0.006577253341674805\n",
      "91\n",
      "0.006985902786254883\n",
      "92\n",
      "0.008273601531982422\n",
      "93\n",
      "0.003171205520629883\n",
      "94\n",
      "0.009008646011352539\n",
      "95\n",
      "0.006196260452270508\n",
      "96\n",
      "0.00700831413269043\n",
      "97\n",
      "0.003917217254638672\n",
      "98\n",
      "0.0041162967681884766\n",
      "99\n",
      "0.003983974456787109\n",
      "100\n",
      "0.0\n",
      "101\n",
      "0.0040013790130615234\n",
      "102\n",
      "0.008060693740844727\n",
      "103\n",
      "0.0\n",
      "104\n",
      "0.0\n",
      "105\n",
      "0.015626907348632812\n",
      "106\n",
      "0.0\n",
      "107\n",
      "0.0\n",
      "108\n",
      "0.0\n",
      "109\n",
      "0.017830371856689453\n",
      "110\n",
      "0.00500798225402832\n",
      "111\n",
      "0.003984689712524414\n",
      "112\n",
      "0.005003929138183594\n",
      "113\n",
      "0.00307464599609375\n",
      "114\n",
      "0.0\n",
      "115\n",
      "0.0\n",
      "116\n",
      "0.015645265579223633\n",
      "117\n",
      "0.0\n",
      "118\n",
      "0.0\n",
      "119\n",
      "0.0\n",
      "120\n",
      "0.01560831069946289\n",
      "121\n",
      "0.0\n",
      "122\n",
      "0.0\n",
      "123\n",
      "0.0\n",
      "124\n",
      "0.015623807907104492\n",
      "125\n",
      "0.0\n",
      "126\n",
      "0.0\n",
      "127\n",
      "0.01562643051147461\n",
      "128\n",
      "0.0\n",
      "129\n",
      "0.0\n",
      "130\n",
      "0.015624046325683594\n",
      "131\n",
      "0.006777524948120117\n",
      "132\n",
      "0.006006002426147461\n",
      "133\n",
      "0.0053195953369140625\n",
      "134\n",
      "0.0030050277709960938\n",
      "135\n",
      "0.006001949310302734\n",
      "136\n",
      "0.009006261825561523\n",
      "137\n",
      "0.004774332046508789\n",
      "138\n",
      "0.0050048828125\n",
      "139\n",
      "0.007173299789428711\n",
      "140\n",
      "0.00400090217590332\n",
      "141\n",
      "0.008001089096069336\n",
      "142\n",
      "0.004067897796630859\n",
      "143\n",
      "0.0\n",
      "144\n",
      "0.010645866394042969\n",
      "145\n",
      "0.006166219711303711\n",
      "146\n",
      "0.008006811141967773\n",
      "147\n",
      "0.005611896514892578\n",
      "148\n",
      "0.005006313323974609\n",
      "149\n",
      "0.006002187728881836\n",
      "150\n",
      "0.00700831413269043\n",
      "151\n",
      "0.005002260208129883\n",
      "152\n",
      "0.00600433349609375\n",
      "153\n",
      "0.003000020980834961\n",
      "154\n",
      "0.005135536193847656\n",
      "155\n",
      "0.003981351852416992\n",
      "156\n",
      "0.004000663757324219\n",
      "157\n",
      "0.004000425338745117\n",
      "158\n",
      "0.0042266845703125\n",
      "159\n",
      "0.0\n",
      "160\n",
      "0.015628337860107422\n",
      "161\n",
      "0.0\n",
      "162\n",
      "0.0\n",
      "163\n",
      "0.0\n",
      "164\n",
      "0.015623092651367188\n",
      "165\n",
      "0.0\n",
      "166\n",
      "0.0\n",
      "167\n",
      "0.0156252384185791\n",
      "168\n",
      "0.0\n",
      "169\n",
      "0.0\n",
      "170\n",
      "0.020545244216918945\n",
      "171\n",
      "0.005004405975341797\n",
      "172\n",
      "0.005003929138183594\n",
      "173\n",
      "0.007776737213134766\n",
      "174\n",
      "0.00602269172668457\n",
      "175\n",
      "0.005985736846923828\n",
      "176\n",
      "0.0040056705474853516\n",
      "177\n",
      "0.005000114440917969\n",
      "178\n",
      "0.004004716873168945\n",
      "179\n",
      "0.005003929138183594\n",
      "180\n",
      "0.0030748844146728516\n",
      "181\n",
      "0.008002519607543945\n",
      "182\n",
      "0.004082441329956055\n",
      "183\n",
      "0.0\n",
      "184\n",
      "0.0\n",
      "185\n",
      "0.015649080276489258\n",
      "186\n",
      "0.0\n",
      "187\n",
      "0.0\n",
      "188\n",
      "0.0\n",
      "189\n",
      "0.015604257583618164\n",
      "190\n",
      "0.0\n",
      "191\n",
      "0.0\n",
      "192\n",
      "0.016458749771118164\n",
      "193\n",
      "0.0042574405670166016\n",
      "194\n",
      "0.0\n",
      "195\n",
      "0.0\n",
      "196\n",
      "0.01564478874206543\n",
      "197\n",
      "0.0\n",
      "198\n",
      "0.0\n",
      "199\n",
      "0.0\n",
      "200\n",
      "0.01560664176940918\n",
      "201\n",
      "0.0\n",
      "202\n",
      "0.0\n",
      "203\n",
      "0.01563096046447754\n",
      "204\n",
      "0.0\n",
      "205\n",
      "0.0\n",
      "206\n",
      "0.015619277954101562\n",
      "207\n",
      "0.0\n",
      "208\n",
      "0.0\n",
      "209\n",
      "0.0156252384185791\n",
      "210\n",
      "0.0\n",
      "211\n",
      "0.0\n",
      "212\n",
      "0.01562809944152832\n",
      "213\n",
      "0.009134292602539062\n",
      "214\n",
      "0.005002498626708984\n",
      "215\n",
      "0.006033420562744141\n",
      "216\n",
      "0.004139900207519531\n",
      "217\n",
      "0.005013465881347656\n",
      "218\n",
      "0.008995771408081055\n",
      "219\n",
      "0.0050029754638671875\n",
      "220\n",
      "0.005004167556762695\n",
      "221\n",
      "0.005003452301025391\n",
      "222\n",
      "0.004124879837036133\n",
      "223\n",
      "0.004001140594482422\n",
      "224\n",
      "0.0040128231048583984\n",
      "225\n",
      "0.0045468807220458984\n",
      "226\n",
      "0.0\n",
      "227\n",
      "0.0\n",
      "228\n",
      "0.015645265579223633\n",
      "229\n",
      "0.0\n",
      "230\n",
      "0.0\n",
      "231\n",
      "0.0\n",
      "232\n",
      "0.015606403350830078\n",
      "233\n",
      "0.0\n",
      "234\n",
      "0.0\n",
      "235\n",
      "0.017452239990234375\n",
      "236\n",
      "0.0042645931243896484\n",
      "237\n",
      "0.0\n",
      "238\n",
      "0.0\n",
      "239\n",
      "0.015644311904907227\n",
      "240\n",
      "0.0\n",
      "241\n",
      "0.0\n",
      "242\n",
      "0.01560831069946289\n",
      "243\n",
      "0.0\n",
      "244\n",
      "0.0\n",
      "245\n",
      "0.01692056655883789\n",
      "246\n",
      "0.004999637603759766\n",
      "247\n",
      "0.004985332489013672\n",
      "248\n",
      "0.007004976272583008\n",
      "249\n",
      "0.003075122833251953\n",
      "250\n",
      "0.0\n",
      "251\n",
      "0.0\n",
      "252\n",
      "0.01876068115234375\n",
      "253\n",
      "0.007005214691162109\n",
      "254\n",
      "0.005004405975341797\n",
      "255\n",
      "0.01201176643371582\n",
      "256\n",
      "0.008002042770385742\n",
      "257\n",
      "0.004004955291748047\n",
      "258\n",
      "0.004019737243652344\n",
      "259\n",
      "0.00434422492980957\n",
      "260\n",
      "0.0039997100830078125\n",
      "261\n",
      "0.004002094268798828\n",
      "262\n",
      "0.003998517990112305\n",
      "263\n",
      "0.008019208908081055\n",
      "264\n",
      "0.0039823055267333984\n",
      "265\n",
      "0.008188009262084961\n",
      "266\n",
      "0.0\n",
      "267\n",
      "0.0\n",
      "268\n",
      "0.0\n",
      "269\n",
      "0.015644550323486328\n",
      "270\n",
      "0.0\n",
      "271\n",
      "0.0\n",
      "272\n",
      "0.015609025955200195\n",
      "273\n",
      "0.0\n",
      "274\n",
      "0.0\n",
      "275\n",
      "0.01591801643371582\n",
      "276\n",
      "0.004439353942871094\n",
      "277\n",
      "0.0\n",
      "278\n",
      "0.0\n",
      "279\n",
      "0.0\n",
      "280\n",
      "0.01562643051147461\n",
      "281\n",
      "0.0\n",
      "282\n",
      "0.0\n",
      "283\n",
      "0.015625476837158203\n",
      "284\n",
      "0.0\n",
      "285\n",
      "0.0\n",
      "286\n",
      "0.015625\n",
      "287\n",
      "0.0\n",
      "288\n",
      "0.0\n",
      "289\n",
      "0.0\n",
      "290\n",
      "0.01562666893005371\n",
      "291\n",
      "0.0\n",
      "292\n",
      "0.016045808792114258\n",
      "293\n",
      "0.005701541900634766\n",
      "294\n",
      "0.007005214691162109\n",
      "295\n",
      "0.008005857467651367\n",
      "296\n",
      "0.006005525588989258\n",
      "297\n",
      "0.005002260208129883\n",
      "298\n",
      "0.004004001617431641\n",
      "299\n",
      "0.008003711700439453\n",
      "300\n",
      "0.005003690719604492\n",
      "301\n",
      "0.005339145660400391\n",
      "302\n",
      "0.00400233268737793\n",
      "303\n",
      "0.004000186920166016\n",
      "304\n",
      "0.008325815200805664\n",
      "305\n",
      "0.005006313323974609\n",
      "306\n",
      "0.005003213882446289\n",
      "307\n",
      "0.005002260208129883\n",
      "308\n",
      "0.007005453109741211\n",
      "309\n",
      "0.006005525588989258\n",
      "310\n",
      "0.00400233268737793\n",
      "311\n",
      "0.004001617431640625\n",
      "312\n",
      "0.006022453308105469\n",
      "313\n",
      "0.005112886428833008\n",
      "314\n",
      "0.004004716873168945\n",
      "315\n",
      "0.002336740493774414\n",
      "316\n",
      "0.0\n",
      "317\n",
      "0.0\n",
      "318\n",
      "0.015627622604370117\n",
      "319\n",
      "0.0\n",
      "320\n",
      "0.0\n",
      "321\n",
      "0.0156252384185791\n",
      "322\n",
      "0.0\n",
      "323\n",
      "0.0\n",
      "324\n",
      "0.0\n",
      "325\n",
      "0.015625\n",
      "326\n",
      "0.0\n",
      "Training complete, entering validation...\n",
      "Evaluating inputs...\n",
      "Calculating accuracy...\n",
      "Evaluating validation summary...\n",
      "Done training for 10 epochs, 326 steps.\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "    unique_labels = tf.constant([\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"], dtype=tf.string)\n",
    "    \n",
    "    example_feed, labels_feed = input_pipeline(train_set, possible_labels=unique_labels,\n",
    "                                               batch_size=batch_size, num_epochs=num_epochs)\n",
    "    \n",
    "    valid_example_feed, valid_labels_feed = input_pipeline(\n",
    "        valid_set, possible_labels=unique_labels,\n",
    "        batch_size=valid_len, num_epochs=1)\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "    y_ = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "    \n",
    "    y_hat = make_prediction2(x)\n",
    "    \n",
    "    loss = calculate_loss(y_hat, y_)\n",
    "    \n",
    "    train_op = train(loss, global_step=global_step)\n",
    "    step = 0\n",
    "    \n",
    "    accuracy = evaluate_accuracy(y_hat, y_)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Start populating the filename queue.\n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()  \n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "\n",
    "        step = 0\n",
    "        while not coord.should_stop():\n",
    "            try:\n",
    "                start_time = time.time()                \n",
    "\n",
    "                example_batch, label_batch = sess.run([example_feed, labels_feed])\n",
    "                example_batch = example_batch[:, 1:]\n",
    "\n",
    "                result, summary =  sess.run([train_op, merged],\n",
    "                                            feed_dict={x: example_batch,\n",
    "                                                       y_: label_batch})\n",
    "                writer.add_summary(summary, global_step.eval())\n",
    "\n",
    "                step += 1\n",
    "                print(step)\n",
    "                duration = time.time() - start_time\n",
    "                print(duration)\n",
    "\n",
    "            except (tf.errors.OutOfRangeError, tf.errors.InvalidArgumentError) as e:\n",
    "                print(\"Training complete, entering validation...\")\n",
    "\n",
    "                print(\"Evaluating inputs...\")\n",
    "                valid_examples, valid_labels = sess.run([valid_example_feed, valid_labels_feed])\n",
    "                valid_examples = valid_examples[:, 1:]\n",
    "\n",
    "                print(\"Calculating accuracy...\")\n",
    "                acc = sess.run(accuracy, feed_dict={x: valid_examples,\n",
    "                                                             y_: valid_labels})\n",
    "                print(\"Evaluating validation summary...\")\n",
    "                summary = sess.run(merged,\n",
    "                                   feed_dict={\n",
    "                                       x: valid_examples, y_: valid_labels\n",
    "                                   })\n",
    "\n",
    "                print('Done training for %d epochs, %d steps.' % (num_epochs, step))\n",
    "                # When done, ask the threads to stop.\n",
    "                coord.request_stop()\n",
    "\n",
    "        \n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
