{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical4: Iris Predictions using a Fully-Connected Network\n",
    "=========\n",
    "\n",
    "Author:\n",
    "Jake Dailey\n",
    "\n",
    "**Purpose:** Train a fully-connected neural network to predict Iris classes using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"iris.data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming in and splitting example/label pairs in to training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def datasplitter(filename):\n",
    "    train_rows = 0\n",
    "    valid_rows = 0\n",
    "    test_rows = 0\n",
    "    \n",
    "    if os.path.exists('iristrain'):\n",
    "        shutil.rmtree('iristrain')\n",
    "    os.makedirs('iristrain')\n",
    "    \n",
    "    \n",
    "    if os.path.exists('irisvalid'):\n",
    "        shutil.rmtree('irisvalid')\n",
    "    os.makedirs('irisvalid')\n",
    "    \n",
    "    if os.path.exists('iristest'):\n",
    "        shutil.rmtree('iristest')\n",
    "    os.makedirs('iristest')\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            if len(row)>0:\n",
    "                filtered = (line.replace('\\n', '') for line in row)\n",
    "                filtered = (line.replace(\"\\r\", \"\") for line in filtered)\n",
    "                filtered = (line.replace(\"'\", \"\") for line in filtered)\n",
    "                row = filtered\n",
    "\n",
    "                group = np.random.multinomial(1,[.6,.2,.2])\n",
    "                if np.argmax(group)==0:\n",
    "                    with open('iristrain/iristrain{0}.csv'.format(train_rows), 'w+', newline = None) as trainfile:\n",
    "                        trainwriter = csv.writer(trainfile, delimiter=',')\n",
    "                        trainwriter.writerow(row)\n",
    "                    train_rows += 1\n",
    "\n",
    "\n",
    "                if np.argmax(group)==1:\n",
    "                    with open('irisvalid/irisvalid{0}.csv'.format(valid_rows), 'w+', newline = None) as validfile:\n",
    "                        validwriter = csv.writer(validfile, delimiter=',')\n",
    "                        validwriter.writerow(row)\n",
    "                    valid_rows += 1\n",
    "\n",
    "                if np.argmax(group)==2:\n",
    "                    with open('iristest/iristest{0}.csv'.format(test_rows), 'w+', newline = None) as testfile:\n",
    "                        testwriter = csv.writer(testfile, delimiter=',')\n",
    "                        testwriter.writerow(row)\n",
    "                    test_rows += 1\n",
    "                    \n",
    "    return train_rows, valid_rows, test_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to format inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous valued predicators and one-hot encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file_format(filename_queue, possible_labels):\n",
    "    reader = tf.TextLineReader()\n",
    "    _, value = reader.read(filename_queue)\n",
    "\n",
    "    record_defaults = [tf.constant([],dtype=tf.float32), tf.constant([],dtype=tf.float32),\n",
    "                       tf.constant([],dtype=tf.float32), tf.constant([],dtype=tf.float32),\n",
    "                       tf.constant([],dtype=tf.float32), tf.constant([],dtype=tf.string)]\n",
    "    col1, col2, col3, col4, col5, col6 = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "    \n",
    "    example = tf.stack([col1, col2, col3, col4, col5])\n",
    "    label = tf.one_hot(tf.where(tf.equal(possible_labels, col6))[0], depth = possible_labels.shape[0], on_value = 1, off_value = 0)\n",
    "    label = label[0]\n",
    "    return example, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to format, queue and read inputs in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_pipeline(filenames, possible_labels, batch_size = 3, num_epochs = None, evaluation = False):   \n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        train_set, num_epochs=num_epochs, shuffle=True)\n",
    "\n",
    "    example, label = read_file_format(filename_queue, possible_labels)\n",
    "\n",
    "#     # min_after_dequeue defines how big a buffer we will randomly sample\n",
    "#     #   from -- bigger means better shuffling but slower start up and more\n",
    "#     #   memory used.\n",
    "#     # capacity must be larger than min_after_dequeue and the amount larger\n",
    "#     #   determines the maximum we will prefetch.  Recommendation:\n",
    "#     #   min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
    "    min_after_dequeue = 10\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch, label_batch = tf.train.shuffle_batch(\n",
    "      [example, label], batch_size=batch_size,capacity = capacity, \n",
    "        min_after_dequeue = min_after_dequeue\n",
    "    )    \n",
    "        \n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) is a powerful tool for monitoring training and better understanding the inner workings of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we'll define a helper function for creating layer activation summaries for Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _activation_summary(x):\n",
    "    tensor_name = x.name\n",
    "    tensor_name = tensor_name.replace(':', '_')\n",
    "    tensor_name = tensor_name.replace('(', '_')\n",
    "    tensor_name = tensor_name.replace(')', '_')\n",
    "    tensor_name = tensor_name.replace(' ', '_')\n",
    "\n",
    "    tf.summary.histogram(tensor_name + '/activations', x)\n",
    "    tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the predictive function we're looking to optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, we'll do: linear => sigmoid => linear => log-softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_prediction1(X):  \n",
    "    with tf.variable_scope('sigmoid1') as scope:\n",
    "        weights = tf.Variable(tf.random_normal([4, 3]), name='weights', trainable=True)\n",
    "        biases = tf.Variable(tf.random_normal([3]), name='bias', trainable=True)\n",
    "        lin_y1 = tf.matmul(X, weights) + biases\n",
    "        _activation_summary(lin_y1)\n",
    "        \n",
    "        sig_y1 = tf.sigmoid(lin_y1)\n",
    "        _activation_summary(sig_y1)\n",
    "    \n",
    "    with tf.variable_scope('softmax2') as scope:\n",
    "        weights = tf.Variable(tf.random_normal([3, 3]), name='weights', trainable=True)\n",
    "        biases = tf.Variable(tf.random_normal([3]), name='bias', trainable = True)\n",
    "        lin_y2 = tf.matmul(sig_y1, weights) + biases\n",
    "        _activation_summary(lin_y2)\n",
    "        \n",
    "        smax_num = tf.transpose(tf.exp(lin_y2 - tf.reduce_max(lin_y2)))\n",
    "        smax_den = tf.reduce_sum(tf.exp(lin_y2 - tf.reduce_max(lin_y2)), -1)\n",
    "        softmax_y2 = tf.transpose(tf.divide(smax_num, smax_den))\n",
    "        _activation_summary(softmax_y2)\n",
    "\n",
    "    return softmax_y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also implementing the made-up \"ReQu\" unit; our model is now: linear => ReQu => linear => log-softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_prediction2(X):  \n",
    "    with tf.variable_scope('requ') as scope:\n",
    "        weights = tf.Variable(tf.random_normal([4, 3]), name='weights', trainable=True)\n",
    "        biases = tf.Variable(tf.random_normal([3]), name='bias', trainable=True)\n",
    "        lin_y1 = tf.matmul(X, weights) + biases\n",
    "        _activation_summary(lin_y1)\n",
    "       \n",
    "        requ_y1 = tf.square(tf.maximum(0., lin_y1))\n",
    "        _activation_summary(requ_y1)\n",
    "    \n",
    "    with tf.variable_scope('softmax2') as scope:\n",
    "        weights = tf.Variable(tf.random_normal([3, 3]), name='weights', trainable=True)\n",
    "        biases = tf.Variable(tf.random_normal([3]), name='bias', trainable = True)\n",
    "        lin_y2 = tf.matmul(requ_y1, weights) + biases\n",
    "        _activation_summary(lin_y2)\n",
    "        \n",
    "        smax_num = tf.transpose(tf.exp(lin_y2 - tf.reduce_max(lin_y2)))\n",
    "        smax_den = tf.reduce_sum(tf.exp(lin_y2 - tf.reduce_max(lin_y2)), -1)\n",
    "        softmax_y2 = tf.transpose(tf.divide(smax_num, smax_den))\n",
    "        _activation_summary(softmax_y2)\n",
    "\n",
    "    return softmax_y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note the use of variable_scope so we don't have to name new tf.Variable()'s for every layer; rather we can just keep calling them weights and biases and their \"scope\" will be limited to their specific use in each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying our loss function\n",
    "> #### And giving ourselves Tensorboard summaries to monitor change in loss during SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(logits, labels):\n",
    "    NLLCriterion = -tf.reduce_mean(tf.reduce_sum(tf.multiply(labels, tf.log(logits + 1e-10)), axis=1))\n",
    "\n",
    "    tf.add_to_collection('losses', NLLCriterion)\n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, borrowing a TF function which adds a smoothed loss, to reduce noise in our loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _add_loss_summaries(total_loss):\n",
    "    # Compute the moving average of all individual losses and the total loss.\n",
    "    loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "    losses = tf.get_collection('losses')\n",
    "    loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "\n",
    "    # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "    # same for the averaged version of the losses.\n",
    "    for l in losses + [total_loss]:\n",
    "    # Name each loss as '(raw)' and name the moving average version of the loss\n",
    "    # as the original loss name.\n",
    "        l_name = l.name.replace(\":\", \"_\")\n",
    "        tf.summary.scalar(l_name + '_raw_', l)\n",
    "        tf.summary.scalar(l_name, loss_averages.average(l))\n",
    "\n",
    "    return loss_averages_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying a training operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### We're going to decay our loss rate to avoid jumping over potentially lower error rates in our parameter space. Also using tf.train.MomentumOptimizer() to mitigate the same concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ultimately, our loss function will be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L(X, \\Theta) = -(y_1log(z^4_1) + y_2log(z^4_2) + y_3log(z^4_3))$\n",
    "## $z^4_j = \\frac{e^{z^3_j}}{e^{z^3_1}+e^{z^3_2}+e^{z^3_3}}$\n",
    "## $z^3_j = z^2_j\\Theta^2_j$\n",
    "## $z^2_j = \\frac{1}{1+e^{-z^1_j}}$\n",
    "## $z^1_j = z^0_i\\Theta^1_j$\n",
    "## $z^0_i = x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can find the Jacobian of the loss w.r.t. the data as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\frac{\\partial L}{\\partial x_i} = -(\\frac{y_1}{z^4_1}\\frac{\\partial z^4_1}{\\partial x_i} + \\frac{y_2}{z^4_2}\\frac{\\partial z^4_2}{\\partial x_i} + \\frac{y_3}{z^4_3}\\frac{\\partial z^4_3}{\\partial x_i})$\n",
    "#### Now, just focusing in on z's from one class for simplicity:\n",
    "# $\\frac{\\partial z^4_j}{\\partial x_i} = \\frac{z^3_je^{z^3_j}\\frac{\\partial z^3_j}{\\partial x_i} \\centerdot (e^{z^3_j} + e^{z^3_{j+1}}+e^{z^3_{j+2}})-e^{z^3_j} \\centerdot (z^3_je^{z^3_j}\\frac{\\partial z^3_j}{\\partial x_i} +z^3_{j+1}e^{z^3_{j+1}}\\frac{\\partial z^3_{j+1}}{\\partial x_i}+z^3_{j+2}e^{z^3_{j+2}}\\frac{\\partial z^3_{j+2}}{\\partial x_i})}{(e^{z^3_j}+e^{z^3_{j+1}}+e^{z^3_{j+2}})^2}$\n",
    "# $\\frac{\\partial z^3_j}{\\partial x_i} = \\Theta^2_j \\centerdot \\frac{\\partial z^2_j}{\\partial x_i}$\n",
    "# $\\frac{\\partial z^2_j}{\\partial x_i} = \\frac{z^1_je^{-z^1_j}}{(1+e^{-z^1_j})^2} \\centerdot \\frac{\\partial z^1_j}{\\partial x_i}$\n",
    "# $\\frac{\\partial z^1_j}{\\partial x_i} = \\Theta^1_j \\centerdot \\frac{\\partial z^0}{\\partial x_i}$\n",
    "# $\\frac{\\partial z^0}{\\partial x_i} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rather than code the entire gradient given above, we can use Tensorflow's compute_gradients() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(total_loss, global_step):\n",
    " \n",
    "    # Variables that affect learning rate.\n",
    "    num_batches_per_epoch = num_examples_per_train_epoch / batch_size\n",
    "    decay_steps = int(num_batches_per_epoch * num_epochs_to_decay)\n",
    "\n",
    "    # Decay the learning rate exponentially based on the number of steps.\n",
    "    lr = tf.train.exponential_decay(\n",
    "        initial_learning_rate, global_step,\n",
    "        decay_steps, learning_rate_decay_factor, staircase=True)\n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "\n",
    "    # Generate moving averages of all losses and associated summaries.\n",
    "    loss_averages_op = _add_loss_summaries(total_loss)\n",
    "\n",
    "    # Compute gradients.\n",
    "    with tf.control_dependencies([loss_averages_op]):\n",
    "        opt = tf.train.MomentumOptimizer(lr, momentum)\n",
    "        grads = opt.compute_gradients(total_loss)\n",
    "        \n",
    "    # Apply gradients.\n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "    # Add histograms for trainable variables.\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "    # Add histograms for gradients.\n",
    "    for grad, var in grads:\n",
    "        if grad is not None:\n",
    "            tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "\n",
    "    # Track the moving averages of all trainable variables.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        moving_average_decay, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(y_hat, y_):\n",
    "    correct = tf.equal(tf.argmax(y_hat, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    tf.summary.scalar('validation_accuracy', accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With all of our functions defined, let's get to work!\n",
    "## First, we'll split our data using the function we defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_len, valid_len, test_len = datasplitter(filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, because we know our dataset is small enough to read, we can test our input_pipeline() to ensure our model will be fed data correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = ['iristrain/iristrain{0}.csv'.format(i) for i in range(train_len)]\n",
    "valid_set = ['irisvalid/irisvalid{0}.csv'.format(i) for i in range(valid_len)]\n",
    "test_set = ['iristest/iristest{0}.csv'.format(i) for i in range(test_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(train_len):\n",
    "    if i == 0:\n",
    "        debug = pd.read_csv('iristrain/iristrain{0}.csv'.format(i), header = None)\n",
    "    else:\n",
    "        temp = pd.read_csv('iristrain/iristrain{0}.csv'.format(i), header = None)\n",
    "        debug = debug.append(temp)\n",
    "debug_ex = debug.ix[:,:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll just do one \"epoch\" of reading in data and append the fed-in data to a dataframe to test our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "batch_size = 3\n",
    "\n",
    "pipe_ex = debug_ex.iloc[0:0,:]\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    unique_labels = tf.constant([\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"], dtype=tf.string)\n",
    "    \n",
    "    example_feed, labels_feed = input_pipeline(train_set, possible_labels = unique_labels,\n",
    "                                               batch_size = batch_size, num_epochs = num_epochs)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Start populating the filename queue.\n",
    "        \n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()  \n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "        \n",
    "        step = 0\n",
    "        while not coord.should_stop():\n",
    "            try:\n",
    "                start_time = time.time()                \n",
    "               \n",
    "                example_batch, label_batch = sess.run([example_feed, labels_feed])\n",
    "                \n",
    "                pipe_ex = pipe_ex.append(pd.DataFrame(example_batch))\n",
    "                \n",
    "                duration = time.time() - start_time\n",
    "                step += 1\n",
    "\n",
    "            except (tf.errors.OutOfRangeError, tf.errors.InvalidArgumentError) as e:\n",
    "           \n",
    "                print('Done training for %d epochs, %d steps.' % (num_epochs, step))\n",
    "                # When done, ask the threads to stop.\n",
    "                coord.request_stop()\n",
    "\n",
    "        \n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for alignment between both methods of reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "debug_ex.ix[:,0].astype(float).reset_index(drop=True).equals((pipe_ex.sort_values(0).reset_index(drop=True).ix[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we'll set some of our model's hyperparameters as global variables to avoid passing arguments to all of the functions defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global num_examples_per_train_epoch\n",
    "num_examples_per_train_epoch = train_len\n",
    "\n",
    "global num_epochs\n",
    "num_epochs= 10\n",
    "\n",
    "global batch_size\n",
    "batch_size = 3\n",
    "\n",
    "global moving_average_decay\n",
    "moving_average_decay = 0.9999   \n",
    "\n",
    "global num_epochs_to_decay\n",
    "num_epochs_to_decay = 5  \n",
    "\n",
    "global learning_rate_decay_factor\n",
    "learning_rate_decay_factor = 0.001  \n",
    "\n",
    "global initial_learning_rate\n",
    "initial_learning_rate = 0.01\n",
    "\n",
    "global momentum\n",
    "momentum = 0.05\n",
    "\n",
    "global logdir\n",
    "logdir = 'TF_Logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note here I'm defining \"logdir\", which is the name of the folder I'd like my Tensorboard files to be written to. From a new command prompt, I can type,\n",
    "#### tensorboard --logdir=TF_Logs\n",
    "#### and navigate to the IP:port it provides to get a closer look at my model and the training procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, the moment we've all been waiting for: training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I'm going to construct a new tf.Graph() and fill it with all of the pieces I specified earlier on.\n",
    "### Then, once I'm happy with how my graph is defined, I'll open a new tf.Session(), initialize all of the variables used throughout my pipeline\n",
    ">#### Noteworthy: Tensorflow doesn't just use variables in the \"model\" as we usually understand it; instead, the entire pipeline is viewed as part of the graph, which allows us to make use of relatively flexible modules, but means we need to initialize variables only after they've all been specified to the graph but before they're used by any other functions\n",
    "\n",
    "### Now, we can run our pipeline just as we did with the test pipeline\n",
    ">#### Except now our examples will be passed to our training procedure and we can monitor training progress using merged summaries, written using writer below and read using Tensorboard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "    unique_labels = tf.constant([\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"], dtype=tf.string)\n",
    "    \n",
    "    example_feed, labels_feed = input_pipeline(train_set, possible_labels=unique_labels,\n",
    "                                               batch_size=batch_size, num_epochs=num_epochs)\n",
    "    \n",
    "    valid_example_feed, valid_labels_feed = input_pipeline(\n",
    "        valid_set, possible_labels=unique_labels,\n",
    "        batch_size=valid_len, num_epochs=1)\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "    y_ = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "    \n",
    "    y_hat = make_prediction1(x)\n",
    "    \n",
    "    loss = calculate_loss(y_hat, y_)\n",
    "    \n",
    "    train_op = train(loss, global_step=global_step)\n",
    "    \n",
    "    accuracy = evaluate_accuracy(y_hat, y_)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Start populating the filename queue.\n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()  \n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "\n",
    "        step = 0\n",
    "        while not coord.should_stop():\n",
    "            try:\n",
    "                start_time = time.time()                \n",
    "\n",
    "                example_batch, label_batch = sess.run([example_feed, labels_feed])\n",
    "                example_batch = example_batch[:, 1:]\n",
    "\n",
    "                result, summary =  sess.run([train_op, merged],\n",
    "                                            feed_dict={x: example_batch,\n",
    "                                                       y_: label_batch})\n",
    "                writer.add_summary(summary, global_step.eval())\n",
    "\n",
    "                step += 1\n",
    "                print(step)\n",
    "                duration = time.time() - start_time\n",
    "                print(duration)\n",
    "\n",
    "            except (tf.errors.OutOfRangeError, tf.errors.InvalidArgumentError) as e:\n",
    "                print(\"Training complete, entering validation...\")\n",
    "\n",
    "                print(\"Evaluating inputs...\")\n",
    "                valid_examples, valid_labels = sess.run([valid_example_feed, valid_labels_feed])\n",
    "                valid_examples = valid_examples[:, 1:]\n",
    "\n",
    "                print(\"Calculating accuracy...\")\n",
    "                acc = sess.run(accuracy, feed_dict={x: valid_examples,\n",
    "                                                             y_: valid_labels})\n",
    "                print(\"Evaluating validation summary...\")\n",
    "                summary = sess.run(merged,\n",
    "                                   feed_dict={\n",
    "                                       x: valid_examples, y_: valid_labels\n",
    "                                   })\n",
    "\n",
    "                print('Done training for %d epochs, %d steps.' % (num_epochs, step))\n",
    "                # When done, ask the threads to stop.\n",
    "                coord.request_stop()\n",
    "\n",
    "        \n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking what percent of validation examples we got right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here, testing a second architecture with a \"ReQu\" unit, per practical4's requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "    unique_labels = tf.constant([\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"], dtype=tf.string)\n",
    "    \n",
    "    example_feed, labels_feed = input_pipeline(train_set, possible_labels=unique_labels,\n",
    "                                               batch_size=batch_size, num_epochs=num_epochs)\n",
    "    \n",
    "    valid_example_feed, valid_labels_feed = input_pipeline(\n",
    "        valid_set, possible_labels=unique_labels,\n",
    "        batch_size=valid_len, num_epochs=1)\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "    y_ = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "    \n",
    "    y_hat = make_prediction2(x)\n",
    "    \n",
    "    loss = calculate_loss(y_hat, y_)\n",
    "    \n",
    "    train_op = train(loss, global_step=global_step)\n",
    "    step = 0\n",
    "    \n",
    "    accuracy = evaluate_accuracy(y_hat, y_)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Start populating the filename queue.\n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()  \n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "\n",
    "        step = 0\n",
    "        while not coord.should_stop():\n",
    "            try:\n",
    "                start_time = time.time()                \n",
    "\n",
    "                example_batch, label_batch = sess.run([example_feed, labels_feed])\n",
    "                example_batch = example_batch[:, 1:]\n",
    "\n",
    "                result, summary =  sess.run([train_op, merged],\n",
    "                                            feed_dict={x: example_batch,\n",
    "                                                       y_: label_batch})\n",
    "                writer.add_summary(summary, global_step.eval())\n",
    "\n",
    "                step += 1\n",
    "                print(step)\n",
    "                duration = time.time() - start_time\n",
    "                print(duration)\n",
    "\n",
    "            except (tf.errors.OutOfRangeError, tf.errors.InvalidArgumentError) as e:\n",
    "                print(\"Training complete, entering validation...\")\n",
    "\n",
    "                print(\"Evaluating inputs...\")\n",
    "                valid_examples, valid_labels = sess.run([valid_example_feed, valid_labels_feed])\n",
    "                valid_examples = valid_examples[:, 1:]\n",
    "\n",
    "                print(\"Calculating accuracy...\")\n",
    "                acc = sess.run(accuracy, feed_dict={x: valid_examples,\n",
    "                                                             y_: valid_labels})\n",
    "                print(\"Evaluating validation summary...\")\n",
    "                summary = sess.run(merged,\n",
    "                                   feed_dict={\n",
    "                                       x: valid_examples, y_: valid_labels\n",
    "                                   })\n",
    "\n",
    "                print('Done training for %d epochs, %d steps.' % (num_epochs, step))\n",
    "                # When done, ask the threads to stop.\n",
    "                coord.request_stop()\n",
    "\n",
    "        \n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing validation accuracy on this second network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
