{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename= \"iris.data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(filename) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "        data_len = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def datasplitter(filename):\n",
    "    train_rows = 0\n",
    "    valid_rows = 0\n",
    "    test_rows = 0\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            group = np.random.multinomial(1,[.6,.1,.3])\n",
    "            if np.argmax(group)==0:\n",
    "                if train_rows == 0:\n",
    "                    with open('iristrain.csv', 'w', newline='') as trainfile:\n",
    "                        trainwriter = csv.writer(trainfile, delimiter=',')\n",
    "                        trainwriter.writerow(row)\n",
    "                    train_rows += 1\n",
    "                else:\n",
    "                    with open('iristrain.csv', 'a', newline='') as trainfile:\n",
    "                        trainwriter = csv.writer(trainfile, delimiter=',')\n",
    "                        trainwriter.writerow(row)\n",
    "                    train_rows += 1\n",
    "\n",
    "            if np.argmax(group)==1:\n",
    "                if valid_rows == 0:\n",
    "                    with open('irisvalid.csv', 'w', newline='') as validfile:\n",
    "                        validwriter = csv.writer(validfile, delimiter=',')\n",
    "                        validwriter.writerow(row)\n",
    "                    valid_rows += 1\n",
    "                else:\n",
    "                    with open('irisvalid.csv', 'a', newline='') as validfile:\n",
    "                        validwriter = csv.writer(validfile, delimiter=',')\n",
    "                        validwriter.writerow(row)\n",
    "                    valid_rows += 1\n",
    "\n",
    "            if np.argmax(group)==2:\n",
    "                if test_rows == 0:\n",
    "                    with open('iristest.csv', 'w', newline='') as testfile:\n",
    "                        testwriter = csv.writer(testfile, delimiter=',')\n",
    "                        testwriter.writerow(row)\n",
    "                    test_rows += 1\n",
    "                else:\n",
    "                    with open('iristest.csv', 'a', newline='') as testfile:\n",
    "                        testwriter = csv.writer(testfile, delimiter=',')\n",
    "                        testwriter.writerow(row)\n",
    "                    test_rows += 1\n",
    "    return train_rows, valid_rows, test_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_len, valid_len, test_len = datasplitter(filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainfile = \"iristrain.csv\"\n",
    "validfile = \"irisvalid.csv\"\n",
    "testfile = \"iristest.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_labels = tf.constant([\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"],dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_file_format(filename_queue, unique_labels):\n",
    "    reader = tf.TextLineReader()\n",
    "    key, value = reader.read(filename_queue)\n",
    "    \n",
    "    record_defaults = [[1.], [1.], [1.], [1.], ['None']]\n",
    "    col1, col2, col3, col4, col5 = tf.decode_csv(\n",
    "        value, record_defaults=record_defaults)\n",
    "    \n",
    "    example = tf.stack([col1, col2, col3, col4])\n",
    "    label = tf.one_hot(tf.where(tf.equal(unique_labels,col5))[0],3)\n",
    "    label = label[0]\n",
    "    return example, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def input_pipeline(filenames, unique_labels, batch_size = 3, num_epochs = None, evaluation = False):\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        filenames, num_epochs=num_epochs, shuffle=True)\n",
    "    \n",
    "    example, label = read_file_format(filename_queue, unique_labels)\n",
    "    # min_after_dequeue defines how big a buffer we will randomly sample\n",
    "    #   from -- bigger means better shuffling but slower start up and more\n",
    "    #   memory used.\n",
    "    # capacity must be larger than min_after_dequeue and the amount larger\n",
    "    #   determines the maximum we will prefetch.  Recommendation:\n",
    "    #   min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
    "    min_after_dequeue = 10\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch, label_batch = tf.train.shuffle_batch(\n",
    "      [example, label], batch_size=batch_size,capacity = capacity, \n",
    "        min_after_dequeue = min_after_dequeue\n",
    "    )\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fill_feed_dict(filename, example_pl, label_pl, batch_size, num_epochs = None, evaluation = False):\n",
    "    # Create the feed_dict for the placeholders filled with the next\n",
    "    # `batch size` examples.\n",
    "\n",
    "    images_feed, labels_feed = input_pipeline(\n",
    "        [filename], unique_labels = unique_labels, batch_size = batch_size, num_epochs = num_epochs\n",
    "    )\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "    \n",
    "    images_feed, labels_feed = sess.run([images_feed,labels_feed])\n",
    "    \n",
    "    feed_dict = {\n",
    "      example_pl: images_feed,\n",
    "      label_pl: labels_feed\n",
    "    }\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_dict = fill_feed_dict(filename=trainfile, example_pl = x, label_pl = y_, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining weights and bias for the first node;  4 features * 3 outputs gives 12 weights and 3 bias units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_normal([4,3]).eval(), name='weights', trainable=True)\n",
    "b1 = tf.Variable(tf.random_normal([3]).eval(), name = 'bias', trainable = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting with a linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lin_y1 = tf.matmul(x,W1) + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.7959137 , -4.64607   , -2.67322135],\n",
       "       [-6.24301624, -5.15588379, -2.4481957 ],\n",
       "       [-6.36992073, -4.99537754, -2.40721965]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(lin_y1, feed_dict = fill_feed_dict(filename=trainfile, example_pl = x, label_pl = y_, batch_size = batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now feeding the linear layer into a sigmoid layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sig_y1 = tf.sigmoid(lin_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00144458,  0.00423682,  0.09642544],\n",
       "       [ 0.00170937,  0.00672365,  0.08262381],\n",
       "       [ 0.00318136,  0.00917211,  0.10453955]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(sig_y1, feed_dict = fill_feed_dict(filename=trainfile, example_pl = x, label_pl = y_, batch_size = batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2 = tf.Variable(tf.random_normal([3,3]).eval(), name='weights', trainable=True)\n",
    "b2 = tf.Variable(tf.random_normal([3]).eval(), name = 'bias', trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lin_y2 = tf.matmul(sig_y1,W2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.71941239, -0.45133749,  1.46989536],\n",
       "       [ 0.75678384, -0.4437328 ,  1.51763082],\n",
       "       [ 0.6840266 , -0.45896322,  1.4217186 ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(lin_y2, feed_dict = fill_feed_dict(filename=trainfile, example_pl = x, label_pl = y_, batch_size = batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smax_num = tf.transpose(tf.exp(lin_y2-tf.reduce_max(lin_y2)))\n",
    "smax_den = tf.reduce_sum(tf.exp(lin_y2-tf.reduce_max(lin_y2)),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax_y2 = tf.transpose(tf.divide(smax_num, smax_den))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.29309905,  0.09423386,  0.61266714],\n",
       "       [ 0.29325789,  0.09350947,  0.61323261],\n",
       "       [ 0.29386827,  0.0961777 ,  0.60995406]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(softmax_y2, feed_dict = fill_feed_dict(filename=trainfile, example_pl = x, label_pl = y_, batch_size = batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NLLCriterion = -tf.reduce_mean(tf.reduce_sum(tf.multiply(y_,tf.log(softmax_y2+1e-10)),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2290734"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(NLLCriterion, feed_dict = fill_feed_dict(filename=trainfile, example_pl = x, label_pl = y_, batch_size = batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = .5\n",
    "momentum = .15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.MomentumOptimizer(learning_rate,momentum).minimize(NLLCriterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost_tracker = pd.DataFrame([None,None,None,None],\n",
    "                                  index = ['Iteration','Training Cost','Training Precision', 'Validation Cost']).T\n",
    "iter_count = 0\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_epochs = 10\n",
    "steps_per_train_epoch = train_len // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "irreg_batch_size = round((train_len/batch_size - round(train_len/batch_size))*batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(training_epochs):\n",
    "    for j in range(steps_per_train_epoch):\n",
    "        if (j < steps_per_train_epoch) | (train_len/batch_size - round(train_len/batch_size) == 0):\n",
    "            batch_dict = fill_feed_dict(trainfile, x, y_, batch_size)\n",
    "        else:\n",
    "            batch_dict = fill_feed_dict(trainfile, x, y_, irreg_batch_size)\n",
    "\n",
    "        sess.run(train_step,feed_dict = batch_dict)\n",
    "        \n",
    "        iter_count += 1\n",
    "        batch_loss = sess.run(NLLCriterion, feed_dict = batch_dict)\n",
    "        losses.append(batch_loss)\n",
    "\n",
    "        if j == steps_per_train_epoch:\n",
    "            print(\"Evaluating against training set.\")\n",
    "            train_cost = sess.run(NLLCriterion, feed_dict = fill_feed_dict(trainfile, x, y_, train_len))\n",
    "\n",
    "            correct = tf.equal(tf.argmax(softmax_y2,1), tf.argmax(y_,1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "            acc = sess.run(accuracy,feed_dict = fill_feed_dict(trainfile, x, y_, train_len))\n",
    "            \n",
    "            ct_row = pd.DataFrame([iter_count+1, train_cost, acc],\n",
    "                                  index = ['Iteration', 'Training Cost', 'Training Accuracy']).T\n",
    "            cost_tracker = cost_tracker.append(ct_row)\n",
    "\n",
    "            print(\"Iteration:\", '%04d' % (iter_count),\"Training Accuracy=\",\"{:.9f}\".format(acc),\"Batch Loss=\",\"{:.9f}\".format(batch_loss))\n",
    "            print(\"Training cost=\", \"{:.9f}\".format(train_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(cost_tracker['Iteration'],cost_tracker['Training Cost'])\n",
    "plt.plot(cost_tracker['Iteration'],cost_tracker['Validation Cost'])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(cost_tracker['Iteration'],cost_tracker['Training Accuracy'])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_cost = sess.run(NLLCriterion, feed_dict = fill_feed_dict(validfile, x, y_, valid_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
