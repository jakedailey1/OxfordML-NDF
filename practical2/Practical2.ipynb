{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- The below data are from an example in Schaum's Outline:\n",
    "-- Dominick Salvator and Derrick Reagle\n",
    "-- Shaum's Outline of Theory and Problems of Statistics and Economics\n",
    "-- 2nd edition\n",
    "-- McGraw-Hill\n",
    "-- 2002\n",
    "\n",
    "-- The data relate the amount of corn produced, given certain amounts\n",
    "-- of fertilizer and insecticide. See p 157 of the text.\n",
    "\n",
    "-- In this example, we want to be able to predict the amount of\n",
    "-- corn produced, given the amount of fertilizer and insecticide used.\n",
    "-- In other words: fertilizer & insecticide are our two input variables,\n",
    "-- and corn is our target value.\n",
    "\n",
    "--  {corn, fertilizer, insecticide}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    [40,  6,  4],\n",
    "    [44, 10,  4],\n",
    "    [46, 12,  5],\n",
    "    [48, 14,  7],\n",
    "    [52, 16,  9],\n",
    "    [58, 18, 12],\n",
    "    [60, 22, 14],\n",
    "    [68, 24, 20],\n",
    "    [74, 26, 21],\n",
    "    [80, 32, 24]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns = ['corn','fertilizer','insecticide'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[mask]\n",
    "df_test = df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RESPONSE = ['corn']\n",
    "CONTINUOUS = ['fertilizer','insecticide']\n",
    "CATEGORICAL = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(df):\n",
    "    # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "    # the values of that column stored in a constant Tensor.\n",
    "    continuous_cols = {k: tf.constant(df[k].values,dtype=tf.float32)\n",
    "                     for k in CONTINUOUS}\n",
    "    # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "    # to the values of that column stored in a tf.SparseTensor.\n",
    "    categorical_cols = {k: tf.SparseTensor(\n",
    "      indices=[[i, 0] for i in range(df[k].size)],\n",
    "      values=df[k].values,\n",
    "      shape=[df[k].size, 1])\n",
    "                      for k in CATEGORICAL}\n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = {**continuous_cols, **categorical_cols}\n",
    "    # Converts the label column into a constant Tensor.\n",
    "    response = tf.constant(df[RESPONSE].values,dtype=tf.float32)\n",
    "    # Returns the feature columns and the label.\n",
    "    return feature_cols, response\n",
    "\n",
    "def train_input_fn():\n",
    "    return input_fn(df_train)\n",
    "\n",
    "def eval_input_fn():\n",
    "    return input_fn(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-level implementation using tf.contrib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fertilizer = tf.contrib.layers.real_valued_column('fertilizer')\n",
    "insecticide = tf.contrib.layers.real_valued_column('insecticide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_steps': None, '_task_id': 0, '_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000000E2553DDF28>, '_save_checkpoints_secs': 600, '_task_type': None, '_keep_checkpoint_max': 5, '_is_chief': True, '_num_ps_replicas': 0, '_keep_checkpoint_every_n_hours': 10000, '_environment': 'local', '_tf_random_seed': None, '_evaluation_master': '', '_save_summary_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model_dir = tempfile.mkdtemp()\n",
    "m = tf.contrib.learn.LinearRegressor(\n",
    "    feature_columns=[\n",
    "        fertilizer,insecticide\n",
    "    ],\n",
    "    model_dir = model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:From C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\DailJa01\\AppData\\Local\\Temp\\tmpxvnt6u6v\\model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 2881.0\n",
      "INFO:tensorflow:global_step/sec: 1523.27\n",
      "INFO:tensorflow:step = 101, loss = 204.757\n",
      "INFO:tensorflow:Saving checkpoints for 200 into C:\\Users\\DailJa01\\AppData\\Local\\Temp\\tmpxvnt6u6v\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 187.993.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegressor(params={'joint_weights': False, 'feature_columns': [_RealValuedColumn(column_name='fertilizer', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='insecticide', dimension=1, default_value=None, dtype=tf.float32, normalizer=None)], 'gradient_clip_norm': None, 'optimizer': None, 'head': <tensorflow.contrib.learn.python.learn.estimators.head._RegressionHead object at 0x000000E2552E26D8>})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(input_fn=train_input_fn,steps = 200.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:From C:\\Users\\DailJa01\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-03-05-00:44:40\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-03-05-00:44:41\n",
      "INFO:tensorflow:Saving dict for global step 200: global_step = 200, loss = 134.326\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "{'loss': 134.3257, 'global_step': 200}\n"
     ]
    }
   ],
   "source": [
    "results = m.evaluate(input_fn = eval_input_fn, steps = 1.)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-d18fa51c94f4>:1: LinearRegressor.weights_ (from tensorflow.contrib.learn.python.learn.estimators.linear) is deprecated and will be removed after 2016-10-30.\n",
      "Instructions for updating:\n",
      "This method will be removed after the deprecation date. To inspect variables, use get_variable_names() and get_variable_value().\n",
      "WARNING:tensorflow:From <ipython-input-11-d18fa51c94f4>:2: LinearRegressor.bias_ (from tensorflow.contrib.learn.python.learn.estimators.linear) is deprecated and will be removed after 2016-10-30.\n",
      "Instructions for updating:\n",
      "This method will be removed after the deprecation date. To inspect variables, use get_variable_names() and get_variable_value().\n"
     ]
    }
   ],
   "source": [
    "m_weights = [i[0][0] for i in m.weights_.values()]\n",
    "m_bias = m.bias_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.942759, 1.4766939]\n",
      "3.03292\n"
     ]
    }
   ],
   "source": [
    "print(m_weights)\n",
    "print(m_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, digging deeper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_resp = tf.constant(df_train[['corn']].values,shape=(len(df_train),1),dtype=tf.float32)\n",
    "train_ft = tf.constant(df_train.drop('corn', axis=1).values,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_resp = tf.constant(df_test[['corn']].values,shape=(len(df_test),1),dtype=tf.float32)\n",
    "test_ft = tf.constant(df_test.drop('corn', axis=1).values,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing placeholder for response & features; initializing variables for model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,shape = [None,2])\n",
    "y_ = tf.placeholder(tf.float32,shape = [None,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_normal([1,2]),name='weights', trainable=True)\n",
    "b1 = tf.Variable(tf.zeros([1]),name='bias', trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.35243195, -0.16925181]], dtype=float32),\n",
       " array([ 0.], dtype=float32)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_y = tf.add(tf.matmul(x, tf.transpose(W1)), b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define L2 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = tf.reduce_sum(tf.pow(model_y - y_, 2))/(2*len(train_resp.eval()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1866.8838"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(criterion,feed_dict = {x:train_ft.eval(),y_:train_resp.eval()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the gradient of the loss w.r.t. our parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dl_dB = tf.gradients(criterion, [W1,b1])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-962.43774414, -689.33428955]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(dl_dB,feed_dict = {x:train_ft.eval(),y_:train_resp.eval()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the gradient of the loss w.r.t. the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dl_dx = tf.gradients(criterion,x)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.77028179,  1.8106389 ],\n",
       "       [ 4.24692202,  2.03954053],\n",
       "       [ 5.84814978,  2.80851364],\n",
       "       [ 7.03484106,  3.3784101 ]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(dl_dx,feed_dict = {x:train_ft.eval(),y_:train_resp.eval()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding update procedure to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_iterations = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 training cost= 95.793487549 W= [[ 2.51591182  0.76748139]] b= [ 0.48090363]\n",
      "Epoch: 0050 test cost= 85.390640259 W= [[ 2.51591182  0.76748139]] b= [ 0.48090363]\n",
      "Epoch: 0100 training cost= 81.106338501 W= [[ 2.98711801  0.1254739 ]] b= [ 0.80018961]\n",
      "Epoch: 0100 test cost= 74.600921631 W= [[ 2.98711801  0.1254739 ]] b= [ 0.80018961]\n",
      "Epoch: 0150 training cost= 70.353988647 W= [[ 3.38394141 -0.41676134]] b= [ 1.09209573]\n",
      "Epoch: 0150 test cost= 69.575279236 W= [[ 3.38394141 -0.41676134]] b= [ 1.09209573]\n",
      "Epoch: 0200 training cost= 62.447483063 W= [[ 3.71762013 -0.87429124]] b= [ 1.36065149]\n",
      "Epoch: 0200 test cost= 68.239273071 W= [[ 3.71762013 -0.87429124]] b= [ 1.36065149]\n",
      "Epoch: 0250 training cost= 56.600425720 W= [[ 3.99769855 -1.25991118]] b= [ 1.60928118]\n",
      "Epoch: 0250 test cost= 69.169715881 W= [[ 3.99769855 -1.25991118]] b= [ 1.60928118]\n",
      "Epoch: 0300 training cost= 52.244888306 W= [[ 4.23227739 -1.58448386]] b= [ 1.84089136]\n",
      "Epoch: 0300 test cost= 71.400871277 W= [[ 4.23227739 -1.58448386]] b= [ 1.84089136]\n",
      "Epoch: 0350 training cost= 48.970428467 W= [[ 4.42823935 -1.85723436]] b= [ 2.05795097]\n",
      "Epoch: 0350 test cost= 74.289230347 W= [[ 4.42823935 -1.85723436]] b= [ 2.05795097]\n",
      "Epoch: 0400 training cost= 46.480426788 W= [[ 4.59142733 -2.08599758]] b= [ 2.26255488]\n",
      "Epoch: 0400 test cost= 77.413681030 W= [[ 4.59142733 -2.08599758]] b= [ 2.26255488]\n",
      "Epoch: 0450 training cost= 44.560409546 W= [[ 4.72680044 -2.27742171]] b= [ 2.4564867]\n",
      "Epoch: 0450 test cost= 80.507934570 W= [[ 4.72680044 -2.27742171]] b= [ 2.4564867]\n",
      "Epoch: 0500 training cost= 43.055156708 W= [[ 4.83857298 -2.43715549]] b= [ 2.64125776]\n",
      "Epoch: 0500 test cost= 83.411560059 W= [[ 4.83857298 -2.43715549]] b= [ 2.64125776]\n",
      "Epoch: 0550 training cost= 41.852413177 W= [[ 4.93031788 -2.5699935 ]] b= [ 2.81815028]\n",
      "Epoch: 0550 test cost= 86.034133911 W= [[ 4.93031788 -2.5699935 ]] b= [ 2.81815028]\n",
      "Epoch: 0600 training cost= 40.870754242 W= [[ 5.0050745  -2.68000722]] b= [ 2.98826075]\n",
      "Epoch: 0600 test cost= 88.334152222 W= [[ 5.0050745  -2.68000722]] b= [ 2.98826075]\n",
      "Epoch: 0650 training cost= 40.051216125 W= [[ 5.06542015 -2.77065444]] b= [ 3.15251303]\n",
      "Epoch: 0650 test cost= 90.299789429 W= [[ 5.06542015 -2.77065444]] b= [ 3.15251303]\n",
      "Epoch: 0700 training cost= 39.350971222 W= [[ 5.11354589 -2.84487009]] b= [ 3.31169462]\n",
      "Epoch: 0700 test cost= 91.939353943 W= [[ 5.11354589 -2.84487009]] b= [ 3.31169462]\n",
      "Epoch: 0750 training cost= 38.738952637 W= [[ 5.15130997 -2.90514803]] b= [ 3.46647692]\n",
      "Epoch: 0750 test cost= 93.271446228 W= [[ 5.15130997 -2.90514803]] b= [ 3.46647692]\n",
      "Epoch: 0800 training cost= 38.192523956 W= [[ 5.18029022 -2.95360231]] b= [ 3.61742806]\n",
      "Epoch: 0800 test cost= 94.321617126 W= [[ 5.18029022 -2.95360231]] b= [ 3.61742806]\n",
      "Epoch: 0850 training cost= 37.695224762 W= [[ 5.20182705 -2.99203229]] b= [ 3.76503158]\n",
      "Epoch: 0850 test cost= 95.117134094 W= [[ 5.20182705 -2.99203229]] b= [ 3.76503158]\n",
      "Epoch: 0900 training cost= 37.235000610 W= [[ 5.21705914 -3.02196169]] b= [ 3.90970206]\n",
      "Epoch: 0900 test cost= 95.687133789 W= [[ 5.21705914 -3.02196169]] b= [ 3.90970206]\n",
      "Epoch: 0950 training cost= 36.803100586 W= [[ 5.22695255 -3.0446887 ]] b= [ 4.05178213]\n",
      "Epoch: 0950 test cost= 96.058280945 W= [[ 5.22695255 -3.0446887 ]] b= [ 4.05178213]\n",
      "Epoch: 1000 training cost= 36.393058777 W= [[ 5.2323246  -3.06130934]] b= [ 4.19157505]\n",
      "Epoch: 1000 test cost= 96.255889893 W= [[ 5.2323246  -3.06130934]] b= [ 4.19157505]\n",
      "Epoch: 1050 training cost= 36.000160217 W= [[ 5.23387527 -3.07275867]] b= [ 4.32932854]\n",
      "Epoch: 1050 test cost= 96.304199219 W= [[ 5.23387527 -3.07275867]] b= [ 4.32932854]\n",
      "Epoch: 1100 training cost= 35.620922089 W= [[ 5.23219585 -3.07983327]] b= [ 4.46525574]\n",
      "Epoch: 1100 test cost= 96.223464966 W= [[ 5.23219585 -3.07983327]] b= [ 4.46525574]\n",
      "Epoch: 1150 training cost= 35.252712250 W= [[ 5.22778606 -3.08320212]] b= [ 4.59954643]\n",
      "Epoch: 1150 test cost= 96.032630920 W= [[ 5.22778606 -3.08320212]] b= [ 4.59954643]\n",
      "Epoch: 1200 training cost= 34.893642426 W= [[ 5.22107315 -3.08343935]] b= [ 4.73235512]\n",
      "Epoch: 1200 test cost= 95.747756958 W= [[ 5.22107315 -3.08343935]] b= [ 4.73235512]\n",
      "Epoch: 1250 training cost= 34.542270660 W= [[ 5.21241951 -3.08102894]] b= [ 4.86381531]\n",
      "Epoch: 1250 test cost= 95.384048462 W= [[ 5.21241951 -3.08102894]] b= [ 4.86381531]\n",
      "Epoch: 1300 training cost= 34.197528839 W= [[ 5.20213175 -3.07638335]] b= [ 4.99403858]\n",
      "Epoch: 1300 test cost= 94.953460693 W= [[ 5.20213175 -3.07638335]] b= [ 4.99403858]\n",
      "Epoch: 1350 training cost= 33.858581543 W= [[ 5.1904707  -3.06985259]] b= [ 5.12312078]\n",
      "Epoch: 1350 test cost= 94.466796875 W= [[ 5.1904707  -3.06985259]] b= [ 5.12312078]\n",
      "Epoch: 1400 training cost= 33.524826050 W= [[ 5.17765903 -3.06173468]] b= [ 5.25114632]\n",
      "Epoch: 1400 test cost= 93.933685303 W= [[ 5.17765903 -3.06173468]] b= [ 5.25114632]\n",
      "Epoch: 1450 training cost= 33.195789337 W= [[ 5.16388178 -3.05227947]] b= [ 5.37818384]\n",
      "Epoch: 1450 test cost= 93.361564636 W= [[ 5.16388178 -3.05227947]] b= [ 5.37818384]\n",
      "Epoch: 1500 training cost= 32.871070862 W= [[ 5.14930153 -3.04170251]] b= [ 5.50429344]\n",
      "Epoch: 1500 test cost= 92.758117676 W= [[ 5.14930153 -3.04170251]] b= [ 5.50429344]\n",
      "Epoch: 1550 training cost= 32.550434113 W= [[ 5.13405323 -3.03018808]] b= [ 5.62952566]\n",
      "Epoch: 1550 test cost= 92.128219604 W= [[ 5.13405323 -3.03018808]] b= [ 5.62952566]\n",
      "Epoch: 1600 training cost= 32.233612061 W= [[ 5.11824894 -3.01788402]] b= [ 5.75392771]\n",
      "Epoch: 1600 test cost= 91.477706909 W= [[ 5.11824894 -3.01788402]] b= [ 5.75392771]\n",
      "Epoch: 1650 training cost= 31.920421600 W= [[ 5.10198784 -3.00492477]] b= [ 5.87753773]\n",
      "Epoch: 1650 test cost= 90.810287476 W= [[ 5.10198784 -3.00492477]] b= [ 5.87753773]\n",
      "Epoch: 1700 training cost= 31.610708237 W= [[ 5.08535147 -2.99142051]] b= [ 6.00038719]\n",
      "Epoch: 1700 test cost= 90.129623413 W= [[ 5.08535147 -2.99142051]] b= [ 6.00038719]\n",
      "Epoch: 1750 training cost= 31.304367065 W= [[ 5.0684123  -2.97746587]] b= [ 6.12250185]\n",
      "Epoch: 1750 test cost= 89.439216614 W= [[ 5.0684123  -2.97746587]] b= [ 6.12250185]\n",
      "Epoch: 1800 training cost= 31.001281738 W= [[ 5.05122948 -2.96314335]] b= [ 6.24390984]\n",
      "Epoch: 1800 test cost= 88.741111755 W= [[ 5.05122948 -2.96314335]] b= [ 6.24390984]\n",
      "Epoch: 1850 training cost= 30.701370239 W= [[ 5.03385258 -2.94851804]] b= [ 6.36462975]\n",
      "Epoch: 1850 test cost= 88.037864685 W= [[ 5.03385258 -2.94851804]] b= [ 6.36462975]\n",
      "Epoch: 1900 training cost= 30.404544830 W= [[ 5.01632404 -2.93364716]] b= [ 6.48468208]\n",
      "Epoch: 1900 test cost= 87.331161499 W= [[ 5.01632404 -2.93364716]] b= [ 6.48468208]\n",
      "Epoch: 1950 training cost= 30.110761642 W= [[ 4.99867868 -2.91857934]] b= [ 6.60408115]\n",
      "Epoch: 1950 test cost= 86.622268677 W= [[ 4.99867868 -2.91857934]] b= [ 6.60408115]\n",
      "Epoch: 2000 training cost= 29.819952011 W= [[ 4.98095036 -2.90335774]] b= [ 6.72284126]\n",
      "Epoch: 2000 test cost= 85.912994385 W= [[ 4.98095036 -2.90335774]] b= [ 6.72284126]\n",
      "Epoch: 2050 training cost= 29.532045364 W= [[ 4.96316528 -2.88801861]] b= [ 6.84098101]\n",
      "Epoch: 2050 test cost= 85.204467773 W= [[ 4.96316528 -2.88801861]] b= [ 6.84098101]\n",
      "Epoch: 2100 training cost= 29.247005463 W= [[ 4.94534206 -2.87258816]] b= [ 6.95850515]\n",
      "Epoch: 2100 test cost= 84.497085571 W= [[ 4.94534206 -2.87258816]] b= [ 6.95850515]\n",
      "Epoch: 2150 training cost= 28.964826584 W= [[ 4.92750359 -2.85709381]] b= [ 7.0754137]\n",
      "Epoch: 2150 test cost= 83.792419434 W= [[ 4.92750359 -2.85709381]] b= [ 7.0754137]\n",
      "Epoch: 2200 training cost= 28.685422897 W= [[ 4.90966082 -2.8415544 ]] b= [ 7.19172478]\n",
      "Epoch: 2200 test cost= 83.090263367 W= [[ 4.90966082 -2.8415544 ]] b= [ 7.19172478]\n",
      "Epoch: 2250 training cost= 28.408775330 W= [[ 4.89182949 -2.82599068]] b= [ 7.30745173]\n",
      "Epoch: 2250 test cost= 82.391365051 W= [[ 4.89182949 -2.82599068]] b= [ 7.30745173]\n",
      "Epoch: 2300 training cost= 28.134830475 W= [[ 4.87401772 -2.81041265]] b= [ 7.42259121]\n",
      "Epoch: 2300 test cost= 81.696228027 W= [[ 4.87401772 -2.81041265]] b= [ 7.42259121]\n",
      "Epoch: 2350 training cost= 27.863584518 W= [[ 4.85624027 -2.79483986]] b= [ 7.53715801]\n",
      "Epoch: 2350 test cost= 81.005554199 W= [[ 4.85624027 -2.79483986]] b= [ 7.53715801]\n",
      "Epoch: 2400 training cost= 27.595006943 W= [[ 4.83850479 -2.77928233]] b= [ 7.65114307]\n",
      "Epoch: 2400 test cost= 80.319549561 W= [[ 4.83850479 -2.77928233]] b= [ 7.65114307]\n",
      "Epoch: 2450 training cost= 27.329004288 W= [[ 4.82081699 -2.76375055]] b= [ 7.76457548]\n",
      "Epoch: 2450 test cost= 79.638122559 W= [[ 4.82081699 -2.76375055]] b= [ 7.76457548]\n",
      "Epoch: 2500 training cost= 27.065612793 W= [[ 4.80318213 -2.74824858]] b= [ 7.87744331]\n",
      "Epoch: 2500 test cost= 78.962059021 W= [[ 4.80318213 -2.74824858]] b= [ 7.87744331]\n",
      "Epoch: 2550 training cost= 26.804794312 W= [[ 4.78560257 -2.73278284]] b= [ 7.98975039]\n",
      "Epoch: 2550 test cost= 78.290679932 W= [[ 4.78560257 -2.73278284]] b= [ 7.98975039]\n",
      "Epoch: 2600 training cost= 26.546459198 W= [[ 4.76808548 -2.71736145]] b= [ 8.10152626]\n",
      "Epoch: 2600 test cost= 77.624908447 W= [[ 4.76808548 -2.71736145]] b= [ 8.10152626]\n",
      "Epoch: 2650 training cost= 26.290639877 W= [[ 4.75063324 -2.70198727]] b= [ 8.21274948]\n",
      "Epoch: 2650 test cost= 76.964653015 W= [[ 4.75063324 -2.70198727]] b= [ 8.21274948]\n",
      "Epoch: 2700 training cost= 26.037313461 W= [[ 4.73324633 -2.68666315]] b= [ 8.32342434]\n",
      "Epoch: 2700 test cost= 76.309394836 W= [[ 4.73324633 -2.68666315]] b= [ 8.32342434]\n",
      "Epoch: 2750 training cost= 25.786464691 W= [[ 4.71592808 -2.67139363]] b= [ 8.43355274]\n",
      "Epoch: 2750 test cost= 75.659400940 W= [[ 4.71592808 -2.67139363]] b= [ 8.43355274]\n",
      "Epoch: 2800 training cost= 25.538032532 W= [[ 4.69868469 -2.65618277]] b= [ 8.54314423]\n",
      "Epoch: 2800 test cost= 75.015640259 W= [[ 4.69868469 -2.65618277]] b= [ 8.54314423]\n",
      "Epoch: 2850 training cost= 25.292005539 W= [[ 4.68150949 -2.64102626]] b= [ 8.65220165]\n",
      "Epoch: 2850 test cost= 74.377014160 W= [[ 4.68150949 -2.64102626]] b= [ 8.65220165]\n",
      "Epoch: 2900 training cost= 25.048355103 W= [[ 4.66441059 -2.62593412]] b= [ 8.76073074]\n",
      "Epoch: 2900 test cost= 73.744064331 W= [[ 4.66441059 -2.62593412]] b= [ 8.76073074]\n",
      "Epoch: 2950 training cost= 24.807069778 W= [[ 4.64738464 -2.61090255]] b= [ 8.8687315]\n",
      "Epoch: 2950 test cost= 73.116485596 W= [[ 4.64738464 -2.61090255]] b= [ 8.8687315]\n",
      "Epoch: 3000 training cost= 24.568109512 W= [[ 4.63043785 -2.59593797]] b= [ 8.97620678]\n",
      "Epoch: 3000 test cost= 72.494804382 W= [[ 4.63043785 -2.59593797]] b= [ 8.97620678]\n",
      "Epoch: 3050 training cost= 24.331466675 W= [[ 4.6135664  -2.58103824]] b= [ 9.0831604]\n",
      "Epoch: 3050 test cost= 71.878463745 W= [[ 4.6135664  -2.58103824]] b= [ 9.0831604]\n",
      "Epoch: 3100 training cost= 24.097112656 W= [[ 4.59677267 -2.56620502]] b= [ 9.18959522]\n",
      "Epoch: 3100 test cost= 71.267745972 W= [[ 4.59677267 -2.56620502]] b= [ 9.18959522]\n",
      "Epoch: 3150 training cost= 23.865022659 W= [[ 4.58005619 -2.55143857]] b= [ 9.2955122]\n",
      "Epoch: 3150 test cost= 70.662414551 W= [[ 4.58005619 -2.55143857]] b= [ 9.2955122]\n",
      "Epoch: 3200 training cost= 23.635173798 W= [[ 4.56341791 -2.53673887]] b= [ 9.40091419]\n",
      "Epoch: 3200 test cost= 70.062835693 W= [[ 4.56341791 -2.53673887]] b= [ 9.40091419]\n",
      "Epoch: 3250 training cost= 23.407541275 W= [[ 4.54685783 -2.52210641]] b= [ 9.50580978]\n",
      "Epoch: 3250 test cost= 69.468795776 W= [[ 4.54685783 -2.52210641]] b= [ 9.50580978]\n",
      "Epoch: 3300 training cost= 23.182107925 W= [[ 4.53037214 -2.50754046]] b= [ 9.61020184]\n",
      "Epoch: 3300 test cost= 68.879623413 W= [[ 4.53037214 -2.50754046]] b= [ 9.61020184]\n",
      "Epoch: 3350 training cost= 22.958831787 W= [[ 4.51396704 -2.49304247]] b= [ 9.7140913]\n",
      "Epoch: 3350 test cost= 68.296554565 W= [[ 4.51396704 -2.49304247]] b= [ 9.7140913]\n",
      "Epoch: 3400 training cost= 22.737712860 W= [[ 4.49763775 -2.47861338]] b= [ 9.81747627]\n",
      "Epoch: 3400 test cost= 67.718139648 W= [[ 4.49763775 -2.47861338]] b= [ 9.81747627]\n",
      "Epoch: 3450 training cost= 22.518747330 W= [[ 4.48138666 -2.46425152]] b= [ 9.92035675]\n",
      "Epoch: 3450 test cost= 67.145355225 W= [[ 4.48138666 -2.46425152]] b= [ 9.92035675]\n",
      "Epoch: 3500 training cost= 22.301891327 W= [[ 4.4652133  -2.44995785]] b= [ 10.02274132]\n",
      "Epoch: 3500 test cost= 66.577835083 W= [[ 4.4652133  -2.44995785]] b= [ 10.02274132]\n",
      "Epoch: 3550 training cost= 22.087120056 W= [[ 4.44911671 -2.43573165]] b= [ 10.12462997]\n",
      "Epoch: 3550 test cost= 66.015411377 W= [[ 4.44911671 -2.43573165]] b= [ 10.12462997]\n",
      "Epoch: 3600 training cost= 21.874496460 W= [[ 4.43309784 -2.42157125]] b= [ 10.22599602]\n",
      "Epoch: 3600 test cost= 65.458389282 W= [[ 4.43309784 -2.42157125]] b= [ 10.22599602]\n",
      "Epoch: 3650 training cost= 21.663951874 W= [[ 4.41715717 -2.40747976]] b= [ 10.32685471]\n",
      "Epoch: 3650 test cost= 64.906410217 W= [[ 4.41715717 -2.40747976]] b= [ 10.32685471]\n",
      "Epoch: 3700 training cost= 21.455438614 W= [[ 4.4012928  -2.39345551]] b= [ 10.42722607]\n",
      "Epoch: 3700 test cost= 64.359443665 W= [[ 4.4012928  -2.39345551]] b= [ 10.42722607]\n",
      "Epoch: 3750 training cost= 21.248943329 W= [[ 4.38550615 -2.37950087]] b= [ 10.52711201]\n",
      "Epoch: 3750 test cost= 63.817558289 W= [[ 4.38550615 -2.37950087]] b= [ 10.52711201]\n",
      "Epoch: 3800 training cost= 21.044424057 W= [[ 4.36979485 -2.36561251]] b= [ 10.62652302]\n",
      "Epoch: 3800 test cost= 63.280662537 W= [[ 4.36979485 -2.36561251]] b= [ 10.62652302]\n",
      "Epoch: 3850 training cost= 20.841876984 W= [[ 4.35416031 -2.35179162]] b= [ 10.72545338]\n",
      "Epoch: 3850 test cost= 62.748950958 W= [[ 4.35416031 -2.35179162]] b= [ 10.72545338]\n",
      "Epoch: 3900 training cost= 20.641281128 W= [[ 4.33860064 -2.33803654]] b= [ 10.82390404]\n",
      "Epoch: 3900 test cost= 62.222106934 W= [[ 4.33860064 -2.33803654]] b= [ 10.82390404]\n",
      "Epoch: 3950 training cost= 20.442628860 W= [[ 4.32311487 -2.3243475 ]] b= [ 10.92188072]\n",
      "Epoch: 3950 test cost= 61.699905396 W= [[ 4.32311487 -2.3243475 ]] b= [ 10.92188072]\n",
      "Epoch: 4000 training cost= 20.245889664 W= [[ 4.30770397 -2.31072426]] b= [ 11.01938343]\n",
      "Epoch: 4000 test cost= 61.182548523 W= [[ 4.30770397 -2.31072426]] b= [ 11.01938343]\n",
      "Epoch: 4050 training cost= 20.051044464 W= [[ 4.29236841 -2.2971673 ]] b= [ 11.11641407]\n",
      "Epoch: 4050 test cost= 60.670127869 W= [[ 4.29236841 -2.2971673 ]] b= [ 11.11641407]\n",
      "Epoch: 4100 training cost= 19.858085632 W= [[ 4.27710772 -2.28367805]] b= [ 11.21297646]\n",
      "Epoch: 4100 test cost= 60.162284851 W= [[ 4.27710772 -2.28367805]] b= [ 11.21297646]\n",
      "Epoch: 4150 training cost= 19.666976929 W= [[ 4.26192045 -2.27025247]] b= [ 11.30907726]\n",
      "Epoch: 4150 test cost= 59.659366608 W= [[ 4.26192045 -2.27025247]] b= [ 11.30907726]\n",
      "Epoch: 4200 training cost= 19.477714539 W= [[ 4.24680567 -2.25689125]] b= [ 11.40471458]\n",
      "Epoch: 4200 test cost= 59.160987854 W= [[ 4.24680567 -2.25689125]] b= [ 11.40471458]\n",
      "Epoch: 4250 training cost= 19.290267944 W= [[ 4.23176479 -2.24359584]] b= [ 11.49989128]\n",
      "Epoch: 4250 test cost= 58.667221069 W= [[ 4.23176479 -2.24359584]] b= [ 11.49989128]\n",
      "Epoch: 4300 training cost= 19.104640961 W= [[ 4.21679544 -2.23036289]] b= [ 11.59460449]\n",
      "Epoch: 4300 test cost= 58.177932739 W= [[ 4.21679544 -2.23036289]] b= [ 11.59460449]\n",
      "Epoch: 4350 training cost= 18.920804977 W= [[ 4.2018981  -2.21719313]] b= [ 11.68886089]\n",
      "Epoch: 4350 test cost= 57.693229675 W= [[ 4.2018981  -2.21719313]] b= [ 11.68886089]\n",
      "Epoch: 4400 training cost= 18.738742828 W= [[ 4.18707275 -2.20408773]] b= [ 11.78266048]\n",
      "Epoch: 4400 test cost= 57.212902069 W= [[ 4.18707275 -2.20408773]] b= [ 11.78266048]\n",
      "Epoch: 4450 training cost= 18.558420181 W= [[ 4.17231941 -2.19104743]] b= [ 11.87601566]\n",
      "Epoch: 4450 test cost= 56.736900330 W= [[ 4.17231941 -2.19104743]] b= [ 11.87601566]\n",
      "Epoch: 4500 training cost= 18.379842758 W= [[ 4.15763807 -2.17806959]] b= [ 11.96891785]\n",
      "Epoch: 4500 test cost= 56.265522003 W= [[ 4.15763807 -2.17806959]] b= [ 11.96891785]\n",
      "Epoch: 4550 training cost= 18.202980042 W= [[ 4.14302635 -2.16515279]] b= [ 12.0613699]\n",
      "Epoch: 4550 test cost= 55.798385620 W= [[ 4.14302635 -2.16515279]] b= [ 12.0613699]\n",
      "Epoch: 4600 training cost= 18.027839661 W= [[ 4.12848616 -2.15229917]] b= [ 12.15337467]\n",
      "Epoch: 4600 test cost= 55.335697174 W= [[ 4.12848616 -2.15229917]] b= [ 12.15337467]\n",
      "Epoch: 4650 training cost= 17.854389191 W= [[ 4.1140151  -2.13950682]] b= [ 12.24493027]\n",
      "Epoch: 4650 test cost= 54.877109528 W= [[ 4.1140151  -2.13950682]] b= [ 12.24493027]\n",
      "Epoch: 4700 training cost= 17.682594299 W= [[ 4.09961605 -2.12677884]] b= [ 12.3360548]\n",
      "Epoch: 4700 test cost= 54.422882080 W= [[ 4.09961605 -2.12677884]] b= [ 12.3360548]\n",
      "Epoch: 4750 training cost= 17.512454987 W= [[ 4.08528233 -2.11410809]] b= [ 12.42673588]\n",
      "Epoch: 4750 test cost= 53.972537994 W= [[ 4.08528233 -2.11410809]] b= [ 12.42673588]\n",
      "Epoch: 4800 training cost= 17.343971252 W= [[ 4.0710206  -2.10150123]] b= [ 12.51698112]\n",
      "Epoch: 4800 test cost= 53.526599884 W= [[ 4.0710206  -2.10150123]] b= [ 12.51698112]\n",
      "Epoch: 4850 training cost= 17.177110672 W= [[ 4.05682755 -2.08895421]] b= [ 12.60678387]\n",
      "Epoch: 4850 test cost= 53.084735870 W= [[ 4.05682755 -2.08895421]] b= [ 12.60678387]\n",
      "Epoch: 4900 training cost= 17.011846542 W= [[ 4.04270172 -2.0764668 ]] b= [ 12.69615936]\n",
      "Epoch: 4900 test cost= 52.646934509 W= [[ 4.04270172 -2.0764668 ]] b= [ 12.69615936]\n",
      "Epoch: 4950 training cost= 16.848215103 W= [[ 4.02864456 -2.06404018]] b= [ 12.78508854]\n",
      "Epoch: 4950 test cost= 52.212997437 W= [[ 4.02864456 -2.06404018]] b= [ 12.78508854]\n",
      "Epoch: 5000 training cost= 16.686208725 W= [[ 4.01465797 -2.05167365]] b= [ 12.87355804]\n",
      "Epoch: 5000 test cost= 51.783378601 W= [[ 4.01465797 -2.05167365]] b= [ 12.87355804]\n",
      "Epoch: 5050 training cost= 16.525772095 W= [[ 4.00074005 -2.03936863]] b= [ 12.96159649]\n",
      "Epoch: 5050 test cost= 51.357582092 W= [[ 4.00074005 -2.03936863]] b= [ 12.96159649]\n",
      "Epoch: 5100 training cost= 16.366886139 W= [[ 3.98688865 -2.02712274]] b= [ 13.04920864]\n",
      "Epoch: 5100 test cost= 50.935615540 W= [[ 3.98688865 -2.02712274]] b= [ 13.04920864]\n",
      "Epoch: 5150 training cost= 16.209522247 W= [[ 3.97310495 -2.01493716]] b= [ 13.13640594]\n",
      "Epoch: 5150 test cost= 50.517669678 W= [[ 3.97310495 -2.01493716]] b= [ 13.13640594]\n",
      "Epoch: 5200 training cost= 16.053668976 W= [[ 3.95938873 -2.00281143]] b= [ 13.22318268]\n",
      "Epoch: 5200 test cost= 50.103584290 W= [[ 3.95938873 -2.00281143]] b= [ 13.22318268]\n",
      "Epoch: 5250 training cost= 15.899324417 W= [[ 3.94573832 -1.9907434 ]] b= [ 13.30953693]\n",
      "Epoch: 5250 test cost= 49.693336487 W= [[ 3.94573832 -1.9907434 ]] b= [ 13.30953693]\n",
      "Epoch: 5300 training cost= 15.746474266 W= [[ 3.93215442 -1.97873497]] b= [ 13.39547253]\n",
      "Epoch: 5300 test cost= 49.286743164 W= [[ 3.93215442 -1.97873497]] b= [ 13.39547253]\n",
      "Epoch: 5350 training cost= 15.595091820 W= [[ 3.91863632 -1.96678436]] b= [ 13.48099804]\n",
      "Epoch: 5350 test cost= 48.884006500 W= [[ 3.91863632 -1.96678436]] b= [ 13.48099804]\n",
      "Epoch: 5400 training cost= 15.445158958 W= [[ 3.90518403 -1.9548924 ]] b= [ 13.56611347]\n",
      "Epoch: 5400 test cost= 48.484992981 W= [[ 3.90518403 -1.9548924 ]] b= [ 13.56611347]\n",
      "Epoch: 5450 training cost= 15.296686172 W= [[ 3.89179635 -1.94305742]] b= [ 13.65081501]\n",
      "Epoch: 5450 test cost= 48.089622498 W= [[ 3.89179635 -1.94305742]] b= [ 13.65081501]\n",
      "Epoch: 5500 training cost= 15.149642944 W= [[ 3.87847352 -1.93127966]] b= [ 13.73510456]\n",
      "Epoch: 5500 test cost= 47.697822571 W= [[ 3.87847352 -1.93127966]] b= [ 13.73510456]\n",
      "Epoch: 5550 training cost= 15.004007339 W= [[ 3.86521482 -1.91955912]] b= [ 13.81899452]\n",
      "Epoch: 5550 test cost= 47.309654236 W= [[ 3.86521482 -1.91955912]] b= [ 13.81899452]\n",
      "Epoch: 5600 training cost= 14.859772682 W= [[ 3.85202003 -1.90789533]] b= [ 13.90248013]\n",
      "Epoch: 5600 test cost= 46.924995422 W= [[ 3.85202003 -1.90789533]] b= [ 13.90248013]\n",
      "Epoch: 5650 training cost= 14.716944695 W= [[ 3.8388896  -1.89628768]] b= [ 13.98555756]\n",
      "Epoch: 5650 test cost= 46.543998718 W= [[ 3.8388896  -1.89628768]] b= [ 13.98555756]\n",
      "Epoch: 5700 training cost= 14.575488091 W= [[ 3.82582283 -1.88473606]] b= [ 14.06822968]\n",
      "Epoch: 5700 test cost= 46.166496277 W= [[ 3.82582283 -1.88473606]] b= [ 14.06822968]\n",
      "Epoch: 5750 training cost= 14.435387611 W= [[ 3.81281805 -1.87324035]] b= [ 14.15051651]\n",
      "Epoch: 5750 test cost= 45.792316437 W= [[ 3.81281805 -1.87324035]] b= [ 14.15051651]\n",
      "Epoch: 5800 training cost= 14.296639442 W= [[ 3.79987645 -1.86180007]] b= [ 14.23239994]\n",
      "Epoch: 5800 test cost= 45.421623230 W= [[ 3.79987645 -1.86180007]] b= [ 14.23239994]\n",
      "Epoch: 5850 training cost= 14.159242630 W= [[ 3.78699756 -1.85041451]] b= [ 14.31388283]\n",
      "Epoch: 5850 test cost= 45.054412842 W= [[ 3.78699756 -1.85041451]] b= [ 14.31388283]\n",
      "Epoch: 5900 training cost= 14.023155212 W= [[ 3.77418089 -1.83908486]] b= [ 14.39497757]\n",
      "Epoch: 5900 test cost= 44.690479279 W= [[ 3.77418089 -1.83908486]] b= [ 14.39497757]\n",
      "Epoch: 5950 training cost= 13.888377190 W= [[ 3.76142597 -1.82780969]] b= [ 14.47568512]\n",
      "Epoch: 5950 test cost= 44.329944611 W= [[ 3.76142597 -1.82780969]] b= [ 14.47568512]\n",
      "Epoch: 6000 training cost= 13.754912376 W= [[ 3.74873257 -1.81658912]] b= [ 14.55599594]\n",
      "Epoch: 6000 test cost= 43.972553253 W= [[ 3.74873257 -1.81658912]] b= [ 14.55599594]\n",
      "Epoch: 6050 training cost= 13.622738838 W= [[ 3.73610067 -1.80542219]] b= [ 14.63591766]\n",
      "Epoch: 6050 test cost= 43.618595123 W= [[ 3.73610067 -1.80542219]] b= [ 14.63591766]\n",
      "Epoch: 6100 training cost= 13.491815567 W= [[ 3.72352958 -1.79430997]] b= [ 14.71546555]\n",
      "Epoch: 6100 test cost= 43.267829895 W= [[ 3.72352958 -1.79430997]] b= [ 14.71546555]\n",
      "Epoch: 6150 training cost= 13.362170219 W= [[ 3.71101928 -1.78325129]] b= [ 14.79462242]\n",
      "Epoch: 6150 test cost= 42.920276642 W= [[ 3.71101928 -1.78325129]] b= [ 14.79462242]\n",
      "Epoch: 6200 training cost= 13.233781815 W= [[ 3.69856977 -1.77224553]] b= [ 14.87339211]\n",
      "Epoch: 6200 test cost= 42.575935364 W= [[ 3.69856977 -1.77224553]] b= [ 14.87339211]\n",
      "Epoch: 6250 training cost= 13.106614113 W= [[ 3.68617988 -1.76129413]] b= [ 14.95179176]\n",
      "Epoch: 6250 test cost= 42.234603882 W= [[ 3.68617988 -1.76129413]] b= [ 14.95179176]\n",
      "Epoch: 6300 training cost= 12.980684280 W= [[ 3.6738503  -1.75039482]] b= [ 15.02980614]\n",
      "Epoch: 6300 test cost= 41.896568298 W= [[ 3.6738503  -1.75039482]] b= [ 15.02980614]\n",
      "Epoch: 6350 training cost= 12.855965614 W= [[ 3.66157961 -1.73954725]] b= [ 15.10744476]\n",
      "Epoch: 6350 test cost= 41.561595917 W= [[ 3.66157961 -1.73954725]] b= [ 15.10744476]\n",
      "Epoch: 6400 training cost= 12.732446671 W= [[ 3.64936852 -1.72875309]] b= [ 15.18471336]\n",
      "Epoch: 6400 test cost= 41.229675293 W= [[ 3.64936852 -1.72875309]] b= [ 15.18471336]\n",
      "Epoch: 6450 training cost= 12.610122681 W= [[ 3.63721609 -1.71801078]] b= [ 15.26160812]\n",
      "Epoch: 6450 test cost= 40.900756836 W= [[ 3.63721609 -1.71801078]] b= [ 15.26160812]\n",
      "Epoch: 6500 training cost= 12.489037514 W= [[ 3.62512398 -1.7073195 ]] b= [ 15.33808804]\n",
      "Epoch: 6500 test cost= 40.574859619 W= [[ 3.62512398 -1.7073195 ]] b= [ 15.33808804]\n",
      "Epoch: 6550 training cost= 12.369131088 W= [[ 3.61309099 -1.69668078]] b= [ 15.41419601]\n",
      "Epoch: 6550 test cost= 40.251945496 W= [[ 3.61309099 -1.69668078]] b= [ 15.41419601]\n",
      "Epoch: 6600 training cost= 12.250371933 W= [[ 3.60111666 -1.68609393]] b= [ 15.48993874]\n",
      "Epoch: 6600 test cost= 39.932010651 W= [[ 3.60111666 -1.68609393]] b= [ 15.48993874]\n",
      "Epoch: 6650 training cost= 12.132772446 W= [[ 3.58920097 -1.67555916]] b= [ 15.56530952]\n",
      "Epoch: 6650 test cost= 39.615009308 W= [[ 3.58920097 -1.67555916]] b= [ 15.56530952]\n",
      "Epoch: 6700 training cost= 12.016301155 W= [[ 3.57734251 -1.6650753 ]] b= [ 15.64032459]\n",
      "Epoch: 6700 test cost= 39.300952911 W= [[ 3.57734251 -1.6650753 ]] b= [ 15.64032459]\n",
      "Epoch: 6750 training cost= 11.900944710 W= [[ 3.5655427  -1.65464354]] b= [ 15.71497631]\n",
      "Epoch: 6750 test cost= 38.989814758 W= [[ 3.5655427  -1.65464354]] b= [ 15.71497631]\n",
      "Epoch: 6800 training cost= 11.786712646 W= [[ 3.55379987 -1.64426208]] b= [ 15.78925991]\n",
      "Epoch: 6800 test cost= 38.681442261 W= [[ 3.55379987 -1.64426208]] b= [ 15.78925991]\n",
      "Epoch: 6850 training cost= 11.673561096 W= [[ 3.54211402 -1.63393128]] b= [ 15.86319828]\n",
      "Epoch: 6850 test cost= 38.375984192 W= [[ 3.54211402 -1.63393128]] b= [ 15.86319828]\n",
      "Epoch: 6900 training cost= 11.561519623 W= [[ 3.53048396 -1.62364984]] b= [ 15.9367733]\n",
      "Epoch: 6900 test cost= 38.073219299 W= [[ 3.53048396 -1.62364984]] b= [ 15.9367733]\n",
      "Epoch: 6950 training cost= 11.450550079 W= [[ 3.51891041 -1.6134187 ]] b= [ 16.0099926]\n",
      "Epoch: 6950 test cost= 37.773223877 W= [[ 3.51891041 -1.6134187 ]] b= [ 16.0099926]\n",
      "Epoch: 7000 training cost= 11.340639114 W= [[ 3.50739288 -1.60323763]] b= [ 16.08286858]\n",
      "Epoch: 7000 test cost= 37.475933075 W= [[ 3.50739288 -1.60323763]] b= [ 16.08286858]\n",
      "Epoch: 7050 training cost= 11.231835365 W= [[ 3.49593258 -1.59310532]] b= [ 16.15536118]\n",
      "Epoch: 7050 test cost= 37.181465149 W= [[ 3.49593258 -1.59310532]] b= [ 16.15536118]\n",
      "Epoch: 7100 training cost= 11.124033928 W= [[ 3.48452711 -1.58302307]] b= [ 16.22753525]\n",
      "Epoch: 7100 test cost= 36.889709473 W= [[ 3.48452711 -1.58302307]] b= [ 16.22753525]\n",
      "Epoch: 7150 training cost= 11.017264366 W= [[ 3.47317648 -1.57299018]] b= [ 16.29936409]\n",
      "Epoch: 7150 test cost= 36.600513458 W= [[ 3.47317648 -1.57299018]] b= [ 16.29936409]\n",
      "Epoch: 7200 training cost= 10.911560059 W= [[ 3.46188188 -1.56300545]] b= [ 16.37082291]\n",
      "Epoch: 7200 test cost= 36.314025879 W= [[ 3.46188188 -1.56300545]] b= [ 16.37082291]\n",
      "Epoch: 7250 training cost= 10.806862831 W= [[ 3.45064068 -1.5530684 ]] b= [ 16.44194794]\n",
      "Epoch: 7250 test cost= 36.030078888 W= [[ 3.45064068 -1.5530684 ]] b= [ 16.44194794]\n",
      "Epoch: 7300 training cost= 10.703143120 W= [[ 3.43945289 -1.54317904]] b= [ 16.51274872]\n",
      "Epoch: 7300 test cost= 35.748783112 W= [[ 3.43945289 -1.54317904]] b= [ 16.51274872]\n",
      "Epoch: 7350 training cost= 10.600467682 W= [[ 3.42831969 -1.53333724]] b= [ 16.58317757]\n",
      "Epoch: 7350 test cost= 35.469886780 W= [[ 3.42831969 -1.53333724]] b= [ 16.58317757]\n",
      "Epoch: 7400 training cost= 10.498785019 W= [[ 3.41724086 -1.52354205]] b= [ 16.653265]\n",
      "Epoch: 7400 test cost= 35.193763733 W= [[ 3.41724086 -1.52354205]] b= [ 16.653265]\n",
      "Epoch: 7450 training cost= 10.398021698 W= [[ 3.40621352 -1.51379514]] b= [ 16.72305489]\n",
      "Epoch: 7450 test cost= 34.919998169 W= [[ 3.40621352 -1.51379514]] b= [ 16.72305489]\n",
      "Epoch: 7500 training cost= 10.298274994 W= [[ 3.39523864 -1.50409293]] b= [ 16.79248238]\n",
      "Epoch: 7500 test cost= 34.648654938 W= [[ 3.39523864 -1.50409293]] b= [ 16.79248238]\n",
      "Epoch: 7550 training cost= 10.199524879 W= [[ 3.38431978 -1.49443936]] b= [ 16.86154175]\n",
      "Epoch: 7550 test cost= 34.379821777 W= [[ 3.38431978 -1.49443936]] b= [ 16.86154175]\n",
      "Epoch: 7600 training cost= 10.101644516 W= [[ 3.37345195 -1.48483372]] b= [ 16.93033409]\n",
      "Epoch: 7600 test cost= 34.113555908 W= [[ 3.37345195 -1.48483372]] b= [ 16.93033409]\n",
      "Epoch: 7650 training cost= 10.004743576 W= [[ 3.36263633 -1.47527337]] b= [ 16.99876785]\n",
      "Epoch: 7650 test cost= 33.849525452 W= [[ 3.36263633 -1.47527337]] b= [ 16.99876785]\n",
      "Epoch: 7700 training cost= 9.908819199 W= [[ 3.35187507 -1.46575916]] b= [ 17.06683922]\n",
      "Epoch: 7700 test cost= 33.588066101 W= [[ 3.35187507 -1.46575916]] b= [ 17.06683922]\n",
      "Epoch: 7750 training cost= 9.813766479 W= [[ 3.34116483 -1.45629168]] b= [ 17.13461685]\n",
      "Epoch: 7750 test cost= 33.328964233 W= [[ 3.34116483 -1.45629168]] b= [ 17.13461685]\n",
      "Epoch: 7800 training cost= 9.719624519 W= [[ 3.33050513 -1.44686973]] b= [ 17.20207787]\n",
      "Epoch: 7800 test cost= 33.072158813 W= [[ 3.33050513 -1.44686973]] b= [ 17.20207787]\n",
      "Epoch: 7850 training cost= 9.626433372 W= [[ 3.31989813 -1.43749249]] b= [ 17.26917839]\n",
      "Epoch: 7850 test cost= 32.817703247 W= [[ 3.31989813 -1.43749249]] b= [ 17.26917839]\n",
      "Epoch: 7900 training cost= 9.534140587 W= [[ 3.30934262 -1.42816079]] b= [ 17.33595276]\n",
      "Epoch: 7900 test cost= 32.565509796 W= [[ 3.30934262 -1.42816079]] b= [ 17.33595276]\n",
      "Epoch: 7950 training cost= 9.442676544 W= [[ 3.29883599 -1.41887462]] b= [ 17.40245628]\n",
      "Epoch: 7950 test cost= 32.315689087 W= [[ 3.29883599 -1.41887462]] b= [ 17.40245628]\n",
      "Epoch: 8000 training cost= 9.352141380 W= [[ 3.28838181 -1.40963304]] b= [ 17.46859932]\n",
      "Epoch: 8000 test cost= 32.068038940 W= [[ 3.28838181 -1.40963304]] b= [ 17.46859932]\n",
      "Epoch: 8050 training cost= 9.262500763 W= [[ 3.2779789  -1.40043604]] b= [ 17.53440857]\n",
      "Epoch: 8050 test cost= 31.822729111 W= [[ 3.2779789  -1.40043604]] b= [ 17.53440857]\n",
      "Epoch: 8100 training cost= 9.173673630 W= [[ 3.26762462 -1.39128399]] b= [ 17.59993935]\n",
      "Epoch: 8100 test cost= 31.579578400 W= [[ 3.26762462 -1.39128399]] b= [ 17.59993935]\n",
      "Epoch: 8150 training cost= 9.085712433 W= [[ 3.25732136 -1.38217688]] b= [ 17.66514587]\n",
      "Epoch: 8150 test cost= 31.338705063 W= [[ 3.25732136 -1.38217688]] b= [ 17.66514587]\n",
      "Epoch: 8200 training cost= 8.998626709 W= [[ 3.2470665  -1.37311172]] b= [ 17.73001862]\n",
      "Epoch: 8200 test cost= 31.099822998 W= [[ 3.2470665  -1.37311172]] b= [ 17.73001862]\n",
      "Epoch: 8250 training cost= 8.912393570 W= [[ 3.23686314 -1.36409044]] b= [ 17.7945652]\n",
      "Epoch: 8250 test cost= 30.863328934 W= [[ 3.23686314 -1.36409044]] b= [ 17.7945652]\n",
      "Epoch: 8300 training cost= 8.826931953 W= [[ 3.22670603 -1.35511339]] b= [ 17.85885048]\n",
      "Epoch: 8300 test cost= 30.628807068 W= [[ 3.22670603 -1.35511339]] b= [ 17.85885048]\n",
      "Epoch: 8350 training cost= 8.742316246 W= [[ 3.21659946 -1.34617925]] b= [ 17.92280388]\n",
      "Epoch: 8350 test cost= 30.396545410 W= [[ 3.21659946 -1.34617925]] b= [ 17.92280388]\n",
      "Epoch: 8400 training cost= 8.658574104 W= [[ 3.20654273 -1.33728814]] b= [ 17.98641205]\n",
      "Epoch: 8400 test cost= 30.166255951 W= [[ 3.20654273 -1.33728814]] b= [ 17.98641205]\n",
      "Epoch: 8450 training cost= 8.575566292 W= [[ 3.19653296 -1.32844055]] b= [ 18.04976463]\n",
      "Epoch: 8450 test cost= 29.938148499 W= [[ 3.19653296 -1.32844055]] b= [ 18.04976463]\n",
      "Epoch: 8500 training cost= 8.493365288 W= [[ 3.18657184 -1.31963539]] b= [ 18.11280441]\n",
      "Epoch: 8500 test cost= 29.712120056 W= [[ 3.18657184 -1.31963539]] b= [ 18.11280441]\n",
      "Epoch: 8550 training cost= 8.411981583 W= [[ 3.17665863 -1.31087208]] b= [ 18.17552376]\n",
      "Epoch: 8550 test cost= 29.488052368 W= [[ 3.17665863 -1.31087208]] b= [ 18.17552376]\n",
      "Epoch: 8600 training cost= 8.331417084 W= [[ 3.16679454 -1.30215085]] b= [ 18.23791122]\n",
      "Epoch: 8600 test cost= 29.266021729 W= [[ 3.16679454 -1.30215085]] b= [ 18.23791122]\n",
      "Epoch: 8650 training cost= 8.251552582 W= [[ 3.15697551 -1.29347289]] b= [ 18.30006409]\n",
      "Epoch: 8650 test cost= 29.045976639 W= [[ 3.15697551 -1.29347289]] b= [ 18.30006409]\n",
      "Epoch: 8700 training cost= 8.172475815 W= [[ 3.14720511 -1.28483641]] b= [ 18.36189461]\n",
      "Epoch: 8700 test cost= 28.827960968 W= [[ 3.14720511 -1.28483641]] b= [ 18.36189461]\n",
      "Epoch: 8750 training cost= 8.094207764 W= [[ 3.13748217 -1.27624047]] b= [ 18.42339516]\n",
      "Epoch: 8750 test cost= 28.611860275 W= [[ 3.13748217 -1.27624047]] b= [ 18.42339516]\n",
      "Epoch: 8800 training cost= 8.016683578 W= [[ 3.12780666 -1.26768661]] b= [ 18.4846077]\n",
      "Epoch: 8800 test cost= 28.397815704 W= [[ 3.12780666 -1.26768661]] b= [ 18.4846077]\n",
      "Epoch: 8850 training cost= 7.939871311 W= [[ 3.11817694 -1.25917554]] b= [ 18.54555702]\n",
      "Epoch: 8850 test cost= 28.185615540 W= [[ 3.11817694 -1.25917554]] b= [ 18.54555702]\n",
      "Epoch: 8900 training cost= 7.863807201 W= [[ 3.1085937  -1.25070441]] b= [ 18.60620117]\n",
      "Epoch: 8900 test cost= 27.975389481 W= [[ 3.1085937  -1.25070441]] b= [ 18.60620117]\n",
      "Epoch: 8950 training cost= 7.788517952 W= [[ 3.09905815 -1.24227357]] b= [ 18.66651535]\n",
      "Epoch: 8950 test cost= 27.767049789 W= [[ 3.09905815 -1.24227357]] b= [ 18.66651535]\n",
      "Epoch: 9000 training cost= 7.713923454 W= [[ 3.08956695 -1.23388422]] b= [ 18.72657585]\n",
      "Epoch: 9000 test cost= 27.560543060 W= [[ 3.08956695 -1.23388422]] b= [ 18.72657585]\n",
      "Epoch: 9050 training cost= 7.640030861 W= [[ 3.08012152 -1.22553575]] b= [ 18.78635406]\n",
      "Epoch: 9050 test cost= 27.355892181 W= [[ 3.08012152 -1.22553575]] b= [ 18.78635406]\n",
      "Epoch: 9100 training cost= 7.566859722 W= [[ 3.0707221  -1.21722662]] b= [ 18.84583092]\n",
      "Epoch: 9100 test cost= 27.153160095 W= [[ 3.0707221  -1.21722662]] b= [ 18.84583092]\n",
      "Epoch: 9150 training cost= 7.494443893 W= [[ 3.0613687  -1.20895684]] b= [ 18.90498734]\n",
      "Epoch: 9150 test cost= 26.952178955 W= [[ 3.0613687  -1.20895684]] b= [ 18.90498734]\n",
      "Epoch: 9200 training cost= 7.422659874 W= [[ 3.05205941 -1.20072854]] b= [ 18.96390533]\n",
      "Epoch: 9200 test cost= 26.753049850 W= [[ 3.05205941 -1.20072854]] b= [ 18.96390533]\n",
      "Epoch: 9250 training cost= 7.351581573 W= [[ 3.04279399 -1.19253838]] b= [ 19.02253532]\n",
      "Epoch: 9250 test cost= 26.555681229 W= [[ 3.04279399 -1.19253838]] b= [ 19.02253532]\n",
      "Epoch: 9300 training cost= 7.281203747 W= [[ 3.0335741 -1.1843878]] b= [ 19.08086586]\n",
      "Epoch: 9300 test cost= 26.360059738 W= [[ 3.0335741 -1.1843878]] b= [ 19.08086586]\n",
      "Epoch: 9350 training cost= 7.211606979 W= [[ 3.02440214 -1.17627621]] b= [ 19.13884163]\n",
      "Epoch: 9350 test cost= 26.166240692 W= [[ 3.02440214 -1.17627621]] b= [ 19.13884163]\n",
      "Epoch: 9400 training cost= 7.142664909 W= [[ 3.01527667 -1.16820598]] b= [ 19.19653702]\n",
      "Epoch: 9400 test cost= 25.974319458 W= [[ 3.01527667 -1.16820598]] b= [ 19.19653702]\n",
      "Epoch: 9450 training cost= 7.074380398 W= [[ 3.00619531 -1.16017616]] b= [ 19.25396156]\n",
      "Epoch: 9450 test cost= 25.783975601 W= [[ 3.00619531 -1.16017616]] b= [ 19.25396156]\n",
      "Epoch: 9500 training cost= 7.006804466 W= [[ 2.99716067 -1.15218556]] b= [ 19.31106949]\n",
      "Epoch: 9500 test cost= 25.595481873 W= [[ 2.99716067 -1.15218556]] b= [ 19.31106949]\n",
      "Epoch: 9550 training cost= 6.939888477 W= [[ 2.98817134 -1.14423513]] b= [ 19.36789131]\n",
      "Epoch: 9550 test cost= 25.408710480 W= [[ 2.98817134 -1.14423513]] b= [ 19.36789131]\n",
      "Epoch: 9600 training cost= 6.873571396 W= [[ 2.97922421 -1.13632512]] b= [ 19.42448425]\n",
      "Epoch: 9600 test cost= 25.223569870 W= [[ 2.97922421 -1.13632512]] b= [ 19.42448425]\n",
      "Epoch: 9650 training cost= 6.807884216 W= [[ 2.97032142 -1.12845445]] b= [ 19.48080826]\n",
      "Epoch: 9650 test cost= 25.040176392 W= [[ 2.97032142 -1.12845445]] b= [ 19.48080826]\n",
      "Epoch: 9700 training cost= 6.742861748 W= [[ 2.9614625  -1.12062156]] b= [ 19.5368309]\n",
      "Epoch: 9700 test cost= 24.858436584 W= [[ 2.9614625  -1.12062156]] b= [ 19.5368309]\n",
      "Epoch: 9750 training cost= 6.678499699 W= [[ 2.95264816 -1.1128267 ]] b= [ 19.59255791]\n",
      "Epoch: 9750 test cost= 24.678333282 W= [[ 2.95264816 -1.1128267 ]] b= [ 19.59255791]\n",
      "Epoch: 9800 training cost= 6.614680767 W= [[ 2.94387436 -1.10507131]] b= [ 19.64807892]\n",
      "Epoch: 9800 test cost= 24.499814987 W= [[ 2.94387436 -1.10507131]] b= [ 19.64807892]\n",
      "Epoch: 9850 training cost= 6.551498890 W= [[ 2.93514276 -1.09735203]] b= [ 19.70331955]\n",
      "Epoch: 9850 test cost= 24.322940826 W= [[ 2.93514276 -1.09735203]] b= [ 19.70331955]\n",
      "Epoch: 9900 training cost= 6.488941193 W= [[ 2.92645502 -1.08967042]] b= [ 19.75827217]\n",
      "Epoch: 9900 test cost= 24.147705078 W= [[ 2.92645502 -1.08967042]] b= [ 19.75827217]\n",
      "Epoch: 9950 training cost= 6.427017212 W= [[ 2.91780996 -1.08202696]] b= [ 19.8129425]\n",
      "Epoch: 9950 test cost= 23.973934174 W= [[ 2.91780996 -1.08202696]] b= [ 19.8129425]\n",
      "Epoch: 10000 training cost= 6.365642548 W= [[ 2.90920639 -1.07442141]] b= [ 19.86738014]\n",
      "Epoch: 10000 test cost= 23.801780701 W= [[ 2.90920639 -1.07442141]] b= [ 19.86738014]\n",
      "Epoch: 10050 training cost= 6.304841042 W= [[ 2.90064335 -1.06685281]] b= [ 19.92157555]\n",
      "Epoch: 10050 test cost= 23.631195068 W= [[ 2.90064335 -1.06685281]] b= [ 19.92157555]\n",
      "Epoch: 10100 training cost= 6.244659424 W= [[ 2.89212275 -1.05932021]] b= [ 19.97548294]\n",
      "Epoch: 10100 test cost= 23.462142944 W= [[ 2.89212275 -1.05932021]] b= [ 19.97548294]\n",
      "Epoch: 10150 training cost= 6.185090542 W= [[ 2.8836441  -1.05182338]] b= [ 20.0290966]\n",
      "Epoch: 10150 test cost= 23.294548035 W= [[ 2.8836441  -1.05182338]] b= [ 20.0290966]\n",
      "Epoch: 10200 training cost= 6.126059055 W= [[ 2.87520647 -1.04436398]] b= [ 20.0824852]\n",
      "Epoch: 10200 test cost= 23.128541946 W= [[ 2.87520647 -1.04436398]] b= [ 20.0824852]\n",
      "Epoch: 10250 training cost= 6.067565918 W= [[ 2.8668077  -1.03694117]] b= [ 20.13564491]\n",
      "Epoch: 10250 test cost= 22.963949203 W= [[ 2.8668077  -1.03694117]] b= [ 20.13564491]\n",
      "Epoch: 10300 training cost= 6.009669781 W= [[ 2.85844994 -1.02955282]] b= [ 20.18851852]\n",
      "Epoch: 10300 test cost= 22.800788879 W= [[ 2.85844994 -1.02955282]] b= [ 20.18851852]\n",
      "Epoch: 10350 training cost= 5.952344418 W= [[ 2.85013413 -1.0222007 ]] b= [ 20.24112701]\n",
      "Epoch: 10350 test cost= 22.639234543 W= [[ 2.85013413 -1.0222007 ]] b= [ 20.24112701]\n",
      "Epoch: 10400 training cost= 5.895585537 W= [[ 2.84185982 -1.01488495]] b= [ 20.29346466]\n",
      "Epoch: 10400 test cost= 22.479072571 W= [[ 2.84185982 -1.01488495]] b= [ 20.29346466]\n",
      "Epoch: 10450 training cost= 5.839318752 W= [[ 2.83362174 -1.00760436]] b= [ 20.34560585]\n",
      "Epoch: 10450 test cost= 22.320251465 W= [[ 2.83362174 -1.00760436]] b= [ 20.34560585]\n",
      "Epoch: 10500 training cost= 5.783602715 W= [[ 2.82542491 -1.00035894]] b= [ 20.39748001]\n",
      "Epoch: 10500 test cost= 22.162925720 W= [[ 2.82542491 -1.00035894]] b= [ 20.39748001]\n",
      "Epoch: 10550 training cost= 5.728451252 W= [[ 2.81726718 -0.9931469 ]] b= [ 20.44908524]\n",
      "Epoch: 10550 test cost= 22.006973267 W= [[ 2.81726718 -0.9931469 ]] b= [ 20.44908524]\n",
      "Epoch: 10600 training cost= 5.673854351 W= [[ 2.80915046 -0.98597032]] b= [ 20.50041771]\n",
      "Epoch: 10600 test cost= 21.852413177 W= [[ 2.80915046 -0.98597032]] b= [ 20.50041771]\n",
      "Epoch: 10650 training cost= 5.619737625 W= [[ 2.80107236 -0.9788298 ]] b= [ 20.55154037]\n",
      "Epoch: 10650 test cost= 21.699279785 W= [[ 2.80107236 -0.9788298 ]] b= [ 20.55154037]\n",
      "Epoch: 10700 training cost= 5.566131115 W= [[ 2.79303169 -0.97172278]] b= [ 20.60243797]\n",
      "Epoch: 10700 test cost= 21.547538757 W= [[ 2.79303169 -0.97172278]] b= [ 20.60243797]\n",
      "Epoch: 10750 training cost= 5.513058662 W= [[ 2.78503036 -0.96464998]] b= [ 20.65306282]\n",
      "Epoch: 10750 test cost= 21.397052765 W= [[ 2.78503036 -0.96464998]] b= [ 20.65306282]\n",
      "Epoch: 10800 training cost= 5.460533619 W= [[ 2.77706909 -0.95761144]] b= [ 20.70341873]\n",
      "Epoch: 10800 test cost= 21.247920990 W= [[ 2.77706909 -0.95761144]] b= [ 20.70341873]\n",
      "Epoch: 10850 training cost= 5.408509254 W= [[ 2.76914716 -0.950607  ]] b= [ 20.75352669]\n",
      "Epoch: 10850 test cost= 21.100183487 W= [[ 2.76914716 -0.950607  ]] b= [ 20.75352669]\n",
      "Epoch: 10900 training cost= 5.356918335 W= [[ 2.76125956 -0.94363672]] b= [ 20.80346107]\n",
      "Epoch: 10900 test cost= 20.953680038 W= [[ 2.76125956 -0.94363672]] b= [ 20.80346107]\n",
      "Epoch: 10950 training cost= 5.305863380 W= [[ 2.75341177 -0.93670011]] b= [ 20.85312843]\n",
      "Epoch: 10950 test cost= 20.808526993 W= [[ 2.75341177 -0.93670011]] b= [ 20.85312843]\n",
      "Epoch: 11000 training cost= 5.255315781 W= [[ 2.74560261 -0.9297961 ]] b= [ 20.90253067]\n",
      "Epoch: 11000 test cost= 20.664697647 W= [[ 2.74560261 -0.9297961 ]] b= [ 20.90253067]\n",
      "Epoch: 11050 training cost= 5.205276966 W= [[ 2.73783231 -0.92292595]] b= [ 20.9516716]\n",
      "Epoch: 11050 test cost= 20.522071838 W= [[ 2.73783231 -0.92292595]] b= [ 20.9516716]\n",
      "Epoch: 11100 training cost= 5.155681133 W= [[ 2.73009849 -0.91609001]] b= [ 21.0006218]\n",
      "Epoch: 11100 test cost= 20.380830765 W= [[ 2.73009849 -0.91609001]] b= [ 21.0006218]\n",
      "Epoch: 11150 training cost= 5.106538296 W= [[ 2.72240019 -0.90928674]] b= [ 21.04935455]\n",
      "Epoch: 11150 test cost= 20.240726471 W= [[ 2.72240019 -0.90928674]] b= [ 21.04935455]\n",
      "Epoch: 11200 training cost= 5.057896137 W= [[ 2.71473932 -0.9025147 ]] b= [ 21.0978241]\n",
      "Epoch: 11200 test cost= 20.101863861 W= [[ 2.71473932 -0.9025147 ]] b= [ 21.0978241]\n",
      "Epoch: 11250 training cost= 5.009749889 W= [[ 2.70711708 -0.89577532]] b= [ 21.14603996]\n",
      "Epoch: 11250 test cost= 19.964326859 W= [[ 2.70711708 -0.89577532]] b= [ 21.14603996]\n",
      "Epoch: 11300 training cost= 4.962081432 W= [[ 2.69953251 -0.88906896]] b= [ 21.19400406]\n",
      "Epoch: 11300 test cost= 19.827991486 W= [[ 2.69953251 -0.88906896]] b= [ 21.19400406]\n",
      "Epoch: 11350 training cost= 4.914803505 W= [[ 2.69198155 -0.88239604]] b= [ 21.24180603]\n",
      "Epoch: 11350 test cost= 19.692787170 W= [[ 2.69198155 -0.88239604]] b= [ 21.24180603]\n",
      "Epoch: 11400 training cost= 4.867990017 W= [[ 2.68446827 -0.87575567]] b= [ 21.28936958]\n",
      "Epoch: 11400 test cost= 19.558902740 W= [[ 2.68446827 -0.87575567]] b= [ 21.28936958]\n",
      "Epoch: 11450 training cost= 4.821653366 W= [[ 2.67699075 -0.86914545]] b= [ 21.33667755]\n",
      "Epoch: 11450 test cost= 19.426103592 W= [[ 2.67699075 -0.86914545]] b= [ 21.33667755]\n",
      "Epoch: 11500 training cost= 4.775788784 W= [[ 2.66954947 -0.86256617]] b= [ 21.38373566]\n",
      "Epoch: 11500 test cost= 19.294446945 W= [[ 2.66954947 -0.86256617]] b= [ 21.38373566]\n",
      "Epoch: 11550 training cost= 4.730379105 W= [[ 2.66214633 -0.85601979]] b= [ 21.43054962]\n",
      "Epoch: 11550 test cost= 19.164056778 W= [[ 2.66214633 -0.85601979]] b= [ 21.43054962]\n",
      "Epoch: 11600 training cost= 4.685330391 W= [[ 2.6547761  -0.84950668]] b= [ 21.47721672]\n",
      "Epoch: 11600 test cost= 19.034818649 W= [[ 2.6547761  -0.84950668]] b= [ 21.47721672]\n",
      "Epoch: 11650 training cost= 4.640731335 W= [[ 2.64744186 -0.84302402]] b= [ 21.52364159]\n",
      "Epoch: 11650 test cost= 18.906751633 W= [[ 2.64744186 -0.84302402]] b= [ 21.52364159]\n",
      "Epoch: 11700 training cost= 4.596584797 W= [[ 2.64014244 -0.83657241]] b= [ 21.56982231]\n",
      "Epoch: 11700 test cost= 18.779640198 W= [[ 2.64014244 -0.83657241]] b= [ 21.56982231]\n",
      "Epoch: 11750 training cost= 4.552883625 W= [[ 2.63287997 -0.8301509 ]] b= [ 21.6157589]\n",
      "Epoch: 11750 test cost= 18.653812408 W= [[ 2.63287997 -0.8301509 ]] b= [ 21.6157589]\n",
      "Epoch: 11800 training cost= 4.509617805 W= [[ 2.62565398 -0.82376176]] b= [ 21.66145897]\n",
      "Epoch: 11800 test cost= 18.529073715 W= [[ 2.62565398 -0.82376176]] b= [ 21.66145897]\n",
      "Epoch: 11850 training cost= 4.466703892 W= [[ 2.61846042 -0.81740487]] b= [ 21.70700645]\n",
      "Epoch: 11850 test cost= 18.405467987 W= [[ 2.61846042 -0.81740487]] b= [ 21.70700645]\n",
      "Epoch: 11900 training cost= 4.424217224 W= [[ 2.61130142 -0.81107759]] b= [ 21.75231743]\n",
      "Epoch: 11900 test cost= 18.282897949 W= [[ 2.61130142 -0.81107759]] b= [ 21.75231743]\n",
      "Epoch: 11950 training cost= 4.382158279 W= [[ 2.60417747 -0.80477965]] b= [ 21.7973938]\n",
      "Epoch: 11950 test cost= 18.161476135 W= [[ 2.60417747 -0.80477965]] b= [ 21.7973938]\n",
      "Epoch: 12000 training cost= 4.340522766 W= [[ 2.59708858 -0.79851246]] b= [ 21.84223366]\n",
      "Epoch: 12000 test cost= 18.041042328 W= [[ 2.59708858 -0.79851246]] b= [ 21.84223366]\n",
      "Epoch: 12050 training cost= 4.299297333 W= [[ 2.59003448 -0.79227471]] b= [ 21.88684082]\n",
      "Epoch: 12050 test cost= 17.921751022 W= [[ 2.59003448 -0.79227471]] b= [ 21.88684082]\n",
      "Epoch: 12100 training cost= 4.258413792 W= [[ 2.5830133  -0.78607029]] b= [ 21.93130112]\n",
      "Epoch: 12100 test cost= 17.803514481 W= [[ 2.5830133  -0.78607029]] b= [ 21.93130112]\n",
      "Epoch: 12150 training cost= 4.217931747 W= [[ 2.57602429 -0.7798937 ]] b= [ 21.97553635]\n",
      "Epoch: 12150 test cost= 17.686214447 W= [[ 2.57602429 -0.7798937 ]] b= [ 21.97553635]\n",
      "Epoch: 12200 training cost= 4.177862167 W= [[ 2.56906986 -0.7737456 ]] b= [ 22.01953316]\n",
      "Epoch: 12200 test cost= 17.570009232 W= [[ 2.56906986 -0.7737456 ]] b= [ 22.01953316]\n",
      "Epoch: 12250 training cost= 4.138189793 W= [[ 2.56215    -0.76762718]] b= [ 22.0633049]\n",
      "Epoch: 12250 test cost= 17.454864502 W= [[ 2.56215    -0.76762718]] b= [ 22.0633049]\n",
      "Epoch: 12300 training cost= 4.098924637 W= [[ 2.555264   -0.76153857]] b= [ 22.10684013]\n",
      "Epoch: 12300 test cost= 17.340621948 W= [[ 2.555264   -0.76153857]] b= [ 22.10684013]\n",
      "Epoch: 12350 training cost= 4.059983253 W= [[ 2.54841042 -0.7554813 ]] b= [ 22.15022659]\n",
      "Epoch: 12350 test cost= 17.227512360 W= [[ 2.54841042 -0.7554813 ]] b= [ 22.15022659]\n",
      "Epoch: 12400 training cost= 4.021413803 W= [[ 2.54158998 -0.7494536 ]] b= [ 22.19340324]\n",
      "Epoch: 12400 test cost= 17.115364075 W= [[ 2.54158998 -0.7494536 ]] b= [ 22.19340324]\n",
      "Epoch: 12450 training cost= 3.983228445 W= [[ 2.53480196 -0.74345326]] b= [ 22.23636055]\n",
      "Epoch: 12450 test cost= 17.004219055 W= [[ 2.53480196 -0.74345326]] b= [ 22.23636055]\n",
      "Epoch: 12500 training cost= 3.945425272 W= [[ 2.52804685 -0.73748118]] b= [ 22.27909279]\n",
      "Epoch: 12500 test cost= 16.894025803 W= [[ 2.52804685 -0.73748118]] b= [ 22.27909279]\n",
      "Epoch: 12550 training cost= 3.908009529 W= [[ 2.5213263  -0.73153925]] b= [ 22.32159996]\n",
      "Epoch: 12550 test cost= 16.784805298 W= [[ 2.5213263  -0.73153925]] b= [ 22.32159996]\n",
      "Epoch: 12600 training cost= 3.870922565 W= [[ 2.51463652 -0.72562569]] b= [ 22.36392975]\n",
      "Epoch: 12600 test cost= 16.676563263 W= [[ 2.51463652 -0.72562569]] b= [ 22.36392975]\n",
      "Epoch: 12650 training cost= 3.834177256 W= [[ 2.50797725 -0.71974057]] b= [ 22.40608215]\n",
      "Epoch: 12650 test cost= 16.569206238 W= [[ 2.50797725 -0.71974057]] b= [ 22.40608215]\n",
      "Epoch: 12700 training cost= 3.797790766 W= [[ 2.50135231 -0.71388459]] b= [ 22.44801903]\n",
      "Epoch: 12700 test cost= 16.462923050 W= [[ 2.50135231 -0.71388459]] b= [ 22.44801903]\n",
      "Epoch: 12750 training cost= 3.761769056 W= [[ 2.49475837 -0.70805621]] b= [ 22.48973846]\n",
      "Epoch: 12750 test cost= 16.357452393 W= [[ 2.49475837 -0.70805621]] b= [ 22.48973846]\n",
      "Epoch: 12800 training cost= 3.726115465 W= [[ 2.48819709 -0.70225453]] b= [ 22.53123474]\n",
      "Epoch: 12800 test cost= 16.252952576 W= [[ 2.48819709 -0.70225453]] b= [ 22.53123474]\n",
      "Epoch: 12850 training cost= 3.690814972 W= [[ 2.48166871 -0.69648165]] b= [ 22.57251549]\n",
      "Epoch: 12850 test cost= 16.149368286 W= [[ 2.48166871 -0.69648165]] b= [ 22.57251549]\n",
      "Epoch: 12900 training cost= 3.655790806 W= [[ 2.47516942 -0.69073856]] b= [ 22.61367226]\n",
      "Epoch: 12900 test cost= 16.046714783 W= [[ 2.47516942 -0.69073856]] b= [ 22.61367226]\n",
      "Epoch: 12950 training cost= 3.621125460 W= [[ 2.46870065 -0.6850214 ]] b= [ 22.65461159]\n",
      "Epoch: 12950 test cost= 15.944892883 W= [[ 2.46870065 -0.6850214 ]] b= [ 22.65461159]\n",
      "Epoch: 13000 training cost= 3.586805344 W= [[ 2.46226454 -0.67933184]] b= [ 22.6953392]\n",
      "Epoch: 13000 test cost= 15.844032288 W= [[ 2.46226454 -0.67933184]] b= [ 22.6953392]\n",
      "Epoch: 13050 training cost= 3.552822351 W= [[ 2.45585942 -0.67366934]] b= [ 22.7358551]\n",
      "Epoch: 13050 test cost= 15.743999481 W= [[ 2.45585942 -0.67366934]] b= [ 22.7358551]\n",
      "Epoch: 13100 training cost= 3.519186735 W= [[ 2.44948626 -0.66803432]] b= [ 22.77615929]\n",
      "Epoch: 13100 test cost= 15.644879341 W= [[ 2.44948626 -0.66803432]] b= [ 22.77615929]\n",
      "Epoch: 13150 training cost= 3.485866308 W= [[ 2.44314504 -0.66242766]] b= [ 22.81628036]\n",
      "Epoch: 13150 test cost= 15.546730042 W= [[ 2.44314504 -0.66242766]] b= [ 22.81628036]\n",
      "Epoch: 13200 training cost= 3.452825069 W= [[ 2.43683147 -0.65684891]] b= [ 22.85625648]\n",
      "Epoch: 13200 test cost= 15.449293137 W= [[ 2.43683147 -0.65684891]] b= [ 22.85625648]\n",
      "Epoch: 13250 training cost= 3.420114994 W= [[ 2.43054795 -0.65129542]] b= [ 22.8960228]\n",
      "Epoch: 13250 test cost= 15.352706909 W= [[ 2.43054795 -0.65129542]] b= [ 22.8960228]\n",
      "Epoch: 13300 training cost= 3.387733936 W= [[ 2.42429566 -0.64576811]] b= [ 22.93558311]\n",
      "Epoch: 13300 test cost= 15.257036209 W= [[ 2.42429566 -0.64576811]] b= [ 22.93558311]\n",
      "Epoch: 13350 training cost= 3.355680227 W= [[ 2.41807461 -0.64026845]] b= [ 22.97493553]\n",
      "Epoch: 13350 test cost= 15.162147522 W= [[ 2.41807461 -0.64026845]] b= [ 22.97493553]\n",
      "Epoch: 13400 training cost= 3.323947191 W= [[ 2.41188502 -0.63479525]] b= [ 23.01408005]\n",
      "Epoch: 13400 test cost= 15.068174362 W= [[ 2.41188502 -0.63479525]] b= [ 23.01408005]\n",
      "Epoch: 13450 training cost= 3.292490959 W= [[ 2.40572333 -0.62934893]] b= [ 23.05307198]\n",
      "Epoch: 13450 test cost= 14.974958420 W= [[ 2.40572333 -0.62934893]] b= [ 23.05307198]\n",
      "Epoch: 13500 training cost= 3.261318922 W= [[ 2.39959073 -0.6239292 ]] b= [ 23.09189987]\n",
      "Epoch: 13500 test cost= 14.882594109 W= [[ 2.39959073 -0.6239292 ]] b= [ 23.09189987]\n",
      "Epoch: 13550 training cost= 3.230461836 W= [[ 2.39348769 -0.61853528]] b= [ 23.1305275]\n",
      "Epoch: 13550 test cost= 14.790996552 W= [[ 2.39348769 -0.61853528]] b= [ 23.1305275]\n",
      "Epoch: 13600 training cost= 3.199913025 W= [[ 2.38741374 -0.61316538]] b= [ 23.16895103]\n",
      "Epoch: 13600 test cost= 14.700183868 W= [[ 2.38741374 -0.61316538]] b= [ 23.16895103]\n",
      "Epoch: 13650 training cost= 3.169667959 W= [[ 2.38137054 -0.60782212]] b= [ 23.2071743]\n",
      "Epoch: 13650 test cost= 14.610204697 W= [[ 2.38137054 -0.60782212]] b= [ 23.2071743]\n",
      "Epoch: 13700 training cost= 3.139733791 W= [[ 2.37535739 -0.60250473]] b= [ 23.24519539]\n",
      "Epoch: 13700 test cost= 14.520998001 W= [[ 2.37535739 -0.60250473]] b= [ 23.24519539]\n",
      "Epoch: 13750 training cost= 3.110052109 W= [[ 2.36937213 -0.59721464]] b= [ 23.28307343]\n",
      "Epoch: 13750 test cost= 14.432554245 W= [[ 2.36937213 -0.59721464]] b= [ 23.28307343]\n",
      "Epoch: 13800 training cost= 3.080642462 W= [[ 2.36341357 -0.59194887]] b= [ 23.32079124]\n",
      "Epoch: 13800 test cost= 14.344851494 W= [[ 2.36341357 -0.59194887]] b= [ 23.32079124]\n",
      "Epoch: 13850 training cost= 3.051526308 W= [[ 2.35748506 -0.58670843]] b= [ 23.35831261]\n",
      "Epoch: 13850 test cost= 14.257985115 W= [[ 2.35748506 -0.58670843]] b= [ 23.35831261]\n",
      "Epoch: 13900 training cost= 3.022698879 W= [[ 2.35158539 -0.58149308]] b= [ 23.39563942]\n",
      "Epoch: 13900 test cost= 14.171831131 W= [[ 2.35158539 -0.58149308]] b= [ 23.39563942]\n",
      "Epoch: 13950 training cost= 2.994160175 W= [[ 2.34571457 -0.57630289]] b= [ 23.43277359]\n",
      "Epoch: 13950 test cost= 14.086400032 W= [[ 2.34571457 -0.57630289]] b= [ 23.43277359]\n",
      "Epoch: 14000 training cost= 2.965909481 W= [[ 2.33987331 -0.57113743]] b= [ 23.4697113]\n",
      "Epoch: 14000 test cost= 14.001775742 W= [[ 2.33987331 -0.57113743]] b= [ 23.4697113]\n",
      "Epoch: 14050 training cost= 2.937912941 W= [[ 2.33405972 -0.56599855]] b= [ 23.50650215]\n",
      "Epoch: 14050 test cost= 13.917886734 W= [[ 2.33405972 -0.56599855]] b= [ 23.50650215]\n",
      "Epoch: 14100 training cost= 2.910159588 W= [[ 2.32827282 -0.56088531]] b= [ 23.54314804]\n",
      "Epoch: 14100 test cost= 13.834709167 W= [[ 2.32827282 -0.56088531]] b= [ 23.54314804]\n",
      "Epoch: 14150 training cost= 2.882679701 W= [[ 2.32251287 -0.55579418]] b= [ 23.5796032]\n",
      "Epoch: 14150 test cost= 13.752260208 W= [[ 2.32251287 -0.55579418]] b= [ 23.5796032]\n",
      "Epoch: 14200 training cost= 2.855481386 W= [[ 2.31678271 -0.55072904]] b= [ 23.61586571]\n",
      "Epoch: 14200 test cost= 13.670532227 W= [[ 2.31678271 -0.55072904]] b= [ 23.61586571]\n",
      "Epoch: 14250 training cost= 2.828555822 W= [[ 2.31107998 -0.54568744]] b= [ 23.65193748]\n",
      "Epoch: 14250 test cost= 13.589468002 W= [[ 2.31107998 -0.54568744]] b= [ 23.65193748]\n",
      "Epoch: 14300 training cost= 2.801898241 W= [[ 2.30540514 -0.54066896]] b= [ 23.68781853]\n",
      "Epoch: 14300 test cost= 13.509141922 W= [[ 2.30540514 -0.54066896]] b= [ 23.68781853]\n",
      "Epoch: 14350 training cost= 2.775505543 W= [[ 2.29975891 -0.5356763 ]] b= [ 23.72352409]\n",
      "Epoch: 14350 test cost= 13.429517746 W= [[ 2.29975891 -0.5356763 ]] b= [ 23.72352409]\n",
      "Epoch: 14400 training cost= 2.749319315 W= [[ 2.29413795 -0.53070986]] b= [ 23.75912094]\n",
      "Epoch: 14400 test cost= 13.350585938 W= [[ 2.29413795 -0.53070986]] b= [ 23.75912094]\n",
      "Epoch: 14450 training cost= 2.723387003 W= [[ 2.28854275 -0.52576524]] b= [ 23.79454041]\n",
      "Epoch: 14450 test cost= 13.272304535 W= [[ 2.28854275 -0.52576524]] b= [ 23.79454041]\n",
      "Epoch: 14500 training cost= 2.697716713 W= [[ 2.2829752  -0.52084357]] b= [ 23.82977486]\n",
      "Epoch: 14500 test cost= 13.194750786 W= [[ 2.2829752  -0.52084357]] b= [ 23.82977486]\n",
      "Epoch: 14550 training cost= 2.672304869 W= [[ 2.2774353  -0.51594645]] b= [ 23.8648262]\n",
      "Epoch: 14550 test cost= 13.117821693 W= [[ 2.2774353  -0.51594645]] b= [ 23.8648262]\n",
      "Epoch: 14600 training cost= 2.647150040 W= [[ 2.27192211 -0.51107204]] b= [ 23.89969063]\n",
      "Epoch: 14600 test cost= 13.041536331 W= [[ 2.27192211 -0.51107204]] b= [ 23.89969063]\n",
      "Epoch: 14650 training cost= 2.622250795 W= [[ 2.26643729 -0.50622106]] b= [ 23.93436813]\n",
      "Epoch: 14650 test cost= 12.965981483 W= [[ 2.26643729 -0.50622106]] b= [ 23.93436813]\n",
      "Epoch: 14700 training cost= 2.597558498 W= [[ 2.26097655 -0.50139457]] b= [ 23.96892548]\n",
      "Epoch: 14700 test cost= 12.891036987 W= [[ 2.26097655 -0.50139457]] b= [ 23.96892548]\n",
      "Epoch: 14750 training cost= 2.573092699 W= [[ 2.25554252 -0.49659255]] b= [ 24.00333405]\n",
      "Epoch: 14750 test cost= 12.816780090 W= [[ 2.25554252 -0.49659255]] b= [ 24.00333405]\n",
      "Epoch: 14800 training cost= 2.548867702 W= [[ 2.25013471 -0.49181288]] b= [ 24.03756332]\n",
      "Epoch: 14800 test cost= 12.743160248 W= [[ 2.25013471 -0.49181288]] b= [ 24.03756332]\n",
      "Epoch: 14850 training cost= 2.524884462 W= [[ 2.24475241 -0.48705524]] b= [ 24.07162094]\n",
      "Epoch: 14850 test cost= 12.670137405 W= [[ 2.24475241 -0.48705524]] b= [ 24.07162094]\n",
      "Epoch: 14900 training cost= 2.501143217 W= [[ 2.23939681 -0.48232049]] b= [ 24.10549927]\n",
      "Epoch: 14900 test cost= 12.597745895 W= [[ 2.23939681 -0.48232049]] b= [ 24.10549927]\n",
      "Epoch: 14950 training cost= 2.477640867 W= [[ 2.23406768 -0.47760859]] b= [ 24.13920021]\n",
      "Epoch: 14950 test cost= 12.525974274 W= [[ 2.23406768 -0.47760859]] b= [ 24.13920021]\n",
      "Epoch: 15000 training cost= 2.454402685 W= [[ 2.22876716 -0.47291964]] b= [ 24.17268372]\n",
      "Epoch: 15000 test cost= 12.454813004 W= [[ 2.22876716 -0.47291964]] b= [ 24.17268372]\n",
      "Epoch: 15050 training cost= 2.431373119 W= [[ 2.2234931 -0.4682546]] b= [ 24.20602036]\n",
      "Epoch: 15050 test cost= 12.384356499 W= [[ 2.2234931 -0.4682546]] b= [ 24.20602036]\n",
      "Epoch: 15100 training cost= 2.408576488 W= [[ 2.21824551 -0.46361345]] b= [ 24.23918533]\n",
      "Epoch: 15100 test cost= 12.314450264 W= [[ 2.21824551 -0.46361345]] b= [ 24.23918533]\n",
      "Epoch: 15150 training cost= 2.386002779 W= [[ 2.21302366 -0.45899493]] b= [ 24.27218819]\n",
      "Epoch: 15150 test cost= 12.245182037 W= [[ 2.21302366 -0.45899493]] b= [ 24.27218819]\n",
      "Epoch: 15200 training cost= 2.363656759 W= [[ 2.207829   -0.45439985]] b= [ 24.30501747]\n",
      "Epoch: 15200 test cost= 12.176561356 W= [[ 2.207829   -0.45439985]] b= [ 24.30501747]\n",
      "Epoch: 15250 training cost= 2.341533184 W= [[ 2.20266104 -0.44982862]] b= [ 24.33767509]\n",
      "Epoch: 15250 test cost= 12.108525276 W= [[ 2.20266104 -0.44982862]] b= [ 24.33767509]\n",
      "Epoch: 15300 training cost= 2.319638014 W= [[ 2.19751978 -0.44528055]] b= [ 24.37015152]\n",
      "Epoch: 15300 test cost= 12.041061401 W= [[ 2.19751978 -0.44528055]] b= [ 24.37015152]\n",
      "Epoch: 15350 training cost= 2.297946453 W= [[ 2.19240403 -0.44075501]] b= [ 24.40247726]\n",
      "Epoch: 15350 test cost= 11.974242210 W= [[ 2.19240403 -0.44075501]] b= [ 24.40247726]\n",
      "Epoch: 15400 training cost= 2.276426554 W= [[ 2.18731165 -0.43625468]] b= [ 24.43471336]\n",
      "Epoch: 15400 test cost= 11.907995224 W= [[ 2.18731165 -0.43625468]] b= [ 24.43471336]\n",
      "Epoch: 15450 training cost= 2.255113602 W= [[ 2.18224287 -0.43177447]] b= [ 24.46678925]\n",
      "Epoch: 15450 test cost= 11.842285156 W= [[ 2.18224287 -0.43177447]] b= [ 24.46678925]\n",
      "Epoch: 15500 training cost= 2.234017134 W= [[ 2.1771996 -0.4273155]] b= [ 24.49869537]\n",
      "Epoch: 15500 test cost= 11.777203560 W= [[ 2.1771996 -0.4273155]] b= [ 24.49869537]\n",
      "Epoch: 15550 training cost= 2.213137627 W= [[ 2.17218184 -0.42287868]] b= [ 24.53042603]\n",
      "Epoch: 15550 test cost= 11.712637901 W= [[ 2.17218184 -0.42287868]] b= [ 24.53042603]\n",
      "Epoch: 15600 training cost= 2.192464828 W= [[ 2.16718888 -0.41846287]] b= [ 24.56198883]\n",
      "Epoch: 15600 test cost= 11.648630142 W= [[ 2.16718888 -0.41846287]] b= [ 24.56198883]\n",
      "Epoch: 15650 training cost= 2.172002554 W= [[ 2.16222119 -0.4140698 ]] b= [ 24.59338951]\n",
      "Epoch: 15650 test cost= 11.585168839 W= [[ 2.16222119 -0.4140698 ]] b= [ 24.59338951]\n",
      "Epoch: 15700 training cost= 2.151731253 W= [[ 2.15727711 -0.40969723]] b= [ 24.62464523]\n",
      "Epoch: 15700 test cost= 11.522283554 W= [[ 2.15727711 -0.40969723]] b= [ 24.62464523]\n",
      "Epoch: 15750 training cost= 2.131617308 W= [[ 2.15235567 -0.40534854]] b= [ 24.65581131]\n",
      "Epoch: 15750 test cost= 11.459941864 W= [[ 2.15235567 -0.40534854]] b= [ 24.65581131]\n",
      "Epoch: 15800 training cost= 2.111705542 W= [[ 2.14745855 -0.40102044]] b= [ 24.68681526]\n",
      "Epoch: 15800 test cost= 11.398163795 W= [[ 2.14745855 -0.40102044]] b= [ 24.68681526]\n",
      "Epoch: 15850 training cost= 2.091998816 W= [[ 2.1425848  -0.39671239]] b= [ 24.71764755]\n",
      "Epoch: 15850 test cost= 11.336826324 W= [[ 2.1425848  -0.39671239]] b= [ 24.71764755]\n",
      "Epoch: 15900 training cost= 2.072469950 W= [[ 2.13773465 -0.39242536]] b= [ 24.74834251]\n",
      "Epoch: 15900 test cost= 11.276080132 W= [[ 2.13773465 -0.39242536]] b= [ 24.74834251]\n",
      "Epoch: 15950 training cost= 2.053150415 W= [[ 2.13290954 -0.38815907]] b= [ 24.77886581]\n",
      "Epoch: 15950 test cost= 11.215864182 W= [[ 2.13290954 -0.38815907]] b= [ 24.77886581]\n",
      "Epoch: 16000 training cost= 2.034024477 W= [[ 2.12810683 -0.38391197]] b= [ 24.80922508]\n",
      "Epoch: 16000 test cost= 11.156095505 W= [[ 2.12810683 -0.38391197]] b= [ 24.80922508]\n",
      "Epoch: 16050 training cost= 2.015094280 W= [[ 2.12332964 -0.37968644]] b= [ 24.83941841]\n",
      "Epoch: 16050 test cost= 11.096900940 W= [[ 2.12332964 -0.37968644]] b= [ 24.83941841]\n",
      "Epoch: 16100 training cost= 1.996305823 W= [[ 2.1185739  -0.37548366]] b= [ 24.86953163]\n",
      "Epoch: 16100 test cost= 11.038240433 W= [[ 2.1185739  -0.37548366]] b= [ 24.86953163]\n",
      "Epoch: 16150 training cost= 1.977686524 W= [[ 2.11383963 -0.37130108]] b= [ 24.89952087]\n",
      "Epoch: 16150 test cost= 10.980043411 W= [[ 2.11383963 -0.37130108]] b= [ 24.89952087]\n",
      "Epoch: 16200 training cost= 1.959262490 W= [[ 2.10912848 -0.36713716]] b= [ 24.92933846]\n",
      "Epoch: 16200 test cost= 10.922332764 W= [[ 2.10912848 -0.36713716]] b= [ 24.92933846]\n",
      "Epoch: 16250 training cost= 1.941026330 W= [[ 2.10443974 -0.36299202]] b= [ 24.95899582]\n",
      "Epoch: 16250 test cost= 10.865092278 W= [[ 2.10443974 -0.36299202]] b= [ 24.95899582]\n",
      "Epoch: 16300 training cost= 1.922958970 W= [[ 2.09977365 -0.35886732]] b= [ 24.98852158]\n",
      "Epoch: 16300 test cost= 10.808331490 W= [[ 2.09977365 -0.35886732]] b= [ 24.98852158]\n",
      "Epoch: 16350 training cost= 1.905081868 W= [[ 2.09513259 -0.35476336]] b= [ 25.01787949]\n",
      "Epoch: 16350 test cost= 10.752148628 W= [[ 2.09513259 -0.35476336]] b= [ 25.01787949]\n",
      "Epoch: 16400 training cost= 1.887385368 W= [[ 2.09051323 -0.3506788 ]] b= [ 25.04707909]\n",
      "Epoch: 16400 test cost= 10.696313858 W= [[ 2.09051323 -0.3506788 ]] b= [ 25.04707909]\n",
      "Epoch: 16450 training cost= 1.869864225 W= [[ 2.08591771 -0.34661445]] b= [ 25.07612801]\n",
      "Epoch: 16450 test cost= 10.641023636 W= [[ 2.08591771 -0.34661445]] b= [ 25.07612801]\n",
      "Epoch: 16500 training cost= 1.852465153 W= [[ 2.08134079 -0.34257054]] b= [ 25.1051178]\n",
      "Epoch: 16500 test cost= 10.586211205 W= [[ 2.08134079 -0.34257054]] b= [ 25.1051178]\n",
      "Epoch: 16550 training cost= 1.835238695 W= [[ 2.07678604 -0.33854598]] b= [ 25.13396263]\n",
      "Epoch: 16550 test cost= 10.531857491 W= [[ 2.07678604 -0.33854598]] b= [ 25.13396263]\n",
      "Epoch: 16600 training cost= 1.818187356 W= [[ 2.07225394 -0.33453998]] b= [ 25.16264725]\n",
      "Epoch: 16600 test cost= 10.477986336 W= [[ 2.07225394 -0.33453998]] b= [ 25.16264725]\n",
      "Epoch: 16650 training cost= 1.801301122 W= [[ 2.06774354 -0.33055362]] b= [ 25.19119263]\n",
      "Epoch: 16650 test cost= 10.424526215 W= [[ 2.06774354 -0.33055362]] b= [ 25.19119263]\n",
      "Epoch: 16700 training cost= 1.784584045 W= [[ 2.06325459 -0.32658535]] b= [ 25.21959114]\n",
      "Epoch: 16700 test cost= 10.371504784 W= [[ 2.06325459 -0.32658535]] b= [ 25.21959114]\n",
      "Epoch: 16750 training cost= 1.768041968 W= [[ 2.05878806 -0.32263544]] b= [ 25.24783134]\n",
      "Epoch: 16750 test cost= 10.318955421 W= [[ 2.05878806 -0.32263544]] b= [ 25.24783134]\n",
      "Epoch: 16800 training cost= 1.751662254 W= [[ 2.0543437  -0.31870452]] b= [ 25.27592278]\n",
      "Epoch: 16800 test cost= 10.266841888 W= [[ 2.0543437  -0.31870452]] b= [ 25.27592278]\n",
      "Epoch: 16850 training cost= 1.735442519 W= [[ 2.04992366 -0.31479663]] b= [ 25.30387878]\n",
      "Epoch: 16850 test cost= 10.215198517 W= [[ 2.04992366 -0.31479663]] b= [ 25.30387878]\n",
      "Epoch: 16900 training cost= 1.719345450 W= [[ 2.04552293 -0.31090802]] b= [ 25.33175659]\n",
      "Epoch: 16900 test cost= 10.164075851 W= [[ 2.04552293 -0.31090802]] b= [ 25.33175659]\n",
      "Epoch: 16950 training cost= 1.703409553 W= [[ 2.04114151 -0.30703679]] b= [ 25.35949516]\n",
      "Epoch: 16950 test cost= 10.113267899 W= [[ 2.04114151 -0.30703679]] b= [ 25.35949516]\n",
      "Epoch: 17000 training cost= 1.687615037 W= [[ 2.03677988 -0.30318257]] b= [ 25.38711739]\n",
      "Epoch: 17000 test cost= 10.062924385 W= [[ 2.03677988 -0.30318257]] b= [ 25.38711739]\n",
      "Epoch: 17050 training cost= 1.671989202 W= [[ 2.03244019 -0.29934651]] b= [ 25.41457558]\n",
      "Epoch: 17050 test cost= 10.012983322 W= [[ 2.03244019 -0.29934651]] b= [ 25.41457558]\n",
      "Epoch: 17100 training cost= 1.656528711 W= [[ 2.02812243 -0.29552835]] b= [ 25.44187737]\n",
      "Epoch: 17100 test cost= 9.963448524 W= [[ 2.02812243 -0.29552835]] b= [ 25.44187737]\n",
      "Epoch: 17150 training cost= 1.641205430 W= [[ 2.02382541 -0.29172957]] b= [ 25.46906281]\n",
      "Epoch: 17150 test cost= 9.914367676 W= [[ 2.02382541 -0.29172957]] b= [ 25.46906281]\n",
      "Epoch: 17200 training cost= 1.626045227 W= [[ 2.01954985 -0.28794873]] b= [ 25.49609756]\n",
      "Epoch: 17200 test cost= 9.865683556 W= [[ 2.01954985 -0.28794873]] b= [ 25.49609756]\n",
      "Epoch: 17250 training cost= 1.611042738 W= [[ 2.01529622 -0.28418612]] b= [ 25.52297592]\n",
      "Epoch: 17250 test cost= 9.817405701 W= [[ 2.01529622 -0.28418612]] b= [ 25.52297592]\n",
      "Epoch: 17300 training cost= 1.596154094 W= [[ 2.01106215 -0.28044438]] b= [ 25.54978371]\n",
      "Epoch: 17300 test cost= 9.769573212 W= [[ 2.01106215 -0.28044438]] b= [ 25.54978371]\n",
      "Epoch: 17350 training cost= 1.581391215 W= [[ 2.00684667 -0.27672008]] b= [ 25.57648849]\n",
      "Epoch: 17350 test cost= 9.722152710 W= [[ 2.00684667 -0.27672008]] b= [ 25.57648849]\n",
      "Epoch: 17400 training cost= 1.566788197 W= [[ 2.00265169 -0.27301222]] b= [ 25.60303497]\n",
      "Epoch: 17400 test cost= 9.675086021 W= [[ 2.00265169 -0.27301222]] b= [ 25.60303497]\n",
      "Epoch: 17450 training cost= 1.552318573 W= [[ 1.99847674 -0.26932293]] b= [ 25.62946892]\n",
      "Epoch: 17450 test cost= 9.628427505 W= [[ 1.99847674 -0.26932293]] b= [ 25.62946892]\n",
      "Epoch: 17500 training cost= 1.537997365 W= [[ 1.99432254 -0.26565066]] b= [ 25.6557579]\n",
      "Epoch: 17500 test cost= 9.582172394 W= [[ 1.99432254 -0.26565066]] b= [ 25.6557579]\n",
      "Epoch: 17550 training cost= 1.523829460 W= [[ 1.99018872 -0.26199502]] b= [ 25.68189049]\n",
      "Epoch: 17550 test cost= 9.536255836 W= [[ 1.99018872 -0.26199502]] b= [ 25.68189049]\n",
      "Epoch: 17600 training cost= 1.509788990 W= [[ 1.98607564 -0.25835916]] b= [ 25.70791435]\n",
      "Epoch: 17600 test cost= 9.490772247 W= [[ 1.98607564 -0.25835916]] b= [ 25.70791435]\n",
      "Epoch: 17650 training cost= 1.495896339 W= [[ 1.98198295 -0.25473967]] b= [ 25.73379135]\n",
      "Epoch: 17650 test cost= 9.445658684 W= [[ 1.98198295 -0.25473967]] b= [ 25.73379135]\n",
      "Epoch: 17700 training cost= 1.482147574 W= [[ 1.97791123 -0.25113818]] b= [ 25.75951767]\n",
      "Epoch: 17700 test cost= 9.400911331 W= [[ 1.97791123 -0.25113818]] b= [ 25.75951767]\n",
      "Epoch: 17750 training cost= 1.468487978 W= [[ 1.97385645 -0.24755596]] b= [ 25.78520966]\n",
      "Epoch: 17750 test cost= 9.356616974 W= [[ 1.97385645 -0.24755596]] b= [ 25.78520966]\n",
      "Epoch: 17800 training cost= 1.454959035 W= [[ 1.96982074 -0.24399017]] b= [ 25.81077194]\n",
      "Epoch: 17800 test cost= 9.312644005 W= [[ 1.96982074 -0.24399017]] b= [ 25.81077194]\n",
      "Epoch: 17850 training cost= 1.441569209 W= [[ 1.96580434 -0.24044086]] b= [ 25.83619881]\n",
      "Epoch: 17850 test cost= 9.269025803 W= [[ 1.96580434 -0.24044086]] b= [ 25.83619881]\n",
      "Epoch: 17900 training cost= 1.428307176 W= [[ 1.96180689 -0.23690806]] b= [ 25.86150742]\n",
      "Epoch: 17900 test cost= 9.225784302 W= [[ 1.96180689 -0.23690806]] b= [ 25.86150742]\n",
      "Epoch: 17950 training cost= 1.415187120 W= [[ 1.95782936 -0.23339134]] b= [ 25.88666344]\n",
      "Epoch: 17950 test cost= 9.182876587 W= [[ 1.95782936 -0.23339134]] b= [ 25.88666344]\n",
      "Epoch: 18000 training cost= 1.402189732 W= [[ 1.95387149 -0.22989225]] b= [ 25.9117012]\n",
      "Epoch: 18000 test cost= 9.140332222 W= [[ 1.95387149 -0.22989225]] b= [ 25.9117012]\n",
      "Epoch: 18050 training cost= 1.389324784 W= [[ 1.94993293 -0.22640975]] b= [ 25.93660736]\n",
      "Epoch: 18050 test cost= 9.098146439 W= [[ 1.94993293 -0.22640975]] b= [ 25.93660736]\n",
      "Epoch: 18100 training cost= 1.376603007 W= [[ 1.94601512 -0.2229439 ]] b= [ 25.96135902]\n",
      "Epoch: 18100 test cost= 9.056299210 W= [[ 1.94601512 -0.2229439 ]] b= [ 25.96135902]\n",
      "Epoch: 18150 training cost= 1.363987088 W= [[ 1.94211602 -0.21949656]] b= [ 25.98601913]\n",
      "Epoch: 18150 test cost= 9.014829636 W= [[ 1.94211602 -0.21949656]] b= [ 25.98601913]\n",
      "Epoch: 18200 training cost= 1.351478934 W= [[ 1.93823528 -0.21606667]] b= [ 26.01059151]\n",
      "Epoch: 18200 test cost= 8.973756790 W= [[ 1.93823528 -0.21606667]] b= [ 26.01059151]\n",
      "Epoch: 18250 training cost= 1.339082241 W= [[ 1.93437171 -0.21265371]] b= [ 26.0350666]\n",
      "Epoch: 18250 test cost= 8.932978630 W= [[ 1.93437171 -0.21265371]] b= [ 26.0350666]\n",
      "Epoch: 18300 training cost= 1.326802611 W= [[ 1.93052602 -0.20925568]] b= [ 26.05942535]\n",
      "Epoch: 18300 test cost= 8.892562866 W= [[ 1.93052602 -0.20925568]] b= [ 26.05942535]\n",
      "Epoch: 18350 training cost= 1.314657688 W= [[ 1.9266994  -0.20587264]] b= [ 26.08363152]\n",
      "Epoch: 18350 test cost= 8.852436066 W= [[ 1.9266994  -0.20587264]] b= [ 26.08363152]\n",
      "Epoch: 18400 training cost= 1.302617311 W= [[ 1.92289078 -0.20250683]] b= [ 26.1077404]\n",
      "Epoch: 18400 test cost= 8.812661171 W= [[ 1.92289078 -0.20250683]] b= [ 26.1077404]\n",
      "Epoch: 18450 training cost= 1.290710330 W= [[ 1.91910136 -0.1991563 ]] b= [ 26.13170624]\n",
      "Epoch: 18450 test cost= 8.773209572 W= [[ 1.91910136 -0.1991563 ]] b= [ 26.13170624]\n",
      "Epoch: 18500 training cost= 1.278921366 W= [[ 1.91533029 -0.19582154]] b= [ 26.15554619]\n",
      "Epoch: 18500 test cost= 8.734066963 W= [[ 1.91533029 -0.19582154]] b= [ 26.15554619]\n",
      "Epoch: 18550 training cost= 1.267243981 W= [[ 1.91157842 -0.19250429]] b= [ 26.17927551]\n",
      "Epoch: 18550 test cost= 8.695270538 W= [[ 1.91157842 -0.19250429]] b= [ 26.17927551]\n",
      "Epoch: 18600 training cost= 1.255699277 W= [[ 1.90784633 -0.18920252]] b= [ 26.20285416]\n",
      "Epoch: 18600 test cost= 8.656813622 W= [[ 1.90784633 -0.18920252]] b= [ 26.20285416]\n",
      "Epoch: 18650 training cost= 1.244241953 W= [[ 1.90413153 -0.18591867]] b= [ 26.22635841]\n",
      "Epoch: 18650 test cost= 8.618677139 W= [[ 1.90413153 -0.18591867]] b= [ 26.22635841]\n",
      "Epoch: 18700 training cost= 1.232874274 W= [[ 1.90043247 -0.18265088]] b= [ 26.24979973]\n",
      "Epoch: 18700 test cost= 8.580883026 W= [[ 1.90043247 -0.18265088]] b= [ 26.24979973]\n",
      "Epoch: 18750 training cost= 1.221621752 W= [[ 1.89675021 -0.17939769]] b= [ 26.27311897]\n",
      "Epoch: 18750 test cost= 8.543367386 W= [[ 1.89675021 -0.17939769]] b= [ 26.27311897]\n",
      "Epoch: 18800 training cost= 1.210476518 W= [[ 1.89308584 -0.17615907]] b= [ 26.29631996]\n",
      "Epoch: 18800 test cost= 8.506172180 W= [[ 1.89308584 -0.17615907]] b= [ 26.29631996]\n",
      "Epoch: 18850 training cost= 1.199452519 W= [[ 1.88944006 -0.17293608]] b= [ 26.31938171]\n",
      "Epoch: 18850 test cost= 8.469252586 W= [[ 1.88944006 -0.17293608]] b= [ 26.31938171]\n",
      "Epoch: 18900 training cost= 1.188527822 W= [[ 1.88581133 -0.16972908]] b= [ 26.34235191]\n",
      "Epoch: 18900 test cost= 8.432669640 W= [[ 1.88581133 -0.16972908]] b= [ 26.34235191]\n",
      "Epoch: 18950 training cost= 1.177722335 W= [[ 1.88220131 -0.16653664]] b= [ 26.36517525]\n",
      "Epoch: 18950 test cost= 8.396369934 W= [[ 1.88220131 -0.16653664]] b= [ 26.36517525]\n",
      "Epoch: 19000 training cost= 1.167010903 W= [[ 1.87860787 -0.16335995]] b= [ 26.38790894]\n",
      "Epoch: 19000 test cost= 8.360350609 W= [[ 1.87860787 -0.16335995]] b= [ 26.38790894]\n",
      "Epoch: 19050 training cost= 1.156420588 W= [[ 1.87503409 -0.16019896]] b= [ 26.41049957]\n",
      "Epoch: 19050 test cost= 8.324678421 W= [[ 1.87503409 -0.16019896]] b= [ 26.41049957]\n",
      "Epoch: 19100 training cost= 1.145935774 W= [[ 1.87147772 -0.15705369]] b= [ 26.43297577]\n",
      "Epoch: 19100 test cost= 8.289261818 W= [[ 1.87147772 -0.15705369]] b= [ 26.43297577]\n",
      "Epoch: 19150 training cost= 1.135541916 W= [[ 1.86793923 -0.15392475]] b= [ 26.4553566]\n",
      "Epoch: 19150 test cost= 8.254198074 W= [[ 1.86793923 -0.15392475]] b= [ 26.4553566]\n",
      "Epoch: 19200 training cost= 1.125227451 W= [[ 1.86441505 -0.15081185]] b= [ 26.47768784]\n",
      "Epoch: 19200 test cost= 8.219407082 W= [[ 1.86441505 -0.15081185]] b= [ 26.47768784]\n",
      "Epoch: 19250 training cost= 1.114999056 W= [[ 1.86090672 -0.14771304]] b= [ 26.49992561]\n",
      "Epoch: 19250 test cost= 8.184924126 W= [[ 1.86090672 -0.14771304]] b= [ 26.49992561]\n",
      "Epoch: 19300 training cost= 1.104896188 W= [[ 1.85741603 -0.14462733]] b= [ 26.52200699]\n",
      "Epoch: 19300 test cost= 8.150646210 W= [[ 1.85741603 -0.14462733]] b= [ 26.52200699]\n",
      "Epoch: 19350 training cost= 1.094874859 W= [[ 1.8539412  -0.14155714]] b= [ 26.54401588]\n",
      "Epoch: 19350 test cost= 8.116699219 W= [[ 1.8539412  -0.14155714]] b= [ 26.54401588]\n",
      "Epoch: 19400 training cost= 1.084967136 W= [[ 1.85048449 -0.13850035]] b= [ 26.5658741]\n",
      "Epoch: 19400 test cost= 8.083000183 W= [[ 1.85048449 -0.13850035]] b= [ 26.5658741]\n",
      "Epoch: 19450 training cost= 1.075147510 W= [[ 1.84704363 -0.13545936]] b= [ 26.58765221]\n",
      "Epoch: 19450 test cost= 8.049594879 W= [[ 1.84704363 -0.13545936]] b= [ 26.58765221]\n",
      "Epoch: 19500 training cost= 1.065436125 W= [[ 1.84362173 -0.13243274]] b= [ 26.60928535]\n",
      "Epoch: 19500 test cost= 8.016471863 W= [[ 1.84362173 -0.13243274]] b= [ 26.60928535]\n",
      "Epoch: 19550 training cost= 1.055817127 W= [[ 1.84021556 -0.1294211 ]] b= [ 26.63082123]\n",
      "Epoch: 19550 test cost= 7.983594894 W= [[ 1.84021556 -0.1294211 ]] b= [ 26.63082123]\n",
      "Epoch: 19600 training cost= 1.046296716 W= [[ 1.83682716 -0.12642409]] b= [ 26.65224075]\n",
      "Epoch: 19600 test cost= 7.951026917 W= [[ 1.83682716 -0.12642409]] b= [ 26.65224075]\n",
      "Epoch: 19650 training cost= 1.036869407 W= [[ 1.8334552  -0.12344203]] b= [ 26.67355156]\n",
      "Epoch: 19650 test cost= 7.918688297 W= [[ 1.8334552  -0.12344203]] b= [ 26.67355156]\n",
      "Epoch: 19700 training cost= 1.027510047 W= [[ 1.83009863 -0.12047603]] b= [ 26.69481277]\n",
      "Epoch: 19700 test cost= 7.886681557 W= [[ 1.83009863 -0.12047603]] b= [ 26.69481277]\n",
      "Epoch: 19750 training cost= 1.018236160 W= [[ 1.82675636 -0.11752354]] b= [ 26.71598625]\n",
      "Epoch: 19750 test cost= 7.854905605 W= [[ 1.82675636 -0.11752354]] b= [ 26.71598625]\n",
      "Epoch: 19800 training cost= 1.009044766 W= [[ 1.82342923 -0.11458439]] b= [ 26.73707008]\n",
      "Epoch: 19800 test cost= 7.823388100 W= [[ 1.82342923 -0.11458439]] b= [ 26.73707008]\n",
      "Epoch: 19850 training cost= 0.999957383 W= [[ 1.8201189  -0.11165792]] b= [ 26.75801086]\n",
      "Epoch: 19850 test cost= 7.792107105 W= [[ 1.8201189  -0.11165792]] b= [ 26.75801086]\n",
      "Epoch: 19900 training cost= 0.990954638 W= [[ 1.81682348 -0.10874531]] b= [ 26.77886963]\n",
      "Epoch: 19900 test cost= 7.761073589 W= [[ 1.81682348 -0.10874531]] b= [ 26.77886963]\n",
      "Epoch: 19950 training cost= 0.982046187 W= [[ 1.81354535 -0.10584697]] b= [ 26.7996006]\n",
      "Epoch: 19950 test cost= 7.730296135 W= [[ 1.81354535 -0.10584697]] b= [ 26.7996006]\n",
      "Epoch: 20000 training cost= 0.973220766 W= [[ 1.81028318 -0.102963  ]] b= [ 26.82024193]\n",
      "Epoch: 20000 test cost= 7.699787617 W= [[ 1.81028318 -0.102963  ]] b= [ 26.82024193]\n"
     ]
    }
   ],
   "source": [
    "cost_tracker = pd.DataFrame([None,None,None],index = ['Iteration','Training Cost', 'Test Cost']).T.iloc[0:0,:]\n",
    "for i in range(training_iterations):\n",
    "    for j in range(len(train_ft.eval())):\n",
    "        sess.run(train_step,feed_dict = {x: train_ft.eval()[j,:].reshape(1,2), y_: train_resp.eval()[j,:].reshape(1,1)})\n",
    "    \n",
    "    if (i+1) % 50 == 0:\n",
    "        train_cost = sess.run(criterion, feed_dict = {x: train_ft.eval(), y_:train_resp.eval()})\n",
    "        test_cost = sess.run(criterion, feed_dict = {x: test_ft.eval(), y_:test_resp.eval()})\n",
    "        ct_row = pd.DataFrame([i+1,train_cost,test_cost],index = ['Iteration','Training Cost', 'Test Cost']).T\n",
    "        cost_tracker = cost_tracker.append(ct_row)\n",
    "        \n",
    "        print(\"Epoch:\", '%04d' % (i+1), \"training cost=\", \"{:.9f}\".format(train_cost),\n",
    "        \"W=\", sess.run(W1), \"b=\", sess.run(b1))\n",
    "        print(\"Epoch:\", '%04d' % (i+1), \"test cost=\", \"{:.9f}\".format(test_cost),\n",
    "        \"W=\", sess.run(W1), \"b=\", sess.run(b1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd4FVX+x/H3CQkkEEoIEBAIVTooKIpKyYoKiCKKKCqK\n2Nuq61rAXQW2KOjPtnZddbFiRYqwICUsWAAFBDGEIgQBSWgBEkhIOb8/5gYCJJTk3px7k8/reebJ\n3Mmdud8MgQ/nnDkzxlqLiIhImOsCREQkOCgQREQEUCCIiIiPAkFERAAFgoiI+CgQREQEOIFAMMa8\nZYxJNcYsL7Qtxhgz0xiTbIyZYYypWeh7I40xa4wxScaYiwJVuIiI+NeJtBDeAfocsW0EMMta2xqY\nA4wEMMa0A64C2gL9gFeMMcZ/5YqISKAcNxCstQuAXUdsvgwY71sfDwz0rQ8AJlhrc621G4A1wFn+\nKVVERAKppGMI9ay1qQDW2q1APd/2hsBvhd632bdNRESCnL8GlXX/CxGREBdewv1SjTFx1tpUY0x9\nIM23fTPQuND7Gvm2HcUYoxARESkBa21AxmZPtIVgfEuBycCNvvVhwKRC24cYYyobY5oBLYFFxR3U\nWqvFT8uoUaOc11CeFp1PnctgXQLpuC0EY8yHQAIQa4zZCIwCxgKfGmNuAlLwrizCWvuLMeYT4Bcg\nB7jLBvonEBERvzhuIFhrry3mWxcU8/4ngSdLU5SIiJQ9zVQuJxISElyXUK7ofPqPzmXoMK56dIwx\n6k0SETlJxhis40FlEREp50p62alT1lomJ09m6uqp1I6qzY2n30jbum1dlyUiEtKcthA+/vjk97HW\ncs+0e3h0zqOcXv90IipF0Os/vRi3YFzAL8kSESnPnLYQPv0Urr765PZ5a+lbfLvpW767+TtqVKkB\nwB1n3kG/D/pxIO8Aj/V6LACVioiUf04DITv75N6fnpXOyNkjmXPDnINhANCoRiO+vv5rerzTgzpV\n63Bn1zv9XKmISPnntMvoZAPh1cWv0q9lPzrGdTzqe/Wj6zNj6AxGJY7ihy0/+KlCEZGKw2kgHDhw\n4u/Ny8/j5cUv8+C5Dxb7nuYxzXn54pcZ8tkQ9mTv8UOFIiIVR8i0EP6X8j/qVqtLp7hOx3zf4PaD\n6dmkJ3+Z/ZdSViciUrGETAvho58/4rqO153Qe//vov/j018+ZdHmYu+rJyIiRwiJFoK1lmlrpjGg\n9YATen/tqNo8feHT3D71dnLzc0tRoYhIxRESLYSV21ZSuVJlTq196gkfe2inodSOqs2/Fv6rhNWJ\niFQsIdFCmLluJn1a9MGYE799hzGGV/u/yhPznyAtM+34O4iIVHAh0UJYsHEBPZv0POnjt4ptxdBO\nQxmdOPqk9xURqWiCvoVgreWb377hvPjzSvQZj/V8jE9/+ZSkbUkl2l9EpKII+kBYt2sdEWERNK7R\n+PhvLkJs1VhGnDeCh75+qET7i4hUFEHfZbRw00K6Nep2UuMHR7rnrHtI2p7E7F9nl/gYIiLlndNA\nyM2F/Pxjv2fp1qV0adClVJ9TJbwKY3uP5c8z/0xefl6pjiUiUl45DYTKlY/fSli6dSmd63cu9Wdd\n2e5KqkZU5YMVH5T6WCIi5ZHTQKhS5djjCNZalm1dRucGpQ8EYwxPXfgUj819jKzcrFIfT0SkvHEe\nCMdqIWzas4nwsHDqR9f3y+d1j+/O6fVP5+VFL/vleCIi5YnzLqNjtRCStifRrm47v37mk72fZNw3\n40jPSvfrcUVEQl1QtxCStyfTOra1Xz+zXd12DGg9gLELxvr1uCIioS6oWwjJO/wfCABjEsbw5pI3\n2bRnk9+PLSISqpy3EI4bCHX8HwgNazTkti63MWruKL8fW0QkVDlvIZR1l1GBR7o/wpTVU1iZtjIg\nxxcRCTVOAyGnzpJiWwiZBzJJy0yjaa2mAfnsWpG1GNF9BCNnjwzI8UVEQo3TQNjS4oliWwhrdq6h\nRe0WVAqrFLDPv7vr3SxPXc78lPkB+wwRkVDhNBAI319sCyGQ3UUFqoRX4e9/+DsPz3oYa21AP0tE\nJNg5DQQbvr/YFkKgrjA60nWdrmN/zn6+XPVlwD9LRCSYOQ2E/ErHaCEE6AqjI4WZMMZeMJaRs0fq\n+csiUqG5bSFUKr6FsHrH6jJpIQD0adGHhjUa8vbSt8vk80REgpHTQMgLK76FsH7XeprHNC+TOowx\njLtgHGPmjSHzQGaZfKaISLBx22UUVnQLYW/2Xvbl7KNetXplVsuZp5xJj/gePP/982X2mSIiwcRt\nC8EU3UJI2Z1Ck1pNSvWUtJL4x/n/4Lnvn2Nb5rYy/VwRkWBQqkAwxvzJGPOzMWa5MeYDY0xlY0yM\nMWamMSbZGDPDGFOzuP1zTVaRgbAhfUPAJqQdS8vaLbmmwzX8c/4/y/yzRURcK3EgGGNOAf4IdLHW\ndgLCgWuAEcAsa21rYA5Q7FTgXIpuIWxI30CzWs1KWlqpPNbrMd5f/j6/7vrVyeeLiLhS2i6jSkA1\nY0w4EAVsBi4Dxvu+Px4YWNzOxkBmVs5R2121EADqVavHvWffy2NzH3Py+SIirpQ4EKy1W4BngI14\nQbDbWjsLiLPWpvresxUodmQ4gij27t9/1HaXgQDwwDkPMHf9XJb8vsRZDSIiZS28pDsaY2rhtQaa\nALuBT40x1wFH3gOi2HtC5Cfm823G3xg9OpqEhAQSEhIA94EQXTmax3o+xohZI5h5/UxndYiIJCYm\nkpiYWCafZUp6Dx9jzJVAH2vtrb7X1wPdgPOBBGttqjGmPjDXWtu2iP1t7N/j6bV+Hp+/1fSw79V5\nqg6/3P1LmV52eqScvBzav9Kely9+mQtbXOisDhGRwowxWGsDcglmacYQNgLdjDGRxrs+tDfwCzAZ\nuNH3nmHApOIOUKVSFJnZh3cZFcxBqFu1bilKK72IShE80fsJHpn1CPk232ktIiJloTRjCIuAz4Cl\nwE+AAd4AxgEXGmOS8UKi2IcXV6kUxb6cwwMhZXcKTWs1LfM5CEUZ1HYQEZUimPDzBNeliIgEXInH\nEACstWOAMUds3glccCL7R1aKPCoQXI8fFGaM4akLnmL4pOEMajuIKuFVXJckIhIwTmcqR0VEsT+I\nAwGgV9NetK3bltd+eM11KSIiAeU8ELJyjw6EJjWbOKqoaGN7j+WJBU+wO2u361JERALGaSBUjYgi\nK+/oQGgW42aWcnE6xnXkklMv4e//+7vrUkREAsZpIFSrEkV2XtZh24Kty6jAE72fYPxP41m1fZXr\nUkREAsJtIFSO4kB+cI8hFIiLjuPR7o9y/3/v1/OXRaRcchoI0ZFRHLCHAiFY5iAU556z7iFldwpT\nV091XYqIiN85DYTqkVHkFAoEV89BOFERlSJ4oe8L/GnGn8jOLeZRbyIiIcpxIFQjPyKDXN+z7YO1\nu6iwi1pcRId6HXju++dclyIi4ldOA6FO1VjCq+8gyzeuvCF9A01rNnVZ0gl55qJn+L9v/4/Neza7\nLkVExG+cBkJsVCxh1XZQcAfslPSUoG8hALSo3YLbz7idR2Y94roUERG/cRsIVWMxhQJhw+7g7zIq\nMLLHSOalzGN+ynzXpYiI+IXzFoKNKhQIITCGUCC6cjTP9XmOO7+6k5y8o5/6JiISapy3EGxkaAYC\neHdDja8ZrwFmESkXnLcQcit7gZBxIIOMAxlOH4pzsowxvHTxSzz1zVNsSN/guhwRkVJxfC+jqmAs\nuzL2sX7X+qB5DsLJaB7TnAfOeYA/Tv+jZjCLSEhzGgjGGCrnxpKWsYOV21bSrm47l+WU2IPnPsi6\nneuYlFzsw+FERIKe00AAqJIXy7aMHaxMW0n7uu1dl1MilStV5tX+r3Lv9HvJOJDhuhwRkRJxHghR\nxLI902shhGoggPcgnfObnc/oxNGuSxERKRHngVDd1GdL5iYvEOqFbiAAPH3h07y3/D2WbV3muhQR\nkZPmPBCamV4s3PchWzO20iq2letySqVutbqMu2AcN026SXMTRCTkOA+E9lX6sCZ/JoPbDaZypcqu\nyym1YacNo261ujzz3TOuSxEROSnOAyG+RjMa5/Tm7q53uy7FL4wxvH7J6zzz3TMkb092XY6IyAlz\nHgjR0dAndRadG3R2XYrfNK3VlFG9RnHz5JvJt/muyxEROSFBEQgZ5fBKzbu63gXAy4tedlyJiMiJ\nCYpA2LvXdRX+F2bCeGvAW4yZN0a3tRCRkBAUgVAeWwgAreu05sFzH+S2KbfpthYiEvQUCAH24LkP\nsmP/Dv695N+uSxEROSYFQoCFh4Xz7sB3eXTOo/y661fX5YiIFEuBUAba12vPiPNGMOzLYeTl57ku\nR0SkSM4DoXr18h8IAH86509UMpV49rtnXZciIlIk42qw0xhjrbXk5EBUFOTkQIg9CuGkbUjfQNc3\nuzLnhjl0jOvouhwRCUHGGKy1AfnX0nkLISICwsMhO9t1JYHXtFZTxl0wjusnXk92bgX4gUUkpDgP\nBCi/cxGKMvz04TSp1YQx88a4LkVE5DBBEwgVYRwBvObeG5e8wdtL32bBxgWuyxEROSgoAqF6ddiz\nx3UVZScuOo43L32ToV8MZdf+Xa7LEREBShkIxpiaxphPjTFJxpiVxpizjTExxpiZxphkY8wMY0zN\n4x2nVi3Yvbs0lYSeS1tfyoDWA7h1yq2axSwiQaG0LYQXgGnW2rbAacAqYAQwy1rbGpgDjDzeQWrV\ngvT0UlYSgp668CnW7lzLm0vedF2KiEjJA8EYUwPoYa19B8Bam2ut3Q1cBoz3vW08MPB4x4qJgV0V\nsOckMjySCVdO4C9z/sLKtJWuyxGRCq40LYRmwHZjzDvGmCXGmDeMMVWBOGttKoC1ditQ73gHqqgt\nBIA2ddowtvdYhnw+hP05+12XIyIVWGkCIRzoArxsre0CZOJ1Fx3ZIX7cDvJatSpmC6HATZ1von3d\n9jw480HXpYhIBRZein03Ab9Za3/wvf4cLxBSjTFx1tpUY0x9IK24A4wePRqAhQuhevUEIKEU5YSu\ngsdudn69M5//8jmD2g1yXZKIBInExEQSExPL5LNKdesKY8w84FZr7WpjzCigqu9bO62144wxjwAx\n1toRRexrCz77nXdg3jz4z39KXEq5sHjzYi7+8GK+uekbWsW2cl2OiAShYL51xb3AB8aYZXhXGT0B\njAMuNMYkA72Bscc7SEUdVD5S14Zd+VvC37jykyvZl7PPdTkiUsE4v7kdQGIijBrltRIqOmstN3x5\nA2EmjP9c9h9Meb/jn4iclGBuIfiFWgiHGGN4rf9r/LjlR81PEJEyFRSBUJEvOy1KtcrV+Pyqz/nL\nnL/ww5Yfjr+DiIgfBEUgxMQoEI7Uuk5rXu3/KoM/HczO/TtdlyMiFUBQjCHk50OVKrBvn/d8BDnk\nzzP+zIq0FUy7bhrhYaW5SlhEyoNyP4YQFgaxsbB9u+tKgs+4C8cB8MjXjziuRETKu6AIBIB69SCt\n2ClsFVd4WDgTrpzApORJvPvTu67LEZFyTIEQAmpH1WbSkEk8OPNBFm1e5LocESmnFAghon299vx7\nwL+54uMr2LJ3i+tyRKQcUiCEkAGtB3DHmXdwxcdXkJWb5bocESlnFAgh5i89/kJ8zXhumXyLnrQm\nIn6lQAgxxhjGDxzP2p1rGZ042nU5IlKOKBBCUFREFJOGTOK95e/pyiMR8ZugmemkQDg5cdFxfHXt\nVySMTyC+ZjwJTRNclyQiIS5oWgj168MWXTxzUtrWbctHgz7i6s+uZtX2Va7LEZEQFzSB0LAhpKZC\nbq7rSkLL+c3OZ2zvsfT/sD9pmWpiiUjJBU0gRERA3brw+++uKwk9wzsP57qO19H/w/7szd7ruhwR\nCVFBEwgAjRvDb7+5riI0jUkYQ+f6nbnikys4kHfAdTkiEoKCLhA2bnRdRWgyxvBK/1eIrhzNsC+H\nkW/zXZckIiEmqAIhPl4thNIIDwvno0EfsWXvFu7/7/2auCYiJyWoAkFdRqUXGR7JpCGTmJcyjyfm\nP+G6HBEJIUEVCPHxsGGD6ypCX63IWvz3uv/y1tK3eP2H112XIyIhImgmpgGceiqsWeO6ivKhQfUG\nfH391ySMTyAqIoobTrvBdUkiEuSCKhBatoT16725COFBVVloalG7BTOHzqT3u72JCo9icPvBrksS\nkSAWVF1GUVHejGV1G/lP27ptmX7ddO6Zfg9Tkqe4LkdEglhQBQJA69aQnOy6ivLltPqnMfWaqdw8\n+Wa+Xve163JEJEgpECqIrg278sXVX3DtF9eSuCHRdTkiEoSCLhDat4eff3ZdRfnUPb47n1z5CYM/\nHcyc9XNclyMiQSboAqFLF1iyxHUV5dcfmv2BzwZ/xtWfXc3MdTNdlyMiQcS4ms1qjLFFfXZWFtSu\nDTt3QmSkg8IqiAUbF3DFx1fw7uXv0rdlX9fliMgJMsZgrTWBOHbQtRAiI735COo2Cqzu8d2ZNGQS\nN0y8ga9Wf+W6HBEJAkEXCABnngmLFrmuovw7p/E5TLlmCsMnDWdi0kTX5YiIY0EZCD17wrx5rquo\nGM5udDbTr5vOXdPu4p2l77guR0QcCroxBPAmpp19NmzdCiYgPWVypFXbV9Hn/T7cd/Z9PHDOA67L\nEZFiVKgxBICmTaFqVUhKcl1JxdGmThvmD5/PGz++wV/n/FW3zhapgIIyEAAuvhim6E4LZSq+Zjzz\nh89n+trp3PXVXeTl57kuSUTKUNAGwsCB8OWXrquoeOpWq8vcYXNJ2p7EdV9cp8dxilQgpQ4EY0yY\nMWaJMWay73WMMWamMSbZGDPDGFOzJMft1QtWr4aUlNJWKCerRpUa/Hfof8nKzaLfB/1Iz0p3XZKI\nlAF/tBDuA34p9HoEMMta2xqYA4wsyUErV4ZrroHx4/1QoZy0yPBIPr/qc9rXbU/3t7uTkq5kFinv\nShUIxphGwMXAvwttvgwo+Gd8PDCwpMe/6SZ46y3IySl5jVJylcIq8ULfF7ilyy2c+/a5/LjlR9cl\niUgAlbaF8BzwEFD4kpQ4a20qgLV2K1CvpAfv0sWbtfz++6UrUkrOGMP93e7nxX4v0veDvkxdPdV1\nSSISICUOBGNMfyDVWrsMONY1saW6fvHxx+Gf//SeoibuXNH2CqZcM4Vbp9zKK4tfcV2OiARAaR5U\neR4wwBhzMRAFVDfGvAdsNcbEWWtTjTH1gbTiDjB69OiD6wkJCSQkJBz1np49oVEjr5Vw442lqFZK\nrVujbiwYvoD+H/YnaVsSz/Z5lohKEa7LEinXEhMTSUxMLJPP8stMZWNML+DP1toBxpingB3W2nHG\nmEeAGGvtiCL2KXam8pG+/x4uv9y74V1sbKnLlVJKz0rnms+vITs3m08Hf0psVf2hiJSVUJupPBa4\n0BiTDPT2vS6Vbt3g6qvhz38udW3iB7UiazH1mql0PaUrXd/syorUFa5LEhE/CMp7GRUlIwM6dYJn\nnvFaCxIcPlj+AffPuJ83LnmDy9vqD0Yk0ALZQgiZQADvltiXXALz53vPXpbg8MOWH7ji4yu4qfNN\nPN7rccJM0E6AFwl5CoRC3ngDnn8eFizwnqwmwWFrxlau/ORKalSpwXuXv6dxBZEACbUxhIC69Vbo\n2xcuvRT27XNdjRSoH12fucPm0r5ue8544wwWblrouiQROUkh10IAyM+HYcMgPR2++AIidOVjUPly\n1ZfcNuU2Hu/1OHd3vRujh1qI+I26jIqQk+PdEbV6dXjvPYVCsFm3cx2DPx1Mq9hWvHnpm1SvUt11\nSSLlgrqMihARAZ9/Dnv3epekHtBdmoNKi9ot+Pbmb6lZpSZnvnkmS35f4rokETmOkG0hFDhwwAuE\nnBz47DOIjPRDceJXE36ewL3T7+Xh8x7mgXMe0FVIIqWgLqPjyMmBoUNh505vTKG6eieCzob0DQz9\nYihREVGMHzieU6qf4rokkZCkLqPjiIiADz6AFi28B+ts3eq6IjlS01pNSbwxke6Nu9Pl9S5MTp7s\nuiQROUK5aCEUsBb+8Q945x2YPl2T14LVNxu/YejEofRt0ZenL3qa6MrRrksSCRlqIZwgY+Cxx+Cv\nf/VaCt9/77oiKcp58eex7PZlZOVl0enVTiRuSHRdkohQzloIhU2b5s1VeOklb9BZgtPU1VO5fert\nDGo7iCd7P0m1ytVclyQS1NRCKIGLL4avv4aHH4ZRo7zJbBJ8Lml1CSvuXMGurF2c9tppLNi4wHVJ\nIhVWuW0hFEhN9e6O2rAhjB8PVasG/COlhL5c9SV3fXUXV7W/in+c/w+NLYgUQS2EUoiLgzlzvCDo\n0QM2bXJdkRRnYJuBLL9zOelZ6bR/pb2e3yxSxsp9C6GAtfD00/DCCzBhghcOErxm/zqbO766g871\nO/NC3xdoUL2B65JEgoJaCH5gjDee8NZbcOWV8OyzXkhIcOrdvDfL71jOqbVPpdNrnXj9h9fJtxoI\nEgmkCtNCKGzDBi8UmjWDt9/WzOZgtyJ1BbdPvZ18m89LF7/Emaec6bokEWfUQvCzpk0PPWCna1f4\n5RfXFcmxdIzryIKbFnD7Gbdz6UeXcuvkW9mWuc11WSLlToUMBPBugvf66zBihDeJ7aOPXFckxxJm\nwhjeeThJdycRXTma9q+058WFL5Kbn+u6NJFyo0J2GR1p2TIYPBh69oR//QuqaW5U0FuZtpJ7/3sv\naZlpvNjvRRKaJrguSaRMqMsowE4/HZYsgbw86NIFli51XZEcT/t67Zl1/SxG9RrFsC+HcfnHl5O8\nPdl1WSIhTYHgU706/Oc/3qzmiy6C557T7OZgZ4zhynZXknxPMuc0Oofu73Tnrq/uIjUj1XVpIiFJ\ngXCEa6+FhQvh44+hf39vprMEt8jwSB4+72FW3b2KyPBI2r3Sjr/N+xuZBzJdlyYSUhQIRWjeHObP\nh86dve6kL75wXZGciNiqsTzb51kW37qYpO1JtHqpFW/8+AY5eTmuSxMJCRpUPo5vv4Ubb/QuT33x\nRe9SVQkNizcvZuTskaxPX8/jPR/nuk7XER4W7roskVLRoLJD557rXYVUpw507AhffeW6IjlRXRt2\nZdYNs3h7wNu8vext2r/Sng9XfEhefp7r0kSCkloIJ2HePBg+HBISvEHnmjVdVyQnylrLnPVzeGzu\nY+zO3s3oXqMZ1G4QYUb/J5LQEsgWggLhJGVkwEMPwZQpXhfS5Ze7rkhOhrWWGetm8Pjcx9mXs48R\n3UcwpMMQdSVJyFAgBKH58+G227znNr/0EjRq5LoiORnWWmaum8mTC55k4+6NPHTuQwzvPJzI8EjX\npYkck8YQglCPHt7YQufO3vLii97ENgkNxhj6tOxD4o2JvH/F+0xbO41mLzTjqW+eYk/2HtfliTih\nFoIfrFoFt98OWVne/ZFOP911RVISy1OXM3bBWGaum8nNnW/mnrPuoXHNxq7LEjmMWghBrk0bmDvX\n60Lq2xfuugt27HBdlZysTnGd+HDQhyy6dREH8g5w2munMeSzIXy/6XvXpYmUCQWCn4SFwc03Q1IS\nVKoE7drBq6+qGykUNY9pznN9n2PD/Rvo1qgb135+Lee8dQ4f//yxJrlJuaYuowBZvhz++EfYs8cb\nX+je3XVFUlJ5+XlMTp7M8wufZ/2u9dzV9S5u6nwT9arVc12aVEC6yihEWevdE+mhh7wJbk88AS1a\nuK5KSmPJ70t4cdGLTEyaSL9T+3HHGXfQs0lPjAnI30+RowRlIBhjGgHvAnFAPvCmtfZfxpgY4GOg\nCbABuMpau7uI/ct9IBTIzPQmsj3/PAwdCn/9qzfzWUJXelY67/30Hq/9+Br5Np/bz7idG067gdpR\nureJBFawBkJ9oL61dpkxJhr4EbgMGA7ssNY+ZYx5BIix1o4oYv8KEwgF0tLgb3+DCRPgwQfhvvsg\nKsp1VVIa1loWbFzA6z++zldrvmJA6wHc0vkWusd3V6tBAiIoA+GoAxnzJfCSb+llrU31hUaitbZN\nEe+vcIFQYPVqePRRWLTIC4jrr/cGoiW0bd+3nfHLxvPOsnfYn7ufYacN44bTbqBpraauS5NyJOgD\nwRjTFEgEOgC/WWtjCn1vp7X2qHZ0RQ6EAt9+6z3TOS3NezDPVVcpGMoDay0//v4j45eNZ8LKCXSo\n14EbT7uRQe0GEV052nV5EuKCOhB83UWJwN+ttZOODABjzA5rbWwR+9lRo0YdfJ2QkEBCQkKpaglF\n1sLs2fD447B7N4weDYMGeZexSujLzs1m6uqpjP9pPPM3zmdA6wFc0+EaejfrTUSlCNflSQhITEwk\nMTHx4OsxY8YEZyAYY8KBqcB0a+0Lvm1JQEKhLqO51tq2Rexb4VsIhVkLM2Z4wZCVBWPGwMCBoG7o\n8iM1I5WPfv6Ij1d+zNqdaxnUdhBDOgyhR3wPKoWpaSgnJmhbCMaYd4Ht1toHCm0bB+y01o7ToPLJ\ns9Z75sKoUZCb63UpDR4M4boZZ7myftd6Pln5CRNWTiA1I5XB7QYzpMMQujXqpsFoOaagDARjzHnA\n/4AVgPUtjwKLgE+AxkAK3mWn6UXsr0A4Bmth+nQYOxY2b/bmMtx4I0TqZpzlzqrtq/j454+ZsHIC\n+3L2cUWbK7i87eWc1/g8tRzkKEEZCKX+YAXCCfvmGxg3DhYv9i5VvfNOPZynPLLWsiJtBROTJjJx\n1US27N3Cpa0u5fK2l3NB8wt0a24BFAji8/PPXjBMm+Y9ue2ee6BpU9dVSaCs37WeScmTmLhqIsu2\nLuOiFhcxsPVA+rfqT63IWq7LE0cUCHKYlBTvoTzvvAM9e3qthp49NQBdnm3L3MbU1VOZuGoiiRsS\nOb3+6fRr2Y+LT72YTnGdNO5QgSgQpEgZGfDuu/Cvf3kznu+7D4YM0ThDebc/Zz/zUuYxbc00pq2Z\nxv7c/fRr2Y9+LftxQfMLqBmp/sTyTIEgx5SfDzNnwgsvwJIlXnfSLbdAy5auK5OysGbHGqatmcb0\ntdP55rdvOKPBGVzQ/AJ6N+tN14Zd9bzockaBICds9Wp4800YPx5OO817aM9ll0Hlyq4rk7KQeSCT\neSnzmP0eg9LcAAALwElEQVTrbGatn0VKego9mvSgd7Pe9G7Wmw71Oqh7KcQpEOSkZWfDxIneIz2T\nkrxLVtVqqHjSMtOYu34us9fPZvb62WQcyOD8ZufTu1lvejXpRcvaLRUQIUaBIKVSuNXQoQPccIN3\ne4zq1V1XJmVtQ/oGZv86mzkb5vC/lP+Rm59L9/ju9IjvQY/4HnSK66S5D0FOgSB+kZ0NU6d6A9Hz\n5kH//l44XHCBbqpXEVlrSdmdwvyU+czf6C2/7/2dcxqfQ/fG3enRpAddT+lKVITu0R5MFAjid9u2\neU9ze/dd2LQJrr3WC4dOnVxXJi5ty9zGgo0LDgbEyrSVtK3blrMbns3ZDc+mW6NunBp7KmFGd190\nRYEgAZWUBO+9B++/73UjXXWVd/+kdu1cVyau7c/Zz9KtS1m4aSELN3tLelY6ZzU862BInN3obOpU\n1SMAy4oCQcpEfj4sXAiffAKffgoxMV44XHUVtG7tujoJFqkZqV44+EJi8ZbFxETG0KVBFzrX7+x9\nbdCZBtENNGAdAAoEKXP5+fDdd4fCoW5dbyD6ssu8biX9PZcC+TaftTvXsvT3pSz5fQlLty5l6dal\nhJmww0OifmeaxzRXSJSSAkGcys+HBQvgiy9g0iTvTqwDBnjh0LMnROg5L3IEay2b9mxi6dZCIfH7\nUvZk76FjXEc61O1Ah3qHltiqRz1DS4qhQJCgYS2sXOkFw+TJ3iWtfft64dC3L9TSPdfkGLbv286K\n1BX8nPazt2zzvlaNqErHeh0PC4l2ddvpkaNFUCBI0Pr9d5gyxQuHefOgc2fo08dbunTRo0Dl+Apa\nE0eGRNK2JOpUrUPrOq1pE9uG1nVa0zq2Na3rtKZRjUYV9konBYKEhH37vFCYMcNbduyACy/0wuGi\ni6B+fdcVSijJy89j4+6NrNq+iuQdySRvT2bVjlUkb09md/ZuWsW28gLCFxKtY1vTonaLcn9rcAWC\nhKSUFO+mezNmwOzZ0KSJNwnuD3+A7t31kB8puT3Ze0jennwwKJJ3eMu6neuoEl6F5jHNaRHTwltq\ntzj4umGNhiHfslAgSMjLzfUuaZ0711sWLYI2bSAh4VBA1KjhukoJddZatu3bxrqd61i3ax2/7vqV\ndbvWHXydnpVO01pNDwZEs1rNiK8ZT5NaTYivGU/dqnWD/iooBYKUO9nZXijMnQuJid56u3ZeOPTs\nCd26QawuPBE/yzyQyfr09QcDIiU9hY17Nnpfd29kX84+Gtds7IVEzSaHfY2vGU/jmo2pXMntrYMV\nCFLuZWUdCoj58731U06Bc889tLRpo0FqCayMAxn8tvs3Nu7eSMpuLyQKr2/Zu4XaUbVpWL0hDWs0\n5JToU7yv1U+hYXXf1xoNiYmMCVhLQ4EgFU5envcM6W+/9SbIffutN0jdrZsXDt26wZlnerOpRcpK\nXn4eWzO2snnvZrbs3cLmPb6vew//mpWbxSnVTzk8KKo3JC46jrhqcdSrVo+46DjqVq170neXVSCI\nAKmpXjgULMuWeTOozzzz0NKliwarxb3MA5ls2bvl8LDYs5nUzFRSM1NJy0wjNSOVXVm7iImMOTwo\nqsUdfF14e71q9agSXkWBIFKUvDxYswZ++OHQ8tNP0KCBFw5nnOEtnTpB7dquqxU5Wm5+Ltv3bSc1\nwxcSmamkZqQeDI7C27dlbiMqIoo9I/coEERORF4erFrlhcOPP3rLihVeq6FTp8OXVq102w0JHdZa\n0rPSqV21tgJBpKTy8705EcuXe+GwfLm3bNzohUKnTtCxI7Rt6y1Nm+qBQRK81GUkEgD79sEvvxwK\niqQkb9m2DU491buqqSAk2rb1wiMy0nXVUtEpEETKUEYGJCd7XU8FIZGUBL/+Cg0bekHRsuWhpUUL\nr1VR2e3l6VJBKBBEgkBOjhcKq1bBunXesnatt2za5IVFixaHB0XLltCsGVSr5rp6KS8UCCJBLifH\nG6coCIi1a73AWLPG2x4d7d3LqbglJkYPHZITo0AQCWHWQlqaFwwFy4YNh7/Ozz88IBo18mZqN2zo\nLaec4l0ppdAQBYJIOZeefigcNm6EzZsPLVu2eF/z8g6FQ+GgKFivXx/i4tQ9Vd4pEESEvXsPD4gj\n17du9WZzV6rkBUO9et7XguXI13FxanWEIgWCiJwQa73gSE31lrS0Q+uFl4Lt2dne7T9iY72lTp1D\n64WXwttr1tRNBl1SIIhIQOzf74XDjh3HX7Zv975mZnqD4AUhUbu29yztmjW9r8db1+W5paNAEJGg\nkZMDO3ceCoqdO2H3bm8cpOBrwVLU64iIosOienVviY4uer2o71XElkpIBoIxpi/wPBAGvGWtHXfE\n9xUIIhWMtd4M8aICY+/eQ0tGxvFfZ2ZCVFTRgVGtmrdUrVqypVo1qFIlOMdXQi4QjDFhwGqgN7AF\nWAwMsdauKvQeBYIfJSYmkpCQ4LqMckPn038CdS7z871QODIs9u71Qud4S2bmsb9/4MDRQREZ6QVF\nZOSJrZ/MewvWq1TxutWqVCn6nlqBDITwQBwUOAtYY61NATDGTAAuA1Ydcy8pMf0D5l86n/4TqHMZ\nFnaoZdCggd8PT16eN8ZSOECys72n+2VlHX89I8MbdzmR9xZeP3DAW8/O9looBeFQuXLgx18CFQgN\ngd8Kvd6EFxIiIiGhUiWvCyo62l0NeXleMBw4cCgo4uMD93mBCgQRESmlSpUOdVeVhUCNIXQDRltr\n+/pejwBs4YFlY4wGEERESiDUBpUrAcl4g8q/A4uAa6y1SX7/MBER8YuAdBlZa/OMMfcAMzl02anC\nQEQkiDmbmCYiIsHFyTw/Y0xfY8wqY8xqY8wjLmoIBcaYDcaYn4wxS40xi3zbYowxM40xycaYGcaY\nmoXeP9IYs8YYk2SMuajQ9i7GmOW+8/28i5/FBWPMW8aYVGPM8kLb/Hb+jDGVjTETfPt8Z4wJ4PUf\n7hVzPkcZYzYZY5b4lr6FvqfzWQxjTCNjzBxjzEpjzApjzL2+7W5/P621ZbrghdBaoAkQASwD2pR1\nHaGwAL8CMUdsGwc87Ft/BBjrW28HLMXrBmzqO8cFLcCFQFff+jSgj+ufrYzOX3fgdGB5IM4fcCfw\nim/9amCC65/ZwfkcBTxQxHvb6nwe81zWB073rUfjjbm2cf376aKFcHDSmrU2ByiYtCZHMxzdirsM\nGO9bHw8M9K0PwPsDz7XWbgDWAGcZY+oD1a21i33ve7fQPuWatXYBsOuIzf48f4WP9RneRRTlVjHn\nE7zf0yNdhs5nsay1W621y3zrGUAS0AjHv58uAqGoSWsNHdQRCizwtTFmsTHmFt+2OGttKni/VEA9\n3/Yjz+tm37aGeOe4QEU/3/X8eP4O7mOtzQPSjTG1A1d60LrHGLPMGPPvQl0cOp8nyBjTFK/l9T3+\n/ft90uezAt4rMKScZ63tAlwM3G2M6YEXEoXpqoDS8ef5C8JboQXcK0Bza+3pwFbgGT8eu9yfT2NM\nNN7/3u/ztRQC+ff7uOfTRSBsBgoPbjTybZMjWGt/933dBnyJ192WaoyJA/A1F9N8b98MNC60e8F5\nLW57ReXP83fwe765NzWstTsDV3rwsdZus75OauBNDt2iRufzOIwx4Xhh8J61dpJvs9PfTxeBsBho\naYxpYoypDAwBJjuoI6gZY6r6/veAMaYacBGwAu9c3eh72zCg4BdpMjDEd2VBM6AlsMjX7NxtjDnL\nGGOAGwrtUxEYDv+fkT/P32TfMQAGA3MC9lMEj8POp+8frQJXAD/71nU+j+9t4Bdr7QuFtrn9/XQ0\nwt4Xb1R9DTDC9Yh/MC5AM7wrsJbiBcEI3/bawCzf+ZsJ1Cq0z0i8qw+SgIsKbT/Dd4w1wAuuf7Yy\nPIcf4t1+PRvYCAwHYvx1/oAqwCe+7d8DTV3/zA7O57vAct/v6pd4feA6n8c/l+cBeYX+ji/x/bvo\nt7/fJTmfmpgmIiKABpVFRMRHgSAiIoACQUREfBQIIiICKBBERMRHgSAiIoACQUREfBQIIiICwP8D\nNMVFA084WkcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe24e6e98d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cost_tracker['Iteration'],cost_tracker['Training Cost'])\n",
    "plt.plot(cost_tracker['Iteration'],cost_tracker['Test Cost'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in range(training_iterations):\n",
    "#     for (X, y) in zip(train_ft.eval(),train_resp.eval()):\n",
    "#         sess.run(train_step,feed_dict = [x: X.reshape(1,2), y_: y.reshape(1,1)})\n",
    "    \n",
    "#     if (i+1) % 50 == 0:\n",
    "#         c = sess.run(criterion, feed_dict=[x: train_ft.eval(), y_:train_resp.eval()})\n",
    "#         print(\"Epoch:\", '%04d' % (training_iterations+1), \"cost=\", \"[:.9f}\".format(c),\n",
    "#         \"W=\", sess.run(W1), \"b=\", sess.run(b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.931552\n",
      "0.0326686\n",
      "0.00360335\n",
      "0.00539651\n"
     ]
    }
   ],
   "source": [
    "for (X, y) in zip(train_ft.eval(),train_resp.eval()):\n",
    "    print(sess.run(criterion,feed_dict = {x: X.reshape(1,2), y_: y.reshape(1,1)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe2579e02b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_ft.eval()[:,0], train_resp.eval(), 'ro', label='Original data')\n",
    "plt.plot(train_ft.eval()[:,0], sess.run(W1)[:,0] * train_ft.eval()[:,0] + sess.run(b1), label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 48.02882385]\n",
      " [ 51.44346619]\n",
      " [ 54.85810852]\n",
      " [ 65.20498657]\n",
      " [ 71.72537994]\n",
      " [ 82.27819061]] [[ 46.]\n",
      " [ 48.]\n",
      " [ 52.]\n",
      " [ 60.]\n",
      " [ 74.]\n",
      " [ 80.]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(model_y, feed_dict = {x: test_ft.eval()}),test_resp.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 12.,   5.],\n",
       "       [ 14.,   7.],\n",
       "       [ 16.,   9.],\n",
       "       [ 22.,  14.],\n",
       "       [ 26.,  21.],\n",
       "       [ 32.,  24.]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ft.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.6997876"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(criterion, feed_dict = {x: test_ft.eval(),y_:test_resp.eval()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Some new observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = [\n",
    "    [6, 4],\n",
    "    [10, 5],\n",
    "    [14, 8]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 37.2700882 ],\n",
       "       [ 44.40826035],\n",
       "       [ 51.34049988]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(model_y, feed_dict = {x: new_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the least squares solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls_B_1 = tf.matrix_inverse(\n",
    "    tf.matmul(tf.transpose(train_ft),train_ft)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls_B_2 = tf.matmul(tf.transpose(train_ft),train_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls_B = tf.matmul(ls_B_1,ls_B_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.016716  ]\n",
      " [-3.78593445]]\n",
      "[[ 1.81028318]\n",
      " [-0.102963  ]]\n"
     ]
    }
   ],
   "source": [
    "print(ls_B.eval())\n",
    "print(tf.transpose(W1).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
